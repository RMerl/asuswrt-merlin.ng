/*
 * PCIEDEV timing critical datapath functions
 * compiled for performance rather than size
 * Copyright 2020 Broadcom
 *
 * This program is the proprietary software of Broadcom and/or
 * its licensors, and may only be used, duplicated, modified or distributed
 * pursuant to the terms and conditions of a separate, written license
 * agreement executed between you and Broadcom (an "Authorized License").
 * Except as set forth in an Authorized License, Broadcom grants no license
 * (express or implied), right to use, or waiver of any kind with respect to
 * the Software, and Broadcom expressly reserves all rights in and to the
 * Software and all intellectual property rights therein.  IF YOU HAVE NO
 * AUTHORIZED LICENSE, THEN YOU HAVE NO RIGHT TO USE THIS SOFTWARE IN ANY
 * WAY, AND SHOULD IMMEDIATELY NOTIFY BROADCOM AND DISCONTINUE ALL USE OF
 * THE SOFTWARE.
 *
 * Except as expressly set forth in the Authorized License,
 *
 * 1. This program, including its structure, sequence and organization,
 * constitutes the valuable trade secrets of Broadcom, and you shall use
 * all reasonable efforts to protect the confidentiality thereof, and to
 * use this information only in connection with your use of Broadcom
 * integrated circuit products.
 *
 * 2. TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED
 * "AS IS" AND WITH ALL FAULTS AND BROADCOM MAKES NO PROMISES,
 * REPRESENTATIONS OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR
 * OTHERWISE, WITH RESPECT TO THE SOFTWARE.  BROADCOM SPECIFICALLY
 * DISCLAIMS ANY AND ALL IMPLIED WARRANTIES OF TITLE, MERCHANTABILITY,
 * NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF VIRUSES,
 * ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
 * CORRESPONDENCE TO DESCRIPTION. YOU ASSUME THE ENTIRE RISK ARISING
 * OUT OF USE OR PERFORMANCE OF THE SOFTWARE.
 *
 * 3. TO THE MAXIMUM EXTENT PERMITTED BY LAW, IN NO EVENT SHALL
 * BROADCOM OR ITS LICENSORS BE LIABLE FOR (i) CONSEQUENTIAL, INCIDENTAL,
 * SPECIAL, INDIRECT, OR EXEMPLARY DAMAGES WHATSOEVER ARISING OUT OF OR
 * IN ANY WAY RELATING TO YOUR USE OF OR INABILITY TO USE THE SOFTWARE EVEN
 * IF BROADCOM HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES; OR (ii)
 * ANY AMOUNT IN EXCESS OF THE AMOUNT ACTUALLY PAID FOR THE SOFTWARE ITSELF
 * OR U.S. $1, WHICHEVER IS GREATER. THESE LIMITATIONS SHALL APPLY
 * NOTWITHSTANDING ANY FAILURE OF ESSENTIAL PURPOSE OF ANY LIMITED REMEDY.
 *
 * $Id: pciedev_data.c  $
 */

/**
 * @file
 * Note that the firmware (of which this file is a part of) uses PROP_TXSTATUS macro's, but the host
 * driver is not built with PROP_TXSTATUS support, reason being that the prop_tx 'channel' runs
 * between wl layer and bus layer instead of between wl layer and host driver. As a consequence,
 * several PROP_TXSTATUS macro's have a different meaning than their names suggest. For instance,
 * WL_SEQ_SET_FROMDRV() does not mean that a packet was generated by the host driver.
 */

#include <typedefs.h>
#include <bcmdevs.h>
#include <osl.h>
#include <pciedev_priv.h>
#include <pciedev.h>
#include <pciedev_dbg.h>
#include <event_log.h>
#include <wlfc_proto.h>
#include <msgtrace.h>
#include <logtrace.h>
#include <sbchipc.h>
#include <flring_fc.h>

/* Static definitions */
typedef int (*d2h_msg_handler)(struct dngl_bus *pciedev, void *p);

/* Static functions */
static bool pciedev_read_host_buffer(struct dngl_bus *pciedev, msgbuf_ring_t *msgbuf);
static int8 pciedev_allocate_flowring_fetch_rqst_entry(struct dngl_bus *pciedev);
static void pciedev_free_flowring_fetch_rqst_entry(struct dngl_bus *pciedev, uint8 index);
static void pciedev_adjust_flow_fetch_ptr(msgbuf_ring_t *flow_ring, uint16 index);
static void pciedev_update_rdp_ptr_unacked(msgbuf_ring_t *flow_ring);
static int pciedev_update_txstatus(struct dngl_bus *pciedev, uint32 status,
	uint16 ring_idx, uint16 flowid, uint16 seq);
void pciedev_free_lclbuf_pool(msgbuf_ring_t * ring, void* p);
static int pciedev_get_host_addr(struct dngl_bus * pciedev, uint32* bufid, uint16 * len,
	dma64addr_t *haddr, dma64addr_t *meta_addr, uint16 *meta_len);
static void pciedev_add_to_inuselist(struct dngl_bus *pciedev, void* p, uint8 max_items);
static void pciedev_process_rxpost_msg(struct dngl_bus *pciedev, void* p, uint32* bufid,
	uint16 *len, dma64addr_t *haddr, dma64addr_t *haddr_meta, uint16 *metadata_len);
static int pciedev_htoddma_queue_avail(struct dngl_bus *pciedev);
static void pciedev_enque_fetch_cmpltq(struct dngl_bus *pciedev, struct fetch_rqst *fr);
static struct fetch_rqst * pciedev_deque_fetch_cmpltq(struct dngl_bus *pciedev);
static void* pciedev_get_src_addr(msgbuf_ring_t * ring, uint16* available_len, uint16 max_len);
static void pciedev_ring_update_readptr(struct dngl_bus *pciedev, msgbuf_ring_t *ring,
	uint16 bytes_read);
static void pciedev_ring_update_writeptr(msgbuf_ring_t *ring, uint16 bytes_written);
static uint32 pciedev_get_ring_space(struct dngl_bus *pciedev,
	msgbuf_ring_t *msgbuf, uint16 msglen);
static bool pciedev_resource_avail_for_txmetadata(struct dngl_bus *pciedev, uint8 num_items);
static bool pciedev_check_process_d2h_message(struct dngl_bus *pciedev, uint32 txdesc,
	uint32 rxdesc, void *p, d2h_msg_handler* rtn);
static int pciedev_process_d2h_rxpyld(struct dngl_bus *pciedev, void *p);
static int pciedev_process_d2h_txmetadata(struct dngl_bus *pciedev, void *p);
static uint16 pciedev_htoddma_deque(struct dngl_bus *pciedev, msgbuf_ring_t **msgbuf,
	uint8 *msgtype);
static void pciedev_queue_rxcomplete_msgring(struct dngl_bus *pciedev,  msgbuf_ring_t *ring);
static int pciedev_create_d2h_messages(struct dngl_bus *bus, void *p, msgbuf_ring_t *msgbuf);
static dma64addr_t pciedev_get_haddr_from_lfrag(struct dngl_bus *pciedev, void* p, uint32* bufid,
	dma64addr_t *haddr_meta, uint16 *metadata_len, uint16 *dataoffset);
static void pciedev_queue_txstatus(struct dngl_bus *pciedev, uint32 pktid,
	uint8 ifindx, uint16 ringid, uint16 txstatus, uint16 metadata_len, msgbuf_ring_t *ring);
static void pciedev_xmit_rxcomplete(struct dngl_bus *pciedev, msgbuf_ring_t *ring);
static int pciedev_tx_msgbuf(struct dngl_bus *bus, void *p, ret_buf_t *ret_buf, uint16 msglen,
	msgbuf_ring_t *msgbuf);
static uint16 pciedev_handle_h2d_pyld(struct dngl_bus *pciedev, uint8 msgtype, void *p,
	uint16 pktlen, msgbuf_ring_t *msgbuf);
static void pciedev_process_tx_post(struct dngl_bus *bus, void* p, uint16 msglen,
	uint16 ring, uint16 fetch_idx);
static void pciedev_h2dmsgbuf_dma(struct dngl_bus *pciedev, dma64addr_t src, uint16 src_len,
	uint8 *dest, uint8 *dummy_rxoff, msgbuf_ring_t *msgbuf, uint8 msgtype);
static void pciedev_push_pkttag_tlv_info(struct dngl_bus *pciedev, void* p,
	msgbuf_ring_t *flow_ring, uint16 index);
static uint8* pciedev_get_cirbuf_pool(msgbuf_ring_t * ring, uint16 len);
static void pciedev_free_cirbuf_pool(msgbuf_ring_t * ring, void* p, uint16 len);

static void pciedev_manage_h2d_dma_clocks(struct dngl_bus *pciedev);

const uint8 tid_prio_map[] = {2, 0, 1, 3, 4, 5, 6, 7};
const uint8 AC_prio_map[] = {1, 0, 2, 3};
const uint8 tid2AC_map[] = {0, 1, 1, 0, 2, 2, 3, 3};

static void pciedev_chained_rxpkt(struct dngl_bus *pciedev, void* p);
static void pciedev_insert_fetch_rqst(struct dngl_bus * pciedev, flow_fetch_rqst_t *node);
static void pciedev_remove_fetch_rqst_head(struct dngl_bus * pciedev);
#ifdef HOST_HDR_FETCH
static int pciedev_mac_dma_submit(struct dngl_bus *pciedev, void **buf, dma_queue_t** item);
static bool pciedev_d2h_desc_avail(struct dngl_bus *pciedev);
#endif /* HOST_HDR_FETCH */

#if defined(PCIE_DMA_INDEX) && defined(SBTOPCIE_INDICES)
static void pciedev_sync_d2h_write_ptrs(struct dngl_bus * pciedev, msgbuf_ring_t* ring);
#endif // endif
#ifdef PCIEDEV_HOST_PKTID_AUDIT_ENABLED
inline void
pciedev_host_pktid_audit(struct dngl_bus *pciedev, uint32 pktid, const bool alloc)
{
	struct bcm_mwbmap *handle = pciedev->host_pktid_audit;

	if (handle == NULL) {
		PCI_ERROR(("pciedev_host_pktid_audit Host PktId Audit: Handle NULL\n"));
		return;
	}

	if ((pktid == 0) || (pktid > PCIEDEV_HOST_PKTID_AUDIT_MAX)) {
		PCI_ERROR(("pciedev_host_pktid_audit Host PktId Audit: Invalid pktid<%d>\n",
			pktid));
		return;
	}

	if (alloc) {
		if (!bcm_mwbmap_isfree(handle, pktid)) {
			PCI_ERROR(("pciedev_host_pktid_audit ERROR: Host Pktid<%d> is not free."
						"recv duplicate\n", pktid));
			PCIEDEV_PKTID_AUDIT_ALLOC_ERR_INCR(pciedev);
			return;
		}
		bcm_mwbmap_force(handle, pktid);
	} else {
		if (bcm_mwbmap_isfree(handle, pktid)) {
			PCI_ERROR(("pciedev_host_pktid_audit ERROR: Host Pktid<%d> is freed."
						"send duplicate\n", pktid));
			PCIEDEV_PKTID_AUDIT_FREE_ERR_INCR(pciedev);
			return;
		}
		bcm_mwbmap_free(handle, pktid);
	}
}
#endif /* PCIEDEV_HOST_PKTID_AUDIT_ENABLED */

#if defined(PCIE_M2M_HOST_READ_BARRIER)
/**
 * XXX: HW WAR for D2H Mem2Mem Rx CurDesc advancement before DMA completion. Performs a read from a
 * known 32bit host address, to flush all pending DMAs before updating the WR index/ring doorbell in
 * the D2H direction.
 */
static uint32 pciedev_d2h_read_barrier(struct dngl_bus * pciedev);

static uint32
pciedev_d2h_read_barrier(struct dngl_bus * pciedev)
{
	uint32 host_loaddr;
	uint32 * host_mem;
	volatile uint32 rd_val32;

	ASSERT(pciedev->regs);

	ASSERT(pciedev->d2h_dma_scratchbuf_len != 0);    /* host scratch buffer */
	ASSERT(pciedev->d2h_dma_scratchbuf.hiaddr == 0); /* 32 bit host */

	/* Fetch host location of D2H magic number to be read.
	 * Host side: reserves an extra 8Bytes following the d2h_dma_scratchbuf.
	 * and applies a pattern <PCIE_SHARED_D2H_MAGIC, PCIE_SHARED_H2D_MAGIC>
	 */
	host_loaddr = pciedev->d2h_dma_scratchbuf.loaddr
		+ pciedev->d2h_dma_scratchbuf_len;

	/* Use sbtopcie translation 0 */
	host_mem = (uint32 *)(0x08000000 + (host_loaddr & ~SBTOPCIE0_MASK));

	/* Program the translation 0 base */
	W_REG(pciedev->osh, &pciedev->regs->sbtopcie0,
		(host_loaddr & SBTOPCIE0_MASK) | 0xC);

	/* Access host memory via sbtopcie translation 0 */
	rd_val32 = *(host_mem);
	if (rd_val32 != PCIE_SHARED_D2H_MAGIC) {
		PCI_ERROR(("*host_mem: <0x%08x> expected <0x%08x>\n",
			rd_val32, PCIE_SHARED_D2H_MAGIC));
	}

	return rd_val32;
}
#endif /* PCIE_M2M_HOST_READ_BARRIER */

/**
 * Called when eg a mailbox 0 interrupt was generated by the host, indicating that the host has
 * added a new message in host memory. Firmware now has to decode host read/write pointers and
 * schedule DMA to pull messages into device memory (local circular buffers).
 */
bool
pciedev_msgbuf_intr_process(struct dngl_bus *pciedev)
{
	/* Host->Dongle --> DoorBell Rang
	* Doorbell intr can be triggered either due to
	* data written to data ring or control ring on host side
	* Need to check both data ring and control ring on the host,
	* but give first preference to the control ring.

	 * MsgBuf is a generic implementation and does
	 * not get into how to read and where to read into. We need to provide the core_read
	 * routine (for eg. htod_msgbuf_read_handler) and ctx which is the pciedev in our case
	 */

	/* Local queue storing responses is full
	 * so process the items in the queue
	 * then read host buffer
	 */
	if (pciedev_ctrl_resp_q_avail(pciedev) <
		(PCIEDEV_CNTRL_CMPLT_Q_IOCTL_ENTRY +
		PCIEDEV_CNTRL_CMPLT_Q_STATUS_ENTRY +
		CTRL_SUB_BUFCNT)) {
	        pciedev->ctrl_resp_q->status |= CTRL_RESP_Q_FULL;
	        pciedev_process_ctrl_cmplt(pciedev);
	        return FALSE;
	}

	/* CHECK the read/write offsets and call the right routines */
	if (pciedev->ioctl_lock == FALSE) { /* block ioctls when a prior one is pending */
		pciedev_read_host_buffer(pciedev, pciedev->htod_ctrl); /* starts pulling */
	}

	/* the above call could change the ioctl lock status */
	if (pciedev->ioctl_lock == FALSE) {
		pciedev_read_host_buffer(pciedev, pciedev->htod_rx); /* starts pulling */
		pciedev_schedule_flow_ring_read_buffer(pciedev);
	}

	return FALSE;
} /* pciedev_msgbuf_intr_process */

/**
 * After the host notified the device that new message(s) are available in host memory, the device
 * needs to pull these message(s) into local memory. This function kicks off the DMA transfer for
 * that. When this function returns, DMA can still be in progress.
 *
 * msgbuf: a ring of local buffers. The '*pciedev' device contains multiple message rings.
 */
static bool
pciedev_read_host_buffer(struct dngl_bus *pciedev, msgbuf_ring_t *msgbuf)
{
#ifdef PCIE_DEBUG_CYCLE_COUNT
	static uint32 cbuf_time = 0, tot_time = 0;
	static int iter = 0;
	uint32 start_time = osl_getcycles();
#endif /* PCIE_DEBUG_CYCLE_COUNT */

	uint8 *dest_addr;
	uint8 *src_addr;
	uint16 src_len;
	uint32 txdesc, rxdesc;
	int dma_qavail;

	if (pciedev->in_d3_suspend) {
		return FALSE;
	}

	while (1) {
		/* First check if there is any data available in the circular buffer */
		if (READ_AVAIL_SPACE(DNGL_RING_WPTR(msgbuf), msgbuf->rd_pending,
			RING_MAX_ITEM(msgbuf)) == 0)
			return FALSE;

		/* Before proceeding with circular buffer, check if enough descrs are available */
		/* minimum of 2 desc are required per messages */
		/* another 2 are required for ioctl request */
		txdesc = PCIEDEV_GET_AVAIL_DESC(pciedev, HTOD, TXDESC);
		rxdesc = PCIEDEV_GET_AVAIL_DESC(pciedev, HTOD, RXDESC);
		dma_qavail = pciedev_htoddma_queue_avail(pciedev);
		if (txdesc < MIN_TXDESC_AVAIL || rxdesc < MIN_RXDESC_AVAIL || dma_qavail <= 1) {
			PCI_TRACE(("resource not avialable. txdesc %d rxdesc %d dma_qavail %d\n",
				txdesc, rxdesc, dma_qavail));
			return TRUE;
		}

		if (!LCL_BUFPOOL_AVAILABLE(msgbuf)) {
			PCI_TRACE(("Ring: %c%c%c%c Local ring bufs not available,"
				"Dont read out from host ring \n",
				msgbuf->name[0], msgbuf->name[1],
				msgbuf->name[2], msgbuf->name[3]));
			return TRUE;
		}

		/* Update the local copy of the write/end ptr with the shared register's value */
		src_addr = pciedev_get_src_addr(msgbuf, &src_len, 0);
		if (src_addr == NULL) {
			return FALSE;
		}

		/* Get from pool of local buffers */
		dest_addr = pciedev_get_lclbuf_pool(msgbuf);
		if (dest_addr != NULL) {
			/* Now we have the src details as well the dest details to copy/dma the data
			 * from htod_msgbuf into the local_htod msgbuf. Schedule the DMA now.
			 */
			dma64addr_t haddr;

			pciedev->msg_pending++;

			PHYSADDR64HISET(haddr,
				(uint32) HIGH_ADDR_32(msgbuf->ringmem->base_addr));
			PHYSADDR64LOSET(haddr, (uint32) src_addr);

			/* kicks off dma */
			pciedev_h2dmsgbuf_dma(pciedev, haddr, src_len, dest_addr,
				pciedev->dummy_rxoff, msgbuf, MSG_TYPE_API_MAX_RSVD);
		} else {
			PCI_TRACE(("Ring: Local ring get failed %d \n",	__LINE__));
			return TRUE;
		}
	}

	return FALSE;
} /* pciedev_read_host_buffer */

/**
 * Host can request device to 'flush' a flow ring, by issuing either a 'flush' or a 'delete'
 * message. When the ring is flushed, firmware has to send a 'flush complete' message to the host.
 *
 * If we have pending flush and need to drain more, fetch those; otherwise if there are pending
 * flush/delete response then send them now.
 */
void
pciedev_process_pending_flring_resp(struct dngl_bus * pciedev, msgbuf_ring_t *flow_ring)
{
	/**
	 * Indicates if flush response should be sent. If ioctl_pend is set, set retry indications
	 * so that sending flush response will be retried using pciedev_handle_pending_resp().
	 */
	bool send_flring_flush_resp = FALSE;
	/**
	 * Indicates if delete response should be sent. If ioctl_pend is set, set retry indications
	 * so that sending delete response will be retried using pciedev_handle_pending_resp().
	 */
	bool send_flring_delete_resp = FALSE;

	if (((DNGL_RING_WPTR(flow_ring) == DNGL_RING_RPTR(flow_ring)) ||
		(DNGL_RING_WPTR(flow_ring) == flow_ring->rd_pending)) &&
		!flow_ring->flow_info.pktinflight && !flow_ring->fetch_pending) {
		flow_ring->status &= ~FLOW_RING_FLUSH_PENDING;
		if (flow_ring->status & FLOW_RING_FLUSH_RESP_PENDING) {
			send_flring_flush_resp = TRUE;
		} else if (flow_ring->status & FLOW_RING_DELETE_RESP_PENDING) {
			send_flring_delete_resp = TRUE;
		}
	}

	if (pciedev->ioctl_pend) {
		if (!(flow_ring->status &
			(FLOW_RING_FLUSH_RESP_RETRY | FLOW_RING_DELETE_RESP_RETRY))) {
			if (send_flring_flush_resp == TRUE) {
				flow_ring->status |= FLOW_RING_FLUSH_RESP_RETRY;
				pciedev->ctrl_resp_q->num_flow_ring_flush_resp_pend++;
			} else if (send_flring_delete_resp == TRUE) {
				flow_ring->status |= FLOW_RING_DELETE_RESP_RETRY;
				pciedev->ctrl_resp_q->num_flow_ring_delete_resp_pend++;
			}
		}
		return;
	}

	if (send_flring_flush_resp == TRUE) {
		pciedev_send_flow_ring_flush_resp(pciedev, flow_ring->ringid,
		BCMPCIE_SUCCESS);
	} else if (send_flring_delete_resp == TRUE) {
		pciedev_send_flow_ring_delete_resp(pciedev, flow_ring->ringid,
		BCMPCIE_SUCCESS);
	} else {
		pciedev_read_flow_ring_host_buffer(pciedev, flow_ring,
			pciedev->fetch_pending);
	}
}

/** to delete elements from flowrings on timeout */
void
pciedev_flow_ageing_timerfn(dngl_timer_t *t)
{
	struct dngl_bus *pciedev = (struct dngl_bus *) hnd_timer_get_ctx(t);
	msgbuf_ring_t	*flow_ring;
	struct dll * cur, * prio;

	/* loop through active queues */
	prio = dll_head_p(&pciedev->active_prioring_list);
	while (!dll_end(prio, &pciedev->active_prioring_list)) {
		prioring_pool_t *prioring = (prioring_pool_t *)prio;
		cur = dll_head_p(&prioring->active_flowring_list);
		while (!dll_end(cur, &prioring->active_flowring_list)) {
			flow_ring = ((flowring_pool_t *)cur)->ring;
			if (!flow_ring->reuse_seq_list) {
				cur = dll_next_p(cur);
				continue;
			} else {
				if (flow_ring->ring_ageing_info.sup_cnt == 0) {
					MFREE(pciedev->osh, flow_ring->reuse_seq_list,
					flow_ring->bitmap_size * sizeof(uint16));
					flow_ring->reuse_seq_list = NULL;
					pciedev->flow_supp_enab--;
					bzero(flow_ring->reuse_sup_seq, /* d11 seq numbers */
						flow_ring->bitmap_size/NBBY);
					PCI_TRACE(("\n DELETING : reuse_seq_list"));
				} else
					flow_ring->ring_ageing_info.sup_cnt = 0;
			}
			cur = dll_next_p(cur);
		}

		/* get next priority ring node */
		prio = dll_next_p(prio);
	}

	if (!pciedev->flow_supp_enab) {
		PCI_TRACE(("\n DELETING : flow_ageing_timer"));
		dngl_del_timer(pciedev->flow_ageing_timer);
		pciedev->flow_ageing_timer_on = FALSE;
	}
}

/** Normalize raw weight (airtime) from WL layer to pciedev weight.
 * Since smaller airtime is corresponds to "heavier" weight
 * therefore need to do invert calculation to derive the weight
 * from the airtime.
 */
static inline void
pciedev_normalize_flow_ring_weight(struct dngl_bus *pciedev,
		uint16 *sch_flows, uint16 num)
{
	uint16 idx, i;
	uint32 W, w;
	msgbuf_ring_t *flow_ring;
#ifdef WLATF_PERC
	uint16 max_perc = 0;
#endif /*  WLATF_PERC */

	for (W = 0, i = 0; i < num; i++) {
		idx = sch_flows[i];
		flow_ring = &pciedev->flow_ring_msgbuf[idx];
		w = SCHEDCXT_SUM_W(pciedev) / SCHEDCXT_FL_W(flow_ring);
		W += w;
		SCHEDCXT_FL_W(flow_ring) = w;
#ifdef WLATF_PERC
		if (SCHEDCXT_ATM_PERC_SUM(pciedev)) {
			SCHEDCXT_ATM_PERC(flow_ring) = (ATM_PERC(flow_ring)*100)/
				SCHEDCXT_ATM_PERC_SUM(pciedev);
		}
		if(SCHEDCXT_ATM_PERC(flow_ring) > max_perc) {
			max_perc = SCHEDCXT_ATM_PERC(flow_ring);
		}
#endif /*  WLATF_PERC */
	}
#ifdef WLATF_PERC
	pciedev->atm_max_perc = max_perc;
#endif /*  WLATF_PERC */
	if (num > 0) {
		SCHEDCXT_SUM_W(pciedev) = W;
		ASSERT(SCHEDCXT_SUM_W(pciedev));
	}
}

static inline void
pciedev_ffsch_reset_weight(struct dngl_bus *pciedev, msgbuf_ring_t *flow_ring,
	uint16 *sch_flows_idx, uint16 *sch_flows_num)
{
	if (!(flow_ring->status & FLOW_RING_PKT_RESET)) {
		uint32 now = hnd_time();

		if (!(flow_ring->status & FLOW_RING_PKT_IDLE)) {
			flow_ring->status |= FLOW_RING_PKT_IDLE;
			flow_ring->idletimer = now;
		}

		if (DELTA(now, flow_ring->idletimer) < pciedev->ffsched_flr_rst_delay) {
			/* sum raw weight */
			SCHEDCXT_SUM_W(pciedev) += FL_W_NEW(flow_ring) *
				FLOW_RING_WEIGHT_SCALE;
			/* scheduling cycle context weight */
			SCHEDCXT_FL_W(flow_ring) = FL_W_NEW(flow_ring);
			sch_flows_idx[(*sch_flows_num)++] = FLRING_INX(FL_ID(flow_ring));
#ifdef WLATF_PERC
			SCHEDCXT_ATM_PERC_SUM(pciedev) += ATM_PERC(flow_ring);
			SCHEDCXT_ATM_PERC(flow_ring) = ATM_PERC(flow_ring);
#endif /*  WLATF_PERC */
		}
		else {
			pciedev_reset_flowring_weight(pciedev, flow_ring);
		}
	}
	return;
}

/** Temp PULL model/partial push : Check for write pointer updates from all data flow rings */
void
pciedev_schedule_flow_ring_read_buffer(struct dngl_bus *pciedev)
{
	uint32 w_ptr;
	bool flow_schedule = FALSE;
	msgbuf_ring_t *flow_ring;
	struct dll *cur, *prio;
	uint16 sch_flows_idx[BCMPCIE_MAX_TX_FLOWS];
	uint16 sch_flows_num = 0;

	/* The scheduler would changes the state for FF scheduler based
	 * on whether the FF Scheduler feature is disabled or enabled.
	 */
	if (!FFSHCED_ENAB(pciedev) &&
		(FFSHCED_ST_ENABLED(pciedev) ||	FFSHCED_ST_UNKNOWN(pciedev)))
		FFSHCED_ST(pciedev) = FFSCH_STATE_DISABLED;
	else if (FFSHCED_ENAB(pciedev) &&
		(FFSHCED_ST_DISABLED(pciedev) || FFSHCED_ST_UNKNOWN(pciedev)))
		FFSHCED_ST(pciedev) = FFSCH_STATE_ENABLED;

	if (pciedev->in_d3_suspend || pciedev->flow_sch_timer_on) {
		return;
	}

	/* get first priority ring out of pool */
	prio = dll_head_p(&pciedev->active_prioring_list);

	SCHEDCXT_SUM_W(pciedev) = 0;
#ifdef WLATF_PERC
	SCHEDCXT_ATM_PERC_SUM(pciedev) = 0;
#endif /*  WLATF_PERC */
	/* loop through all active priority rings */
	while (!dll_end(prio, &pciedev->active_prioring_list)) {
		prioring_pool_t *prioring = (prioring_pool_t *)prio;

		prioring->schedule = FALSE;

		/* get first flow ring out of pool */
		cur = dll_head_p(&prioring->active_flowring_list);

		/* loop through all active flow rings */
		while (!dll_end(cur, &prioring->active_flowring_list)) {
			flow_ring = ((flowring_pool_t *)cur)->ring;
			w_ptr = DNGL_RING_WPTR(flow_ring);
			SCHEDCXT_FL_W(flow_ring) = 0;
#ifdef WLATF_PERC
			SCHEDCXT_ATM_PERC(flow_ring) = 0;
#endif /*  WLATF_PERC */
			/* Check write pointer of the flow ring and mark it for pending pkt pull */
			if (!READ_AVAIL_SPACE(w_ptr, flow_ring->rd_pending,
				RING_MAX_ITEM(flow_ring))) {

				/* Clearing pending bit in case if it is there */
				flow_ring->status &= ~FLOW_RING_PKT_PENDING;

				/* No packets for this flowring.
				 * Setting PKT_IDLE status bit.
				 * Reseting the weight to initial default.
				 */
				if (FFSHCED_ST_ENABLED(pciedev)) {
					pciedev_ffsch_reset_weight(pciedev, flow_ring,
						sch_flows_idx, &sch_flows_num);
				}
				cur = dll_next_p(cur);
				continue;
			}

			flow_ring->status |= FLOW_RING_PKT_PENDING;
			prioring->schedule = TRUE;

			if (FFSHCED_ST_ENABLED(pciedev)) {
				flow_ring->status &= ~(FLOW_RING_PKT_IDLE | FLOW_RING_PKT_RESET);

				flow_ring->idletimer = 0;

				/* sum raw weight */
				SCHEDCXT_SUM_W(pciedev) += FL_W_NEW(flow_ring) *
					FLOW_RING_WEIGHT_SCALE;
				/* scheduling cycle context weight */
				SCHEDCXT_FL_W(flow_ring) = FL_W_NEW(flow_ring);
				sch_flows_idx[sch_flows_num++] = FLRING_INX(FL_ID(flow_ring));
#ifdef WLATF_PERC
				SCHEDCXT_ATM_PERC_SUM(pciedev) += ATM_PERC(flow_ring);
				SCHEDCXT_ATM_PERC(flow_ring) = ATM_PERC(flow_ring);
#endif /*  WLATF_PERC */
			}

			/* get next flow ring node */
			cur = dll_next_p(cur);
		}

		/* adjust maxpktcnt to make the hightest priority ring use max fetch count,
		 *  and others use the min fetch count
		 */
		if (!flow_schedule) {
			if (prioring->maxpktcnt < PCIEDEV_MAX_PACKETFETCH_COUNT &&
				prioring->schedule) {
				prioring->maxpktcnt += PCIEDEV_INC_PACKETFETCH_COUNT;
				if (prioring->maxpktcnt > PCIEDEV_MAX_PACKETFETCH_COUNT)
					prioring->maxpktcnt = PCIEDEV_MAX_PACKETFETCH_COUNT;
			}
		} else {
			prioring->maxpktcnt = PCIEDEV_MIN_PACKETFETCH_COUNT;
		}

		if (prioring->schedule)
			flow_schedule = TRUE;

		/* get next priority ring node */
		prio = dll_next_p(prio);
	}

	if (flow_schedule && !pciedev->flow_sch_timer_on) {
		if (FFSHCED_ST_ENABLED(pciedev)) {
			if (sch_flows_num > 0)
				pciedev_normalize_flow_ring_weight(pciedev, &sch_flows_idx[0],
				sch_flows_num);
		}
		dngl_add_timer(pciedev->flow_schedule_timer, 0, FALSE);
		pciedev->flow_sch_timer_on = TRUE;
	}
} /* pciedev_schedule_flow_ring_read_buffer */

/**
 * Calculating the max lfrag budget to the flowring based in the weight
 */
#ifdef BCMFRAGPOOL
static inline void
pciedev_flow_ring_calc_maxlfrags(struct dngl_bus *pciedev,
	msgbuf_ring_t *flow_ring)
{
	ASSERT(SCHEDCXT_FL_W(flow_ring));
	ASSERT(SCHEDCXT_SUM_W(pciedev));

	SCHEDCXT_FL_MAXLFRAGS(flow_ring) = pktpool_maxlen(pciedev->pktpool_lfrag);

	if (SCHEDCXT_SUM_W(pciedev) > SCHEDCXT_FL_W(flow_ring)) {

		/* Shaping max lfrag count */
		SCHEDCXT_FL_MAXLFRAGS(flow_ring) =
			(SCHEDCXT_FL_MAXLFRAGS(flow_ring) * SCHEDCXT_FL_W(flow_ring)) /
			SCHEDCXT_SUM_W(pciedev);
	}
	if (flow_ring->lfrag_max < pciedev->flr_lfrag_txpkts_min)
		flow_ring->lfrag_max = pciedev->flr_lfrag_txpkts_min;

	if (SCHEDCXT_FL_MAXLFRAGS(flow_ring) >  flow_ring->lfrag_max) {
		SCHEDCXT_FL_MAXLFRAGS(flow_ring) = flow_ring->lfrag_max;
	}
	if ((pciedev->flr_lfrag_max != 0) &&
		(SCHEDCXT_FL_MAXLFRAGS(flow_ring) > pciedev->flr_lfrag_max)) {
		SCHEDCXT_FL_MAXLFRAGS(flow_ring) = pciedev->flr_lfrag_max;
	}
#ifdef WLATF_PERC
	if (pciedev->_ffsched == FFSCH_MODE_PERC) {
		SCHEDCXT_FL_MAXLFRAGS(flow_ring) = (SCHEDCXT_FL_MAXLFRAGS(flow_ring)*
				SCHEDCXT_ATM_PERC(flow_ring))/(pciedev->atm_max_perc);
	}
#endif /*  WLATF_PERC */
	if (SCHEDCXT_FL_MAXLFRAGS(flow_ring) == 0) {
		SCHEDCXT_FL_MAXLFRAGS(flow_ring) = PCIEDEV_MIN_SHCEDLFRAG_COUNT;
	}
}
#endif /* BCMFRAGPOOL */

/**
 * One shot timer function, this timer callback was scheduled for Data Flow rings.
 * Pull the packets from the host and send up as required
 */
void
pciedev_flow_schedule_timerfn(dngl_timer_t *t)
{
	struct dngl_bus *pciedev = (struct dngl_bus *) hnd_timer_get_ctx(t);
	msgbuf_ring_t	*flow_ring;
	struct dll * cur, * prio;
	int	fetch = 1;
	bool ret;
	bool any_fetch = FALSE;
	dll_t* last_fetch_node;
	uint16 pciedev_fetch_pending;
	bool do_fetch = TRUE;

	pciedev->flow_sch_timer_on = FALSE;

	if (pciedev->in_d3_suspend) {
		return;
	}

	/* always start from the highest priority rings */
	prio = dll_head_p(&pciedev->active_prioring_list);
	pciedev_fetch_pending = pciedev->fetch_pending;

	/* loop through all active priority rings */
	while (!dll_end(prio, &pciedev->active_prioring_list) && do_fetch) {
		prioring_pool_t *prioring = (prioring_pool_t *)prio;

		if (!prioring->schedule) {
			/* all flowrings in the prio are empty, skip and move on */
			goto nextprio;
		}

		/* start from the last fetch node to be fair to all rings */
		cur = dll_next_p(prioring->last_fetch_node);
		last_fetch_node = prioring->last_fetch_node;

		/* loop through all nodes */
		while (do_fetch) {
			/* skip the head node which does not hold any info */
			if (dll_end(cur, &prioring->active_flowring_list)) {
				goto nextnode;
			}

			/* get flow ring from nodes */
			flow_ring = ((flowring_pool_t *)cur)->ring;

			if (!flow_ring->flow_info.pktinflight &&
				!flow_ring->fetch_pending)
				flow_ring->status &= ~FLOW_RING_SUP_PENDING;

			if (flow_ring->status & FLOW_RING_SUP_PENDING)
				goto nextnode;

			flow_ring->flow_info.maxpktcnt = prioring->maxpktcnt;

			if ((flow_ring->status & FLOW_RING_PORT_CLOSED)) {
				if ((flow_ring->status & FLOW_RING_PKT_PENDING) &&
					(flow_ring->flow_info.flags
					& FLOW_RING_FLAG_INFORM_PKTPEND) &&
					!(flow_ring->flow_info.flags
					& FLOW_RING_FLAG_LAST_TIM)) {
					/* informs WL subsystem about the TIM_SET operation */
					uint8 tid_ac = flow_ring->flow_info.tid_ac;
					if (pciedev->schedule_prio == PCIEDEV_SCHEDULER_TID_PRIO)
						tid_ac =
						PCIEDEV_TID2AC(flow_ring->flow_info.tid_ac);

					ret = dngl_flowring_update(pciedev->dngl,
						flow_ring->flow_info.ifindex,
						(uint8) flow_ring->handle,
						FLOW_RING_TIM_SET,
						(uint8 *)&flow_ring->flow_info.sa,
						(uint8 *)&flow_ring->flow_info.da, tid_ac);

					if (ret) {
						flow_ring->flow_info.flags |=
							FLOW_RING_FLAG_LAST_TIM;
						goto nextnode;
					} else {
						flow_ring->flow_info.maxpktcnt = 1;
						flow_ring->flow_info.reqpktcnt += 1;
						flow_ring->flow_info.flags |=
							FLOW_RING_FLAG_PKT_REQ;
					}
				} else if (!(flow_ring->status & FLOW_RING_FLUSH_PENDING)) {
					/* Keep flushing packets even if its closed */
					goto nextnode;
				}
			}

			PCI_TRACE(("h2d FLOW RING %d write %p  read %p  WI %d R I%d\n",
				flow_ring->ringid,
				flow_ring->tcm_rs_w_ptr,
				flow_ring->tcm_rs_r_ptr,
				DNGL_RING_WPTR(flow_ring),
				flow_ring->rd_pending));

			if (!(flow_ring->status & FLOW_RING_PKT_PENDING))
				goto nextnode;

			/* assigning max budget of lfrags based on the weight. */
			if (FFSHCED_ST_ENABLED(pciedev) && SCHEDCXT_FL_W(flow_ring)) {
#ifdef BCMFRAGPOOL
				pciedev_flow_ring_calc_maxlfrags(pciedev, flow_ring);
#endif /* BCMFRAGPOOL */
			}

			/* Read out messages from the flow ring */
			FL_STATUS(flow_ring) &= ~FLOW_RING_FETCH_SKIPPED;
			fetch = pciedev_read_flow_ring_host_buffer(pciedev, flow_ring,
				pciedev_fetch_pending);

			/* Fetch may be skipped for flow ring that do not have enough credit
			 * in this scheduling cycle. However others may have and therefore
			 * need to be fetched in this cycle.
			 */
			if ((FL_STATUS(flow_ring) & FLOW_RING_FETCH_SKIPPED)) {
				FL_STATUS(flow_ring) &= ~FLOW_RING_FETCH_SKIPPED;
			}
			else
				do_fetch = (fetch > 0);

			if (fetch) {
				any_fetch = TRUE;
				prioring->last_fetch_node = cur;
			}

			flow_ring->status &= ~FLOW_RING_PKT_PENDING;

	nextnode:
			if (cur == last_fetch_node)
				break;

			cur = dll_next_p(cur);
		}

	nextprio:
		/* get next priority ring node */
		prio = dll_next_p(prio);
	}

	if (any_fetch)
		pciedev_schedule_flow_ring_read_buffer(pciedev);
} /* pciedev_flow_schedule_timerfn */

/**
 * Get max number of packets that can be fetched from the ring.
 * fetch_pending - is pciedev fetch pending packet number.
 * NOTE: This routine also called from scheduler loop for each
 * flowring. Thus in this case fetch_pending must be the pciedev
 * fetch pending packets before starting the loop. For correct
 * lfrag pool shaping per each flowring the fetch_pending should
 * remain the same during entire scheduling loop.
 */
static uint
pciedev_get_maxpkt_fetch_count(struct dngl_bus *pciedev, msgbuf_ring_t *flow_ring,
		uint16 fetch_pending, uint16 availcnt)
{
	uint32 lbuf_avail = 0, ret_len;
#ifdef FLOW_RING_LAZY_FETCH
	uint8 read_avail_space = 0;
	uint32 current_time = hnd_time();
	bool wrap_around = FALSE;
#endif /* FLOW_RING_LAZY_FETCH */
#if defined(FLOWRING_SLIDING_WINDOW) || defined(FLOWRING_USE_SHORT_BITSETS)
	uint32 inuse;
	uint32 available;
#endif // endif

	/* if we are flushing packets, fetch all & send tx status bypassing wl */
	if (flow_ring->status & FLOW_RING_FLUSH_PENDING) {
		ret_len = MIN(READ_AVAIL_SPACE(DNGL_RING_WPTR(flow_ring),
			flow_ring->rd_pending, RING_MAX_ITEM(flow_ring)), availcnt);

#if defined(FLOWRING_SLIDING_WINDOW) || defined(FLOWRING_USE_SHORT_BITSETS)
		/* For sliding windows/short bitmaps, let's make sure max_fetch
		 * does not include RDP - RD.
		 */
		if (flow_ring->rd_pending >= DNGL_RING_RPTR(flow_ring))  {
			inuse = flow_ring->rd_pending - DNGL_RING_RPTR(flow_ring);
		} else {
			inuse = RING_MAX_ITEM(flow_ring) - DNGL_RING_RPTR(flow_ring) +
				flow_ring->rd_pending;
		}
		available = flow_ring->bitmap_size - inuse - 1;
		ret_len = MIN((int) ret_len, (int) available);
#endif /* FLOWRING_SLIDING_WINDOW || FLOWRING_USE_SHORT_BITSETS */

		goto done;
	}

#ifdef BCMFRAGPOOL
	lbuf_avail = pktpool_avail(pciedev->pktpool_lfrag);
#ifdef BCM_DHDHDR
	if (BCMDHDHDR_ENAB())
		lbuf_avail = MIN(lbuf_avail, lfbufpool_avail(pciedev->d3_lfbufpool));
#endif /* BCM_DHDHDR */

	/* If there is no lbuf/lfrags wait for freeups */
	if (!lbuf_avail) {
		EVENT_LOG(EVENT_LOG_TAG_PCI_DBG,
			"No lbus/lfrags Available\n");
		return 0;
	}

	if (FFSHCED_ST_DISABLED(pciedev))
		fetch_pending = pciedev->fetch_pending;

	/* Deducting fetch_pending */
	if (lbuf_avail > fetch_pending) {
		lbuf_avail -= fetch_pending;
	} else {
		return 0;
	}
#endif /* BCMFRAGPOOL */
	/* Fetch packets only if we have enough lbuf/pktpool lbuf_avail */
	ret_len = lbuf_avail;

	/* Adjusting the lfrags credit for given flowring.
	 * If credit is not enough then skipping pkt fetch.
	 */
#ifdef BCMFRAGPOOL
	if (FFSHCED_ST_ENABLED(pciedev) && SCHEDCXT_FL_W(flow_ring)) {
		uint16 tot_pkts = FL_PKTINFLIGHT(flow_ring) + flow_ring->fetch_pending;

		ASSERT(SCHEDCXT_SUM_W(pciedev));
		if (tot_pkts < SCHEDCXT_FL_MAXLFRAGS(flow_ring)) {
			/* If lfrag credit limit is not crossed */
			uint16 lfrags_credit = SCHEDCXT_FL_MAXLFRAGS(flow_ring) - tot_pkts;

			ret_len = MIN(ret_len, lfrags_credit);
		} else {
			/* The credit limit is crossed for this flowring. Skipping fetch. */
			FL_STATUS(flow_ring) |= FLOW_RING_FETCH_SKIPPED;
			return 0;
		}
	}
#endif /* BCMFRAGPOOL */

	/* If the fetch count is less than fetch count threshold and inflight
	 * packets are more than low water mark check, then let's avoid releasing
	 * this chain in order to facilitate better aggregation.
	 */
	if (ret_len < pciedev->fetch_cnt_threshold) {
		if ((FL_PKTINFLIGHT(flow_ring) + flow_ring->fetch_pending) >=
			pciedev->inflight_low_water_mark) {
			FL_STATUS(flow_ring) |= FLOW_RING_FETCH_SKIPPED;
			return 0;
		}
	}

#ifdef FLOW_RING_LAZY_FETCH
	if (pciedev->lazy_fetch_enabled) {
		/* 1. Enable lazy fetch for BK or BE only and
		 *    when active ring > LAZY_FETCH_MIN_ACTIVE_RING.
		 * 2. If READ_AVAIL_SPACE or ret_len is < LAZY_FETCH_WATERMARK
		 *    do lazy flow_ring fetch to make sure we get better aggregation.
		 * 3. Still tx the packet if wait time > LAZY_FETCH_MAX_WAIT_TIME.
		 * 4. To avoid any non-burst packet (e.g. ping) delay as possible,
		 *    skip lazy flow_ring fetch if last active time - current_time >
		 *    LAZY_FETCH_MIN_DELTA_TIME
		 * 5. If there is wrap_around and lfrags are available skip lazy fetch
		 */
		read_avail_space = READ_AVAIL_SPACE(DNGL_RING_WPTR(flow_ring),
			flow_ring->rd_pending, RING_MAX_ITEM(flow_ring));

		if (flow_ring->rd_pending > DNGL_RING_WPTR(flow_ring)) {
			wrap_around = TRUE;
		}

		if (!(wrap_around && (ret_len >= read_avail_space)) &&
			(pciedev->lazy_fetch_max_wait_time &&
			(DELTA(current_time, flow_ring->lazy_fetch_last_active_time) <
			LAZY_FETCH_MIN_DELTA_TIME) &&
			PCIDEV_IS_BK_BE_FLOW_RING(pciedev, flow_ring) &&
			((read_avail_space && read_avail_space < LAZY_FETCH_WATERMARK) ||
			(ret_len && ret_len < LAZY_FETCH_WATERMARK)))) {
			if (!flow_ring->lazy_fetch_start_time) {
				/* start to do lazy fetch and save timestamp for this ring */
				flow_ring->lazy_fetch_start_time = current_time;
				FL_STATUS(flow_ring) |= FLOW_RING_FETCH_SKIPPED;
				return 0;
			} else {
				if (DELTA(current_time, flow_ring->lazy_fetch_start_time) <
					pciedev->lazy_fetch_max_wait_time) {
					FL_STATUS(flow_ring) |= FLOW_RING_FETCH_SKIPPED;
					return 0;
				} else
					flow_ring->lazy_fetch_start_time = 0;
			}
		} else {
			if (flow_ring->lazy_fetch_start_time)
				flow_ring->lazy_fetch_start_time = 0;
		}

		if (read_avail_space)
			flow_ring->lazy_fetch_last_active_time = current_time;
	}
#endif /* FLOW_RING_LAZY_FETCH */

	/* Cap till contiguous available buffer */
	ret_len = MIN(ret_len, READ_AVAIL_SPACE(DNGL_RING_WPTR(flow_ring),
	           flow_ring->rd_pending, RING_MAX_ITEM(flow_ring)));

	/* Do not fetch more than availcnt in circular buffer */
	ret_len = MIN(ret_len, availcnt);
#if defined(FLOWRING_SLIDING_WINDOW) || defined(FLOWRING_USE_SHORT_BITSETS)
	/* For sliding windows/short bitmaps, let's make sure max_fetch
	 * does not include RDP - RD.
	 */
	if (flow_ring->rd_pending >= DNGL_RING_RPTR(flow_ring))  {
		inuse = flow_ring->rd_pending - DNGL_RING_RPTR(flow_ring);
	} else {
		inuse = RING_MAX_ITEM(flow_ring) - DNGL_RING_RPTR(flow_ring) +
			flow_ring->rd_pending;
	}
	available = flow_ring->bitmap_size - inuse - 1;
	ret_len = MIN((int) ret_len, (int) available);
#endif /* FLOWRING_SLIDING_WINDOW || FLOWRING_USE_SHORT_BITSETS */

done:

	/* Cannot request a M2M DMA for big data (>16K) transation. */
	if (ret_len * RING_LEN_ITEMS(flow_ring) > PCIEDEV_MAX_PACKETFETCH_LEN)
		ret_len = PCIEDEV_MAX_PACKETFETCH_LEN / RING_LEN_ITEMS(flow_ring);

	return ret_len;

}

void pciedev_upd_flr_weight(struct dngl_bus * pciedev, uint8 mac_handle, uint8 ac_tid, void *params)
{
	msgbuf_ring_t	*flow_ring;
	struct dll * cur, * prio;
	bool updated = FALSE;

	/* loop through nodes */
	prio = dll_head_p(&pciedev->active_prioring_list);
	while (!dll_end(prio, &pciedev->active_prioring_list)) {
		prioring_pool_t *prioring = (prioring_pool_t *)prio;
		cur = dll_head_p(&prioring->active_flowring_list);
		while (!dll_end(cur, &prioring->active_flowring_list)) {
			flow_ring = ((flowring_pool_t *)cur)->ring;
			if (flow_ring->handle != mac_handle) {
				cur = dll_next_p(cur);
				continue;
			}
			if (flow_ring->flow_info.tid_ac != ac_tid) {
				cur = dll_next_p(cur);
				continue;
			}

			if (params != NULL) {
				flowring_op_param_t *op = (flowring_op_param_t *)params;
				uint32 new_w = op->weight;
				/* updating with new weight value */
				FL_W_NEW(flow_ring) = new_w;
#ifdef WLATF_PERC
				ATM_PERC(flow_ring) = op->atm_perc;
#endif /*  WLATF_PERC */
				flow_ring->phyrate = op->phyrate;
				flow_ring->mumimo = op->mumimo;

				if (!(flow_ring->status & FLOW_RING_PKT_RESET)) {
					flow_ring->lfrag_max = op->lfrag_max;

					/* adjust lfrag_max based on txpkts in period
					 * (50/3=17ms).
					 * PCIEDEV_SHCEDLFRAG_TXPKTS_ADJUST = 3
					 */
					if (pciedev->flr_lfrag_txpkts_adjust &&
							!(flow_ring->mumimo)) {
						flow_ring->lfrag_max /=
							pciedev->flr_lfrag_txpkts_adjust;
					}
					else {
						if (pciedev->flr_lfrag_txpkts_adjust &&
								flow_ring->mumimo) {
							flow_ring->lfrag_max /=
								pciedev->flr_lfrag_txpkts_adjust_mu;
						}
					}
				}

				updated = TRUE;

				/* For DWDS clients 2 sets of flowrings are created.
				 * One set under AP interface and other set under WDS interface.
				 * Weights should be updated for both sets.
				 */
				if (!op->dwds) {
					break;
				}
			}

			/* Next node */
			cur = dll_next_p(cur);
		}
		if (updated)
			break;
		/* get next priority ring node */
		prio = dll_next_p(prio);
	}
}

/** Enable/Disable Fair Fetch Scheduling Feature */
void pciedev_set_ffsched(struct dngl_bus * pciedev, void *params)
{
	if (params != NULL) {
		uint32 ffsched = *(uint32*)params;

		if (ffsched == FFSHCED_ENAB(pciedev)) {
			return;
		}
		PCI_TRACE(("%s: FFS(ATF) change state to %d\n", __FUNCTION__, ffsched));

		if (ffsched) {
			pciedev_reset_all_flowrings_weight(pciedev);
		}
		pciedev->_ffsched = ffsched;
	}
}

/** Reset the weight of the given active flow ring to default */
void
pciedev_reset_flowring_weight(struct dngl_bus *pciedev,
	msgbuf_ring_t *flow_ring)
{
	int weight;

	if (flow_ring->inited) {
		weight = dngl_flowring_update(
			pciedev->dngl, FL_TID_IFINDEX(flow_ring),
			0, FLOW_RING_RESET_WEIGHT, NULL,
			(uint8 *)&(FL_DA(flow_ring)), FL_TID_AC(flow_ring));

		/* XXX: The weight (airtime) never supposed to be 0.
		 * XXX: The only case it would happen due to another bugs.
		 * XXX: Assigning bare minimum weight.
		 */
		if (weight <= 0)
			weight = 1;

		FL_W_NEW(flow_ring) = (uint32) weight;

		flow_ring->status |= FLOW_RING_PKT_RESET;
	}
}

/** Reset the weight of all active flow rings to default */
void
pciedev_reset_all_flowrings_weight(struct dngl_bus *pciedev)
{
	uint16 i;
	for (i = 0; i < pciedev->max_tx_flows; i++)
		pciedev_reset_flowring_weight(pciedev, &pciedev->flow_ring_msgbuf[i]);
}

/*
 * Decide how many packets/msdus to be fetched, during client PS.
 * Independent of AGG SF value in WLC layer.
 * Assumption: FW will not ask for more than one A-MSDU size for
 * each PS POLL.
 */
static uint16
pciedev_get_psmode_fetch_count_amsdu(struct dngl_bus *pciedev,
		msgbuf_ring_t *flow_ring, uint16 ret_len)
{
	uint16 depth;
	uint16 r_m, r_n, msdu_count;
	uint16 fw_req_pktcnt; /* Number of packets FW requested. */
	uint16 r_m_seq, r_n_seq;

	depth = RING_MAX_ITEM(flow_ring);
	r_n = flow_ring->rd_pending;

	/*
	 * flow_info.maxpktcnt was set to number of packets FW requested.
	 * This was done while processing  WLFC_CTL_TYPE_MAC_REQUEST_PACKET in function
	 * pciedev_process_reqst_packet.
	 */
	fw_req_pktcnt = FL_MAXPKTCNT(flow_ring);

	/* All this is about reuse of sequence numbers during AMSDU suppression */
	if (!(flow_ring->reuse_sup_seq) || !(flow_ring->reuse_seq_list)) {
		return MIN(ret_len, fw_req_pktcnt);
	}

	/*
	 * If not suppressed, find out how many non suppressed consequtive
	 * packets. Still cannot send more than requsted,
	 */
	if (!isset(flow_ring->reuse_sup_seq, MODULO_RING_IDX(r_n, flow_ring))) {
		msdu_count = 1;
		while (msdu_count < fw_req_pktcnt) {
			r_m = (r_n + 1) % depth;
			r_n_seq = flow_ring->reuse_seq_list[MODULO_RING_IDX(r_n, flow_ring)];
			r_m_seq = flow_ring->reuse_seq_list[MODULO_RING_IDX(r_m, flow_ring)];
			if ((!isset(flow_ring->reuse_sup_seq, MODULO_RING_IDX(r_m, flow_ring))))
			{
				msdu_count++;
				r_n = r_m;
			}
			else
				break;
		}
		/* and cannot fetch more than available resources. */
		msdu_count = MIN(ret_len, msdu_count);
		return msdu_count;
	}

	r_n_seq = flow_ring->reuse_seq_list[MODULO_RING_IDX(r_n, flow_ring)];

	/*
	 * If not part of amsdu, and suppressed
	 * just return 1 packet, while peer/client in PS.
	 */
	if (!WL_SEQ_GET_AMSDU(r_n_seq)) {
		return 1;
	}

	/* Ok, at this point, r_n_seq is suppressed and amsdu */
	msdu_count = 1;

	/* How many MSDUs have same seq # */
	while (msdu_count < depth) {
		r_m = (r_n + msdu_count) % depth;
		r_m_seq = flow_ring->reuse_seq_list[MODULO_RING_IDX(r_m, flow_ring)];
		if (r_n_seq == r_m_seq)
			msdu_count++;
		else
			break;
	}
	/*
	 * FW requested 1 MSDU, found 2 consecutive MSDUs, fetch 2. (msdu_count > fw_req_pktcnt)
	 * FW requested 2 MSDUs, found 2 consecutive MSDUs, fetch 2. (msdu_count == fw_req_pktcnt)
	 * FW requested 2 MSDUs, found only one such suppressed MSDU
	 * which also happens to be a part of A-MSDU, we will not fetch.
	 * This will prevent OOO due to partial A-MSDU being sent. (msdu_count < fw_req_pktcnt)
	 */
	msdu_count = (msdu_count < fw_req_pktcnt) ?  0 : msdu_count;

	ret_len = MIN(ret_len, msdu_count);

	return ret_len;
} /* pciedev_get_psmode_fetch_count_amsdu */

/*
 * In non PS suppression,
 * The idea is to fetch as many packets as possible, but no partial suppressed AMSDU.
 * Independent of wlc side amsdu_aggsf
 * Assumption: All MSDUs of an AMSDU are suppressed together and there are no holes present.
 */
static uint16
pciedev_get_fetch_count_amsdu(struct dngl_bus *pciedev, msgbuf_ring_t *flow_ring, uint16 ret_len)
{
	uint16 r_ptr, depth;
	uint16 r_m, r_n;
	uint16 r_m_seq, r_n_seq, r_n2l_seq;

	ASSERT(ret_len);

	/* Do not fetch more than maxpktcnt packets for this ring */
	ret_len = MIN(ret_len, FL_MAXPKTCNT(flow_ring));

	if (!ret_len)
		return ret_len;

	r_ptr = flow_ring->rd_pending;
	depth = RING_MAX_ITEM(flow_ring);

	r_n = (r_ptr + (ret_len - 1)) % depth;
	/* if last is not suppressed, just return */
	if (!isset(flow_ring->reuse_sup_seq, MODULO_RING_IDX(r_n, flow_ring))) {
		return ret_len;
	}

	r_n_seq = flow_ring->reuse_seq_list[MODULO_RING_IDX(r_n, flow_ring)];

	/* if last is not part of amsdu, just return */
	if (!WL_SEQ_GET_AMSDU(r_n_seq)) {
		return ret_len;
	}

	/* If next to last sequence number is not same */
	r_n2l_seq = flow_ring->reuse_seq_list[MODULO_RING_IDX(((r_n + 1) % depth), flow_ring)];

	if (r_n_seq != r_n2l_seq)
	{
		/* last msdu is part of amsdu, and on amsdu boundary */
		return ret_len;
	}

	/*
	 * If next to last sequence number is same as last sequence number,
	 * last is part of amsdu, and not on amsdu boundary.
	 * Lets move back, till we find a different sequence
	 * number and reduce ret_len accordingly.
	 */
	r_m = r_n;
	while (r_m != r_ptr)
	{
		r_m = ((r_m + (depth - 1)) % depth);
		r_m_seq = flow_ring->reuse_seq_list[MODULO_RING_IDX(r_m, flow_ring)];
		ret_len--;
		if (r_m_seq != r_n_seq)
			break;
	}

	return ret_len;
} /* pciedev_get_fetch_count_amsdu */

/**
 * Transmit related. 'Flow fetch requests' were queued by firmware. This function takes one such
 * request from the queue and kicks off host->device DMA.
 */
int
pciedev_read_flow_ring_host_buffer(struct dngl_bus *pciedev, msgbuf_ring_t *flow_ring,
	uint16 pciedev_fetch_pending)
{
	uint8 *dest_addr;
	uint8 *src_addr;
	uint16 src_len, ret_len;
	int8 i;
	flow_fetch_rqst_t *flowfetchp;

	cir_buf_pool_t *cpool = flow_ring->cbuf_pool;
	uint16 availcnt = CIR_BUFPOOL_AVAILABLE(cpool);

	/* If there is no flow fetch request come later */
	if ((i = pciedev_allocate_flowring_fetch_rqst_entry(pciedev)) < 0) {
		PCI_ERROR(("Fetch Req alloc error read later %d \n", flow_ring->ringid));
		return 0;
	}

	/* check for availability of local circular buffer */
	if (!availcnt) {
		EVENT_LOG(EVENT_LOG_TAG_PCI_DBG,
			"flow_ring_name: %c%c%c%c circular ring bufs not available,"
			"Dont read out from host ring \n",
			flow_ring->name[0], flow_ring->name[1],
			flow_ring->name[2], flow_ring->name[3]);
		goto cleanup;
	}

	flowfetchp = &pciedev->flowring_fetch_rqsts[i];
	flowfetchp->start_ringinx = flow_ring->rd_pending;

	ret_len = pciedev_get_maxpkt_fetch_count(pciedev, flow_ring,
		pciedev_fetch_pending, availcnt);

	/* If next fetched packets are already status_cmpl do not fetch them */
	if (ret_len) {
		ret_len = bcm_count_zeros_sequence(flow_ring->inprocess,
			MODULO_RING_IDX(flowfetchp->start_ringinx, flow_ring),
			ret_len, flow_ring->bitmap_size);
	}

	if (ret_len) {
		if ((flow_ring->status & FLOW_RING_PORT_CLOSED)) {
			/* PS Suppression */
			/*
			 * Lets check if we have any suppressed AMSDU,
			 * and how many MSDUs can we pull.
			 * This is to club all the MSDUs of a suppressed AMSDU,
			 * to be sent as a response to PS POLL.
			 */
			ret_len = pciedev_get_psmode_fetch_count_amsdu(pciedev, flow_ring,
				ret_len);
		} /* FLOW_RING_PORT_CLOSED */
		else
		{
			/* NON-PS Suppression */
			/*
			* ret_len (#ring items planned to fetch) may contain AMSDU items.
			* Make sure we don't have partial suppressed amsdu as part of fetch.
			* Wlc side assumes this is the case as part of pkt chain to re-aggregate
			* the same suppressed AMSDU. Since there is no simple way to delay and
			* wait for next fetch, let's resolve this during fetch itself.
			*/
			ret_len = pciedev_get_fetch_count_amsdu(pciedev, flow_ring, ret_len);
		}
	}

#ifdef BCMFRAGPOOL
	if (ret_len) {
		uint32 lbuf_avail = pktpool_avail(pciedev->pktpool_lfrag);
#ifdef BCM_DHDHDR
		if (BCMDHDHDR_ENAB())
			lbuf_avail = MIN(lbuf_avail, lfbufpool_avail(pciedev->d3_lfbufpool));
#endif /* BCM_DHDHDR */
		if (ret_len > (lbuf_avail - pciedev->fetch_pending)) {
			ret_len = 0;;
		}
	}
#endif /* BCMFRAGPOOL */

	/* Nothing to fetch go cleanup */
	if (!ret_len)
		goto cleanup;

	ret_len *= RING_LEN_ITEMS(flow_ring);

	/* Update the local copy of the write/end ptr with the shared register's value */
	src_addr = pciedev_get_src_addr(flow_ring, &src_len, ret_len);
	if (src_addr == NULL) {
		goto cleanup;
	}
	ret_len = src_len/RING_LEN_ITEMS(flow_ring);

	/* Get address from circular buffer pool */
	dest_addr = pciedev_get_cirbuf_pool(flow_ring, ret_len);

	/* Need to handle if local buffer avail space is < src_len */
	if (dest_addr != NULL) {
		set_bitrange(flow_ring->inprocess, MODULO_RING_IDX(flowfetchp->start_ringinx,
			flow_ring), MODULO_RING_IDX((flowfetchp->start_ringinx + ret_len - 1),
			flow_ring), flow_ring->bitmap_size - 1);

		/* Place the fetch request */
		flowfetchp->rqst.size = src_len;
		flowfetchp->rqst.dest = dest_addr;
		PHYSADDR64HISET(flowfetchp->rqst.haddr,
			(uint32) ltoh32(HIGH_ADDR_32(flow_ring->ringmem->base_addr)));
		PHYSADDR64LOSET(flowfetchp->rqst.haddr,
			(uint32) ltoh32(src_addr));
		flowfetchp->rqst.flags = 0;
		flowfetchp->msg_ring = flow_ring;
#ifdef BCMPCIEDEV
		if (BCMPCIEDEV_ENAB()) {
			hnd_fetch_rqst(&flowfetchp->rqst);
		}
#endif // endif
		pciedev->last_fetch_ring = FLRING_INX(flow_ring->ringid);
		flow_ring->fetch_pending += ret_len;
		pciedev->fetch_pending += ret_len;
#ifdef PCIEDEV_SCHED_DEBUG
		flow_ring->total_fetch += ret_len;
#endif // endif

		pciedev_update_rdp_ptr_unacked(flow_ring);
		return ret_len;
	}
cleanup:
	pciedev_update_rdp_ptr_unacked(flow_ring);
	pciedev_free_flowring_fetch_rqst_entry(pciedev, i);
	return 0;
} /* pciedev_read_flow_ring_host_buffer */

/**
 * Transmit related. Called when a new so-called 'internal' 'host fetch' message is available in
 * device memory, signalling that new data to transmit to the wireless medium is now available in
 * dongle memory. Called back by eg pciedev_process_tx_payload()
 */
void
pciedev_flowring_fetch_cb(struct fetch_rqst *fr, bool cancelled)
{
	flow_fetch_rqst_t *flow_fetch = fr->ctx;
	struct dngl_bus *pciedev = flow_fetch->msg_ring->pciedev;
	uint32 retlen = 0, processed_items = 0, offset = 0;

	/* Keep the old offset until which we have processed in fetch request */
	offset = flow_fetch->offset * RING_LEN_ITEMS(flow_fetch->msg_ring);

	/* If cancelled, retry: drop request for now
	 * Might need to retry sending it down HostFetch
	 */
	if (cancelled) {
		PCI_ERROR(("pciedev_flowring_fetch_cb: Request cancelled!...\n"));
		processed_items = (fr->size - offset)/RING_LEN_ITEMS(flow_fetch->msg_ring);
		goto cleanup;
	}

	/* retlen holds how much is still pending for process */
	retlen = pciedev_handle_h2d_msg_txsubmit(pciedev, (void*)((uint8*)fr->dest + offset),
		fr->size - offset, flow_fetch->msg_ring, flow_fetch->start_ringinx,  fr->flags);

cleanup:
	/* Handle Clean up and/or update processed information on the fecth */
	/* Store how much we processed so far */
	processed_items = ((fr->size - offset) - retlen)/RING_LEN_ITEMS(flow_fetch->msg_ring);

	/* Update flow msgbuf and global counters */
	flow_fetch->msg_ring->fetch_pending -= processed_items;
	pciedev->fetch_pending -= processed_items;

	if (!retlen) {
		if (!pciedev->fetch_req_pend_list.head ||
			(pciedev->fetch_req_pend_list.head == flow_fetch)) {
			/* We are done with this flow fetch request */
			/* Free up local message space */
			flow_fetch->offset = flow_fetch->flags = 0;
			pciedev_free_cirbuf_pool(flow_fetch->msg_ring, fr->dest, fr->size);
			pciedev_free_flowring_fetch_rqst_entry(pciedev, flow_fetch->index);
			if (pciedev->fetch_req_pend_list.head)
				pciedev_remove_fetch_rqst_head(pciedev);
			flow_fetch->next = NULL;
		} else {
			if (!(flow_fetch->flags & (PCIEDEV_FLOW_FETCH_FLAG_FREE |
				PCIEDEV_FLOW_FETCH_FLAG_REPROCESS))) {
				pciedev_insert_fetch_rqst(pciedev, flow_fetch);
			}
			flow_fetch->offset += processed_items;
			flow_fetch->flags |= PCIEDEV_FLOW_FETCH_FLAG_FREE |
				PCIEDEV_FLOW_FETCH_FLAG_REPROCESS;
		}
	} else {
		/* Partial flow fetch processing at pciedev_handle_h2d_msg_txsubmit
		* Update offset from which we need to process on next callback,
		* Update start index, flags to indicate partial process at fetch.
		*/
		flow_fetch->offset += processed_items;
		flow_fetch->start_ringinx = (flow_fetch->start_ringinx + processed_items) %
		RING_MAX_ITEM(flow_fetch->msg_ring);
		if (!(flow_fetch->flags & PCIEDEV_FLOW_FETCH_FLAG_REPROCESS)) {
			pciedev_insert_fetch_rqst(pciedev, flow_fetch);
		}
		flow_fetch->flags |= PCIEDEV_FLOW_FETCH_FLAG_REPROCESS;
	}

	if (!flow_fetch->msg_ring->flow_info.pktinflight &&
		!flow_fetch->msg_ring->fetch_pending)
		flow_fetch->msg_ring->status &= ~FLOW_RING_SUP_PENDING;

	if ((flow_fetch->msg_ring->status & FLOW_RING_FLUSH_PENDING) &&
		(!(flow_fetch->msg_ring->status & FLOW_RING_NOFLUSH_TXUPDATE)))
		pciedev_process_pending_flring_resp(pciedev, flow_fetch->msg_ring);
	return;
}

static int8 pciedev_allocate_flowring_fetch_rqst_entry(struct dngl_bus *pciedev)
{
	uint8 i;
	for (i = 0; i < PCIEDEV_MAX_FLOWRINGS_FETCH_REQUESTS; i++)
		if (!pciedev->flowring_fetch_rqsts[i].used) {
			pciedev->flowring_fetch_rqsts[i].used = TRUE;
			return i;
		}
	return BCME_ERROR;
}

/* Insert pending fetches at the end of req_pend list */
static void
pciedev_insert_fetch_rqst(struct dngl_bus * pciedev, flow_fetch_rqst_t *node)
{
	if (!pciedev->fetch_req_pend_list.head) {
		pciedev->fetch_req_pend_list.head =
		pciedev->fetch_req_pend_list.tail = node;
	} else {
		pciedev->fetch_req_pend_list.tail->next = node;
		pciedev->fetch_req_pend_list.tail = node;
	}
}

/* Remove pending fetches from head of req_pend list */
static void
pciedev_remove_fetch_rqst_head(struct dngl_bus * pciedev)
{
	if (pciedev->fetch_req_pend_list.head == pciedev->fetch_req_pend_list.tail) {
		pciedev->fetch_req_pend_list.head = pciedev->fetch_req_pend_list.tail = NULL;
	} else
		pciedev->fetch_req_pend_list.head = pciedev->fetch_req_pend_list.head->next;
}

/* Process pending flow fetch requests which could have left due to resource crunch. */
void pciedev_process_pending_fetches(struct dngl_bus *pciedev)
{
	flow_fetch_rqst_t *node;
	node = pciedev->fetch_req_pend_list.head;
	while (node && node->flags) {
		flow_fetch_rqst_t *next = node->next;
		pciedev_flowring_fetch_cb(&node->rqst, FALSE);
		node = next;
	}
}

static void pciedev_free_flowring_fetch_rqst_entry(struct dngl_bus *pciedev, uint8 index)
{
	pciedev->flowring_fetch_rqsts[index].used = FALSE;
}

/**
 * Transmit related. For suppressed packets, dongle can refetch the packets/work items from the host
 * as needed.
 *
 * Adjust Read-pending/next start of fetch pointer
 * Move rdpending to be lowest of rdpending and index, This is to
 * allow all packet suppressed/need-to-be refetched until the index
 * index points to index of the suppressed packet
 */
static void pciedev_adjust_flow_fetch_ptr(msgbuf_ring_t *flow_ring, uint16 index)
{
	if ((flow_ring->rd_pending >= DNGL_RING_RPTR(flow_ring))) {
		/* No wrap condition */
		if ((index >= DNGL_RING_RPTR(flow_ring)) && (index < flow_ring->rd_pending))
			flow_ring->rd_pending = index;
	} else {
		/* Wrap condition */
		if ((index < flow_ring->rd_pending) || (index >= DNGL_RING_RPTR(flow_ring)))
			flow_ring->rd_pending = index;
	}
}

/**
 * Transmit related. For suppressed packets, dongle can refetch the packets/work items from the host
 * as needed.
 *
 * If first packet to be fetched in is already status_cmpl then
 * move rd pending to first non-acked packet skip packets that are alreday acked.
 * This can happen if we get status ok/non-suppressed packet in between
 * the packet stream.
 */
static void pciedev_update_rdp_ptr_unacked(msgbuf_ring_t *flow_ring)
{
	uint16 i, max_item;

	if (isclr(flow_ring->inprocess, MODULO_RING_IDX(flow_ring->rd_pending, flow_ring)))
		return;

	max_item = RING_MAX_ITEM(flow_ring);

	for (i = 0; i < flow_ring->bitmap_size; i++) {
		if (isset(flow_ring->inprocess,
			MODULO_RING_IDX(flow_ring->rd_pending, flow_ring))) {
			flow_ring->rd_pending = (flow_ring->rd_pending + 1) % max_item;
		} else {
			break;
		}
	}
}

/**
 * Called when eg the wl layer reported status back on the transmission/suppression of a packet.
 *
 * Update the flow ring (in device memory) designated by (flowid) with the caller supplied
 * parameters, so the host can adjust itself to the new status of the flow ring. Does not generate
 * an interrupt towards the host. Also, reschedules suppressed packets for retransmission.
 */
static int pciedev_update_txstatus(struct dngl_bus *pciedev, uint32 status,
	uint16 rindex, uint16 flowid, uint16 seq)
{
	msgbuf_ring_t *flow_ring;
	uint16 index, rdptr;
	int ret = BCME_OK;
	int i;
	uint16 first_free_rd_ptr;
	uint16 bit_idx;
	index = flowid - BCMPCIE_H2D_MSGRING_TXFLOW_IDX_START;
	flow_ring = &pciedev->flow_ring_msgbuf[index];
	flow_ring->flow_info.pktinflight--;
	pciedev->pend_user_tx_pkts--;

	bit_idx = MODULO_RING_IDX(rindex, flow_ring);
	if (((status == WLFC_CTL_PKTFLAG_D11SUPPRESS) ||
	     (status == WLFC_CTL_PKTFLAG_WLSUPPRESS)) &&
	    (!(flow_ring->status & FLOW_RING_FLUSH_PENDING))) {
		/* proptxstatus: refetch suppressed packets from host for retransmission later on */
		ret = BCME_NOTREADY;
		if (flow_ring->flow_info.pktinflight || flow_ring->fetch_pending)
			flow_ring->status |= FLOW_RING_SUP_PENDING;
		pciedev_adjust_flow_fetch_ptr(flow_ring, rindex);
		clrbit(flow_ring->inprocess, bit_idx);
		if (WL_SEQ_GET_FROMDRV(seq)) {
			if (!(flow_ring->reuse_seq_list)) {
				flow_ring->reuse_seq_list = (uint16 *)
					MALLOCZ(pciedev->osh,
					flow_ring->bitmap_size * sizeof(uint16));
				pciedev->flow_supp_enab++;
				if (pciedev->flow_ageing_timer_on == FALSE) {
					PCI_TRACE(("\n ADDED: flow_ageing_timer"));
					dngl_add_timer(pciedev->flow_ageing_timer,
						pciedev->flow_age_timeout, TRUE);
					pciedev->flow_ageing_timer_on = TRUE;
				}
			}
			if (flow_ring->reuse_seq_list) {
				setbit(flow_ring->reuse_sup_seq, bit_idx); /* d11 seq numbers */
				flow_ring->ring_ageing_info.sup_cnt++;
				flow_ring->reuse_seq_list[bit_idx] = seq;
			}
		}
	} else if (isset(flow_ring->status_cmpl, bit_idx) ||
	           isclr(flow_ring->inprocess, bit_idx)) {
			/* Should never happen print for now until its stable */
			PCI_TRACE(("%s: I %d RD %d RDp %d W %d\n",
				__FUNCTION__, rindex,
				DNGL_RING_RPTR(flow_ring),
				pciedev->flow_ring_msgbuf[index].rd_pending,
			        DNGL_RING_WPTR(flow_ring)));
			ret = BCME_NOTREADY;
	} else {
		clrbit(flow_ring->reuse_sup_seq, bit_idx);
		if (flow_ring->reuse_seq_list)
			flow_ring->reuse_seq_list[bit_idx] = 0;

		rdptr = DNGL_RING_RPTR(flow_ring);
		if (rdptr == rindex) {
			clrbit(flow_ring->inprocess, bit_idx);
			i = 1;
			first_free_rd_ptr = 0;
			/* Find index until non pending packet and update the read index */
			while (i < flow_ring->bitmap_size)
			{
				first_free_rd_ptr = (rdptr + i) % RING_MAX_ITEM(flow_ring);
				if (!isset(flow_ring->status_cmpl,
					MODULO_RING_IDX(first_free_rd_ptr, flow_ring)))
					break;
				/* Clear inprocess bit, we are done with the packet now */
				clrbit(flow_ring->status_cmpl,
					MODULO_RING_IDX(first_free_rd_ptr, flow_ring));
				clrbit(flow_ring->inprocess,
					MODULO_RING_IDX(first_free_rd_ptr, flow_ring));
				i++;
			}
			BCMMSGBUF_RING_SET_R_PTR(flow_ring, first_free_rd_ptr);
		} else {
			setbit(flow_ring->status_cmpl, bit_idx);
		}
	}

	if (!flow_ring->flow_info.pktinflight && !flow_ring->fetch_pending)
		flow_ring->status &= ~FLOW_RING_SUP_PENDING;

	if ((flow_ring->status & FLOW_RING_FLUSH_PENDING) &&
		(!(flow_ring->status & FLOW_RING_NOFLUSH_TXUPDATE)))
		pciedev_process_pending_flring_resp(pciedev, flow_ring);

	pciedev_schedule_flow_ring_read_buffer(pciedev);
#ifdef FLOW_RING_LAZY_FETCH
	if (pciedev->lazy_fetch_enabled && pciedev->flow_watchdog_timer_on == FALSE) {
		PCI_TRACE(("\n ADDED: flow_watchdog_timer"));
		dngl_add_timer(pciedev->flow_watchdog_timer,
			pciedev->flow_chk_period, TRUE);
		pciedev->flow_watchdog_timer_on = TRUE;
	}
#endif /* FLOW_RING_LAZY_FETCH */
	return ret;
} /* pciedev_update_txstatus */

/* Return a buf from circular free list */
uint8*
pciedev_get_cirbuf_pool(msgbuf_ring_t * ring, uint16 len)
{
	cir_buf_pool_t *cpool = ring->cbuf_pool;
	uint16 w_ptr = cpool->w_ptr;
	uint16 availcnt = CIR_BUFPOOL_AVAILABLE(cpool);

	/* check for avail space */
	if (availcnt == 0) {
		EVENT_LOG(EVENT_LOG_TAG_PCI_DBG,
			"Pool empty Ring name: %c%c%c%c\n",
			ring->name[0], ring->name[1],
			ring->name[2], ring->name[3]);

		return NULL;
	}

	/* we should not get 0 len request (or) more than available count */
	ASSERT(len);
	ASSERT(len <= availcnt);

	/* update w_ptr to next free index & also handle wrap up case */
	if ((cpool->w_ptr + len) == cpool->depth) {
		cpool->w_ptr = 0;
	} else if ((cpool->w_ptr + len) < cpool->depth) {
		cpool->w_ptr += len;
	} else {
		return NULL;
	}

	cpool->availcnt -= len;

	/* Return the free buffer from the pool */
	return (uint8*)(cpool->buf + (w_ptr * cpool->item_size));
}

/* Admit the buffer back to free list */
static void
pciedev_free_cirbuf_pool(msgbuf_ring_t * ring, void* p, uint16 len)
{
	cir_buf_pool_t *cpool = ring->cbuf_pool;

	/* convert len of bytes into index */
	len /= RING_LEN_ITEMS(ring);

	/* update read pointer indicating free of memory */
	cpool->r_ptr += len;

	/* wrap around the buffer if index has crossed the depth */
	if (cpool->r_ptr >= cpool->depth)
		cpool->r_ptr = 0;

	cpool->availcnt += len;

	/* wrap back to index 0 if read & write pointer are same */
	/* This allows maximum contiguous memory available */
	if (cpool->availcnt == cpool->depth)
		cpool->r_ptr = cpool->w_ptr = cpool->r_pend = 0;
}

/** Return a local buffer from the free list */
void*
pciedev_get_lclbuf_pool(msgbuf_ring_t * ring)
{
	void* ret;
	lcl_buf_pool_t * pool = ring->buf_pool;

	/* check for avail space */
	if (pool->free == NULL) {
		EVENT_LOG(EVENT_LOG_TAG_PCI_DBG,
			"Pool empty Ring name: %c%c%c%c\n",
			ring->name[0], ring->name[1],
			ring->name[2], ring->name[3]);

		return NULL;
	}

	/* Retrieve back the buffer */
	ret = pool->free->p;
	ASSERT(ret != NULL);

	/* Make pkt in cur node as NULL */
	pool->free->p = NULL;

	pool->free = pool->free->nxt;
#ifdef PCIEDEV_DBG_LOCAL_POOL
	pciedev_check_free_p(ring, pool->free, ret, __FUNCTION__);
#endif /* PCIEDEV_DBG_LOCAL_POOL */
	pool->availcnt--;
	return ret;
}

/** Admit the buffer back to free list */
void
pciedev_free_lclbuf_pool(msgbuf_ring_t * ring, void* p)
{
	lcl_buf_pool_t * pool = ring->buf_pool;
	lcl_buf_t * free = pool->free;

	if (free == pool->head) {
		PCI_ERROR(("pool allready full cant admit %x to ring name: %c%c%c%c\n",
			(uint32)p,
			ring->name[0], ring->name[1],
			ring->name[2], ring->name[3]));

		ASSERT(0);
		return;
	}

	if (free == NULL) {
		/* If all items are exhausted, free will point to NULL */
		/* restore free to tail the moment we get atleast 1 pkt back */
		free = pool->tail;
		free->p = p;
		pool->free = free;
	} else {
#ifdef PCIEDEV_DBG_LOCAL_POOL
		/* Store pkt in previous node */
		pciedev_check_free_p(ring, free, p, __FUNCTION__);
#endif /* PCIEDEV_DBG_LOCAL_POOL */
		free->prv->p = p;
		/* Move free ptr to previous node */
		pool->free = free->prv;
	}
	/* Increment avail count */
	pool->availcnt++;
}

/**
 * Handles one 'rx buffer post' message from the host. This message is consumed from a queue,
 * which can contain multiple messages.
 *
 * Output parameters:
 *     'bufid':     unique identifier, assigned by the host, for rx buffer posted by the host
 *     'haddr':     64 bit host address
 *     'len':       host addr len
 *     'meta_addr': 64 bit metadata addr
 *     'meta_len':  metadata len
 *
 * Return value: 0 on success, -1 on failure.
 */
static int
pciedev_get_host_addr(struct dngl_bus * pciedev, uint32* bufid, uint16 * len, dma64addr_t *haddr,
	dma64addr_t *meta_addr, uint16 *meta_len)
/*
 * XXX function name could be more descriptive. Eg include 'rx' and 'buf post' in the name, say
 * 'pciedev_rx_process_buf_post_msg'.
 */
{
	uint8 r, depth;
	uint8 max_item;
	uint8 cur_item = 0;
	void* lcl_buf = NULL; /* lcl buffer in pool containing one or more 'rx buf post' messages */
	uint8* p = NULL;      /* points at one 'rx buf post' message in local buffer */
	msgbuf_ring_t * ring = pciedev->htod_rx;
	/* pool/queue containing one or more 'rx buf post' messages to process */
	inuse_lclbuf_pool_t * rxpool = ring->buf_pool->inuse_pool;

	/* intialize haddr to NULL */
	PHYSADDR64HISET(*haddr, 0);
	PHYSADDR64LOSET(*haddr, 0);

	r = rxpool->r_ptr;
	depth = rxpool->depth;
	/* Check if any buffer locally available */
	if (!NTXPACTIVE(rxpool->r_ptr, rxpool->w_ptr, depth)) {
		EVENT_LOG(EVENT_LOG_TAG_PCI_DBG,
			"pciedev_get_host_addr : No active element in local array \n");
		return -1;
	}

	/* Retrieve the head buffer pointer */
	lcl_buf = rxpool->buf[r].p;
	p = (uint8*)lcl_buf;

	/* max no of host buffers available in this chunk */
	max_item = rxpool->buf[r].max_items;

	cur_item = rxpool->buf[r].used_items;
	if ((cur_item < max_item) && (p != NULL)) {
		/* retrieve individual message from the local buffer */
		p = p + (RING_LEN_ITEMS(ring) * (cur_item));

		/* distills bufid, len, ... etc fields out of rx post message ('p') */
		pciedev_process_rxpost_msg(pciedev, p, bufid, len, haddr, meta_addr, meta_len);

#ifdef PCIEDEV_HOST_PKTID_AUDIT_ENABLED
		pciedev_host_pktid_audit(pciedev, *bufid, TRUE);
#endif /* PCIEDEV_HOST_PKTID_AUDIT_ENABLED */

		rxpool->buf[r].used_items++; /* used one 'rx post' message from local buffer */

		if (rxpool->buf[r].used_items == rxpool->buf[r].max_items) {
			rxpool->buf[r].used_items = 0;
			rxpool->buf[r].max_items = 0;
			/* Update read pointer */
			rxpool->r_ptr = NEXTTXP(rxpool->r_ptr, depth);

			/* local buffer contains no more messages, so free that into pool */
			pciedev_free_lclbuf_pool(ring, lcl_buf);

			/* Check for more rxbuffers in the ring */
			pciedev_msgbuf_intr_process(pciedev); /* starts fetching messages */
		}
		return 0;
	}
	return -1;
} /* pciedev_get_host_addr */

/** AMPDU (receive) reordering related. Is called back by the 'pktpool' on a pktpool_get(). */
int
pciedev_fillup_rxcplid(pktpool_t *pool, void *arg, void *p, bool dummy)
{
	struct dngl_bus *pciedev = (struct dngl_bus *) arg;
	rxcpl_info_t *p_rxcpl_info;

	if (PKTRXCPLID(pciedev->osh, p) != 0) {
		return 0;
	}
	p_rxcpl_info = bcm_alloc_rxcplinfo();
	if (p_rxcpl_info == NULL) {
		PCI_ERROR(("couldn't allocate rxcpl info for lbuf: %x \n", (uint32)p));
		PCIEDEV_RXCPL_ERR_INCR(pciedev);
		return -1;
	}
	if (p_rxcpl_info->rxcpl_id.idx == 0) {
		PCI_ERROR(("lbuf: got the p_rxcpl_info->rxcpl_id.idx as zero\n"));
		ASSERT(p_rxcpl_info->rxcpl_id.idx != 0);
	}
	PKTSETRXCPLID(pciedev->osh, p, p_rxcpl_info->rxcpl_id.idx);
	return 0;
}

/**
 * AMPDU (receive) reordering related. Is called back by the 'pktpool' on a pktpool_get(). Takes in
 * a frag as input. Updates frag with host address & len.
 */
int
pciedev_fillup_haddr(pktpool_t *pool, void* arg, void* frag, bool rxcpl_needed)
{

	struct dngl_bus *pciedev = (struct dngl_bus *) arg;
	uint16 len, meta_len;
	dma64addr_t haddr, meta_addr;
	uint32 bufid;
	rxcpl_info_t *p_rxcpl_info = NULL;

	/* Check if lfrag already has host address associated with it */
	if (PKTRXCPLID(pciedev->osh, frag) != 0)
		rxcpl_needed = FALSE;

	if (rxcpl_needed)  {
		/* Check if we have space for rx completion ID */
		p_rxcpl_info = bcm_alloc_rxcplinfo();
		if (p_rxcpl_info == NULL) {
			PCI_ERROR(("couldn't allocate rxcpl info: %x \n", (uint32)frag));
			PCI_ERROR(("couldn't allocate rxcpl\n"));
			return -1;
		}
	}
	if (PKTISRXFRAG(pciedev->osh, frag)) {
		if (rxcpl_needed)
			PKTSETRXCPLID(pciedev->osh, frag, p_rxcpl_info->rxcpl_id.idx);
		EVENT_LOG(EVENT_LOG_TAG_PCI_DBG,
			"Frag allready filled up : %p \n", (uint32)frag);
#if defined(WL_MONITOR) && !defined(WL_MONITOR_DISABLED)
		/* creating a head room of 8 bytes to accomodate cmn_msg_hdr_t.
		 * In non monitor case rxhdr is stripped of in wlc_recv().
		 * So there is enough headroom for cmn_msg_hdr
		 */
		if (pciedev->monitor_mode && !RXFIFO_SPLIT()) {
			PKTPULL(pciedev->osh, frag, sizeof(cmn_msg_hdr_t));
		}
#endif /* WL_MONITOR && WL_MONITOR_DISABLED */
		return 0;
	}

	/* Check if buffers available locally */
	if (pciedev_get_host_addr(pciedev, &bufid, &len, &haddr, &meta_addr, &meta_len)) {
		EVENT_LOG(EVENT_LOG_TAG_PCI_DBG,
			"No lcl RX post message available \n");
		if ((rxcpl_needed) && (p_rxcpl_info != NULL))
			bcm_free_rxcplinfo(p_rxcpl_info);
		return -1;
	}

	/* setup the rx completion ID */
	if (rxcpl_needed) {
		if (p_rxcpl_info->rxcpl_id.idx == 0) {
			PCI_ERROR(("rxlfrag: got the p_rxcpl_info->rxcpl_id.idx as zero\n"));
			ASSERT(p_rxcpl_info->rxcpl_id.idx != 0);
		}
		PKTSETRXCPLID(pciedev->osh, frag, p_rxcpl_info->rxcpl_id.idx);
	}

	/* Load 64 bit host address */
	PKTSETFRAGDATA_HI(pciedev->osh, frag, 1, PHYSADDR64HI(haddr));
	PKTSETFRAGDATA_LO(pciedev->osh, frag, 1, PHYSADDR64LO(haddr) + pciedev->tcmsegsz);

	/* frag len */
	PKTSETFRAGLEN(pciedev->osh, frag, 1, (len - pciedev->tcmsegsz));
	/* pktid */
	PKTSETFRAGPKTID(pciedev->osh, frag, bufid);

	/* set the meta data pointers */
	PKTSETFRAGMETADATALEN(pciedev->osh, frag, meta_len);
	/* Access addr only length is valid */
	if (meta_len) {
		PKTSETFRAGMETADATA_HI(pciedev->osh, frag, PHYSADDR64HI(meta_addr));
		PKTSETFRAGMETADATA_LO(pciedev->osh, frag, PHYSADDR64LO(meta_addr));
	}

	/* Mark rxfrag that host addr is valid */
	PKTSETRXFRAG(pciedev->osh, frag);

#if defined(WL_MONITOR) && !defined(WL_MONITOR_DISABLED)
		/* creating a head room of 8 bytes to accomodate cmn_msg_hdr_t.
		 * In non monitor case rxhdr is stripped of in wlc_recv().
		 * So there is enough headroom for cmn_msg_hdr
		 */
		if (pciedev->monitor_mode && !RXFIFO_SPLIT()) {
			PKTPULL(pciedev->osh, frag, sizeof(cmn_msg_hdr_t));
		}
#endif /* WL_MONITOR && WL_MONITOR_DISABLED */
	return 0;
} /* pciedev_fillup_haddr */

/**
 * Called when one or more 'rx buf post' messages have been received from the host in device memory.
 * Queues the messages in an 'inuse' list for (later) processing.
 *
 * Input parameters:
 *    p        : local buffer containing the message(s)
 *    max_items: number of messages
 *
 */
static void
pciedev_add_to_inuselist(struct dngl_bus *pciedev, void* p, uint8 max_items)
{
	msgbuf_ring_t * ring = pciedev->htod_rx;
	/* rxpool: pool of local buffers receiving messages from host */
	inuse_lclbuf_pool_t * rxpool = ring->buf_pool->inuse_pool;
	uint8 w, r, depth;

	w = rxpool->w_ptr;
	depth = rxpool->depth;
	r = rxpool->r_ptr;

	/* check if inuse list has space */
	if (!NTXPAVAIL(r, w, depth)) {
		return;
	}

	/* Store buf pointer & max items in the buff */
	rxpool->buf[w].p = p; /* type of p: 'inuse_lcl_buf_t' */
	rxpool->buf[w].max_items = max_items;

	/* Update write pointer */
	rxpool->w_ptr = NEXTTXP(rxpool->w_ptr, depth);
}

/**
 * XXX: there is an issue handling different burst lengths on H2D tx side and the problem
 * XXX: is identified to be back plane clock related, so the ask is force HT when there
 * XXX: is a pending H2D transaction
 */
static void
pciedev_manage_h2d_dma_clocks(struct dngl_bus *pciedev)
{
	if (pciedev->force_ht_on) {
		PCI_TRACE(("%s: force clock on\n", __FUNCTION__));
		OR_REG(pciedev->osh, &pciedev->regs->u.pcie2.clk_ctl_st, CCS_FORCEHT);
	} else {
		PCI_TRACE(("%s: removing the force clock on\n", __FUNCTION__));
		AND_REG(pciedev->osh, &pciedev->regs->u.pcie2.clk_ctl_st, ~CCS_FORCEHT);
	}
	return;
}

/**
 * Distills the fields out of an 'rx buffer post' message received from the host.
 * Function input parameters:
 *     p         : rx buf post message received from host
 * Function output parameters:
 *     bufid     : the id that the host assigned to this new rx buffer
 *     len       : the rx buffer length as specified by the host
 *     haddr     : 64 bits host address where rx buffer resides
 *     haddr_meta: 64 bits host address where rx meta data buffer resides
 *     metadata_len: rx meta data buffer length as specified by the host
 */
static void
pciedev_process_rxpost_msg(struct dngl_bus *pciedev, void* p, uint32* bufid, uint16 * len,
	dma64addr_t *haddr, dma64addr_t *haddr_meta, uint16 *metadata_len)
{
	host_rxbuf_post_t *rx_post = (host_rxbuf_post_t *)p;
#ifdef H2D_CHECK_SEQNUM
	uint8 ring_seqnum = pciedev->htod_rx->h2d_seqnum % H2D_EPOCH_MODULO;
	uint8 msg_seqnum = rx_post->cmn_hdr.epoch;

	/* check for sequence number sanity */
	if (msg_seqnum == ring_seqnum) {
		pciedev->htod_rx->h2d_seqnum++;
	} else {
		PCI_ERROR(("RXPOST :error in seqnum : got %d exp %d \n",
			msg_seqnum, ring_seqnum));
		ASSERT(0);
	}
#endif /* H2D_CHECK_SEQNUM */

	/* PKTLEN */
	*len =  ltoh16(rx_post->data_buf_len);

	/* BUFID */
	*bufid = ltoh32(rx_post->cmn_hdr.request_id);

	/* HOST address */
	PHYSADDR64HISET(*haddr, (uint32) rx_post->data_buf_addr.high_addr);
	PHYSADDR64LOSET(*haddr, (uint32) rx_post->data_buf_addr.low_addr);

	/* mark it as pcie address */
	PHYSADDR64HISET(*haddr, PHYSADDR64HI(*haddr) | PCIE_ADDR_OFFSET);

	/* Metadata info */
	*metadata_len = ltoh16(rx_post->metadata_buf_len);

	/* Access addr only length is valid */
	if (*metadata_len) {
		PHYSADDR64HISET(*haddr_meta, (uint32) rx_post->metadata_buf_addr.high_addr);
		PHYSADDR64LOSET(*haddr_meta, (uint32) rx_post->metadata_buf_addr.low_addr);
	}
}

inline int
pciedev_htoddma_queue_avail(struct dngl_bus *pciedev)
{
	uint16 rd_idx = pciedev->htod_dma_rd_idx;
	uint16 wr_idx = pciedev->htod_dma_wr_idx;

	/* NOTE: The maximum no. of elements that can be held in the Q
	 * at a time is (MAX_DMA_QUEUE_LEN - 1). When Q is full, wr_idx
	 * will point to an empty Q location, just before rd_idx. So for Q
	 * to have adequate space for enque, this utility function should return > 1
	 */
	return (rd_idx > wr_idx ? (rd_idx - wr_idx) : (MAX_DMA_QUEUE_LEN_H2D - wr_idx + rd_idx));
}

/**
 * Called by the dongle OS, requesting the PCIe subsystem to start fetching the next item (in 'fr')
 * from the host.
 */
int
pciedev_dispatch_fetch_rqst(struct fetch_rqst *fr, void *arg)
{
	struct dngl_bus *pciedev = (struct dngl_bus *)arg;
	uint32 txdesc, rxdesc;
	dma64addr_t src;
	uint16 dmalen;
	int dma_qavail;

	if (pciedev->in_d3_suspend) {
		return BCME_ERROR;
	}

	PHYSADDR64HISET(src, (uint32) ltoh32(PHYSADDR64HI(fr->haddr)));
	PHYSADDR64LOSET(src, (uint32) ltoh32(PHYSADDR64LO(fr->haddr)));
	dmalen = fr->size;

	txdesc = PCIEDEV_GET_AVAIL_DESC(pciedev, HTOD, TXDESC);
	rxdesc = PCIEDEV_GET_AVAIL_DESC(pciedev, HTOD, RXDESC);
	dma_qavail = pciedev_htoddma_queue_avail(pciedev);

	if (txdesc >= MIN_TXDESC_AVAIL && rxdesc >= MIN_RXDESC_AVAIL && (dma_qavail > 1)) {
		pciedev_enque_fetch_cmpltq(pciedev, fr);
		/* kicks off dma */
		pciedev_h2dmsgbuf_dma(pciedev, src, dmalen,
			(uint8*) fr->dest, pciedev->dummy_rxoff, NULL, MSG_TYPE_HOST_FETCH);
		return BCME_OK;
	} else {
		PCI_ERROR(("FE: %d, %d, %d\n", txdesc, rxdesc, dma_qavail));
		return BCME_ERROR;
	}
}

static void
pciedev_enque_fetch_cmpltq(struct dngl_bus *pciedev, struct fetch_rqst *fr)
{
	pciedev_fetch_cmplt_q_t *fcq = pciedev->fcq;

	if (fcq->head == NULL)
		fcq->head = fcq->tail = fr;
	else {
		fcq->tail->next = fr;
		fcq->tail = fr;
	}
	fcq->count++;
	fcq->tail->next = NULL;
}

static struct fetch_rqst *
pciedev_deque_fetch_cmpltq(struct dngl_bus *pciedev)
{
	pciedev_fetch_cmplt_q_t *fcq = pciedev->fcq;
	struct fetch_rqst *fr;

	if (fcq->head == NULL) {

		fr = NULL;
	} else if (fcq->head == fcq->tail) {
		ASSERT(fcq->count > 0);
		fcq->count--;
		fr = fcq->head;
		fcq->head = fcq->tail = NULL;
	} else {
		ASSERT(fcq->count > 0);
		fcq->count--;
		fr = fcq->head;
		fcq->head = fcq->head->next;
		fr->next = NULL;
	}
	return fr;
}

/**
 * Called when a new so-called 'internal' 'host fetch' message is available in device memory,
 * signalling that new data to transmit to the wireless medium is now available in dongle memory.
 */
void
pciedev_process_tx_payload(struct dngl_bus *pciedev)
{
	struct fetch_rqst *fr;
	bool cancelled = FALSE;

	if (pciedev->in_d3_suspend)
		return;

	fr = pciedev_deque_fetch_cmpltq(pciedev);
	if (fr == NULL)
		return;

	/* If the fetch_rst was cancelled while in bus DMA queue,
	 * need to return it back to Host indicating the cancellation
	 */
	if (FETCH_RQST_FLAG_GET(fr, FETCH_RQST_CANCELLED)) {
		cancelled = TRUE;
		/* Clear the fetch_rqst flag now, to avoid misunderstandings later */
		FETCH_RQST_FLAG_CLEAR(fr, FETCH_RQST_CANCELLED);
	}

	FETCH_RQST_FLAG_CLEAR(fr, FETCH_RQST_IN_BUS_LAYER);

	/* Call the registered callback function, e.g. pciedev_flowring_fetch_cb() */
	if (fr->cb)
		fr->cb(fr, cancelled);
	else {
		PCI_ERROR(("pciedev_process_tx_payload: No callback registered for fetch_rqst!\n"));
		ASSERT(0);
	}
}

/**
 * H2D direction. The 'pciedev' device contains multiple (local) message rings. When a message from
 * the host needs to be (DMA) transferred into local memory, the host address on which the message
 * resides needs to be retrieved, so the caller can subsequently program DMA with that address.
 *
 * Returns host address of ring to be read from (then why is it not a 64 bit address?)
 */
static void*
pciedev_get_src_addr(msgbuf_ring_t * ring, uint16* available_len, uint16 max_len)
{
	uint16 w_ptr;
	uint16 r_ptr;
	uint16 depth;
	void* ret_addr = NULL;
	uint16 availcnt;
	cir_buf_pool_t *cpool = ring->cbuf_pool;

	/* get availalbe count that can fit in local buffer */
	availcnt = (cpool == NULL) ? (ring->buf_pool->item_cnt) : (CIR_BUFPOOL_AVAILABLE(cpool));

	w_ptr = DNGL_RING_WPTR(ring); /* this pointer in local memory is written by the host */
	r_ptr = ring->rd_pending;
	depth = RING_MAX_ITEM(ring);

	/* First check if there is any data available in the (local) circular buffer */
	*available_len = READ_AVAIL_SPACE(w_ptr, r_ptr, depth);
	if (*available_len == 0)
		return NULL;
	if (max_len) {
		max_len = max_len/RING_LEN_ITEMS(ring);
		*available_len = MIN(MIN(*available_len, availcnt), max_len);
	} else
		*available_len = MIN(*available_len, availcnt);
	ASSERT(*available_len <= depth);

	/*
	 * We don't do dma on wrapped around space. Instead do it in two steps where you read end
	 * region first followed by top region.
	 *
	 * Structure '*ring' contains a host address, which for the common rings was written by the
	 * host directly into device memory, and for flow rings by the 'flow ring create' message.
	 */
	ret_addr = (uint8*)RING_START_PTR(ring) + (ring->rd_pending * RING_LEN_ITEMS(ring));

	/*
	 * Please note that we do not update the read pointer here. Only
	 * read pending pointer is updated, so that next reader knows where
	 * to read data from.
	 * read pointer can only be updated when the read is complete.
	 */
	if ((ring->rd_pending + *available_len) >= depth)
		ring->rd_pending = 0;
	else
		ring->rd_pending += *available_len;

	ASSERT(ring->rd_pending < depth);

	/* Make it byte count rather than index count */
	*available_len = *available_len * RING_LEN_ITEMS(ring);
	return ret_addr;
} /* pciedev_get_src_addr */

/** H2D direction: update h2d ring read pointer after h2d dma is complete */
static void
pciedev_ring_update_readptr(struct dngl_bus *pciedev, msgbuf_ring_t *ring, uint16 bytes_read)
{
	uint16 index_read;
	uint16 rd_idx;

	if (ring == NULL)
		return;

	index_read = bytes_read / RING_LEN_ITEMS(ring);
	ASSERT(index_read <= RING_MAX_ITEM(ring));

	if (index_read == 0)
		return;

	rd_idx = DNGL_RING_RPTR(ring);

	/* Update the read pointer */
	if ((rd_idx + index_read) >= RING_MAX_ITEM(ring))
		BCMMSGBUF_RING_SET_R_PTR(ring, 0);
	else
		BCMMSGBUF_RING_SET_R_PTR(ring, rd_idx + index_read);

#if defined(PCIE_DMA_INDEX) && defined(SBTOPCIE_INDICES)
	/* Sync read pointers for control post/rxpost rings */
	if (ring->dma_d2h_indices_supported &&
		(pciedev->h2d_readindx_dmablock->host_dmabuf_inited)) {
		pciedev_sync_h2d_read_ptrs(pciedev, ring);
	}
#endif // endif
}

/** D2H direction: Update write ptr after d2h dma complete */
static void
pciedev_ring_update_writeptr(msgbuf_ring_t *ring, uint16 bytes_written)
{
	uint16 wrt_idx;
	uint16 index;
	struct dngl_bus *pciedev;

	if (ring == NULL)
		return;

	index = bytes_written / RING_LEN_ITEMS(ring);
	ASSERT(index <= RING_MAX_ITEM(ring));

	if (index == 0)
		return;

	pciedev = ring->pciedev;
	wrt_idx = DNGL_RING_WPTR(ring);

	/* Update Write pointer */
	if ((wrt_idx + index) >= RING_MAX_ITEM(ring)) {
		BCMMSGBUF_RING_SET_W_PTR(ring, 0);
	} else {
		BCMMSGBUF_RING_SET_W_PTR(ring, wrt_idx + index);
	}
#ifdef PCIE_PHANTOM_DEV
	phtm_ring_dtoh_doorbell(pciedev->phtm);
#else
#if defined(PCIE_DMA_INDEX)
#if defined(SBTOPCIE_INDICES)
	/* If DMAing r/w indices supported, defer doorbell until indices are DMAed */
	if (ring->dma_d2h_indices_supported &&
		pciedev->d2h_writeindx_dmablock->host_dmabuf_inited) {
			/* Sync up D2H write pointers with host */
			pciedev_sync_d2h_write_ptrs(pciedev, ring);
	}
#else
	if (ring->dma_d2h_indices_supported &&
		(pciedev->h2d_readindx_dmablock->host_dmabuf_inited ||
		pciedev->d2h_writeindx_dmablock->host_dmabuf_inited)) {
			pciedev_dma_set_indices(pciedev, ring);

	}
	else
#endif /* !SBTOPCIE_INDICES */
#endif /* PCIE_DMA_INDEX */
	{
#if defined(PCIE_D2H_DOORBELL_RINGER)
		uint32 index = ring->ringid - BCMPCIE_H2D_COMMON_MSGRINGS;
		d2h_doorbell_ringer_t *ringer = &pciedev->d2h_doorbell_ringer[index];
		ringer->db_fn(pciedev, ringer->db_info.value, ringer->db_info.haddr.low);
#else /* ! PCIE_D2H_DOORBELL_RINGER */
		pciedev_generate_host_db_intr(pciedev, PCIE_D2H_DB0_VAL, PCIE_DB_DEV2HOST_0);
#endif /* ! PCIE_D2H_DOORBELL_RINGER */
	}
#endif /* PCIE_PHANTOM_DEV */
}

/** D2H direction: return ring ptr to put d2h messages. Update write pending pointers */
static uint32
pciedev_get_ring_space(struct dngl_bus *pciedev, msgbuf_ring_t *msgbuf, uint16 msglen)
{
	uint16 wp_idx = msgbuf->wr_pending;
	uint32 retaddr;
	uint16 index = msglen / RING_LEN_ITEMS(msgbuf);

	uint16 avail_ring_entry = CHECK_WRITE_SPACE(DNGL_RING_RPTR(msgbuf), WRT_PEND(msgbuf),
		RING_MAX_ITEM(msgbuf));

	ASSERT(index < RING_MAX_ITEM(msgbuf));

	if (avail_ring_entry < index) {
		PCI_ERROR(("msgbuf name: In ring %c%c%c%c %d"
			"slots not available, cur avail space %d, msglen %d\n",
			msgbuf->name[0], msgbuf->name[1],
			msgbuf->name[2], msgbuf->name[3],
			index, avail_ring_entry, msglen));
		return NULL;
	}

	/* Return space */
	retaddr = RING_START_PTR(msgbuf) + (wp_idx * RING_LEN_ITEMS(msgbuf));

	/* Update write pending */
	if ((msgbuf->wr_pending + index) >= RING_MAX_ITEM(msgbuf)) {
		ASSERT((msgbuf->wr_pending + index) <= RING_MAX_ITEM(msgbuf));
		msgbuf->wr_pending = 0;
	} else {
		msgbuf->wr_pending += index;
	}

	ASSERT(msgbuf->wr_pending < RING_MAX_ITEM(msgbuf));

	return retaddr;
}
/* Check if there is enough resource to process TX status */
static bool
pciedev_resource_avail_for_txmetadata(struct dngl_bus *pciedev, uint8 num_items)
{
	msgbuf_ring_t *dtoh_txcpl = pciedev->dtoh_txcpl;
	cir_buf_pool_t *cpool = dtoh_txcpl->cbuf_pool;
	uint32 wr_space, wr_space_wrap_arnd;

	/* Skip tx status enqueu if ioctl processing is pending */
	if (pciedev->ioctl_pend == TRUE) {
		return FALSE;
	}

	/* Pktfetched packets wont have host section initialized
	 * But still need a txcomplete to go up.
	 * Those packets would come in with fragtotnum =0
	 * So initialize num_items to be atleast 1 to cover that case
	 */
	num_items = MAX(1, num_items);

	/* local circular buffer space */
	if (cpool->availcnt < num_items) {
		/* not enough resources to queue txstatus */
		return FALSE;
	}
	/* continuous write space available */
	wr_space = CHECK_WRITE_SPACE(DNGL_RING_RPTR(dtoh_txcpl),
		WRT_PEND(dtoh_txcpl), RING_MAX_ITEM(dtoh_txcpl));

#if defined(PCIE_DMA_INDEX) && defined(SBTOPCIE_INDICES)
	if ((wr_space < num_items) && (pciedev->d2h_readindx_dmablock->host_dmabuf_inited)) {
		/* if no space sync up read pointers and check once more */
		pciedev_sync_d2h_read_ptrs(pciedev, pciedev->dtoh_txcpl);

		wr_space = CHECK_WRITE_SPACE(DNGL_RING_RPTR(dtoh_txcpl),
			WRT_PEND(dtoh_txcpl), RING_MAX_ITEM(dtoh_txcpl));
	}
#endif /* PCIE_DMA_INDEX */

	if (wr_space == 0) {
		/* no space in d2h_txcpl ring to queue txstatus */
		return FALSE;
	}

	/* If pending count is already filling up host ring, dont admit any more */
	if (cpool->pend_item_cnt >= wr_space) {
		/* We cannot process/queue this fetched one now */
		return FALSE;
	}

	/* Total write space avaialble after wrap over */
	wr_space_wrap_arnd = WRITE_SPACE_AVAIL(DNGL_RING_RPTR(dtoh_txcpl),
		WRT_PEND(dtoh_txcpl), RING_MAX_ITEM(dtoh_txcpl));

	/*
	 * With AMSDUFRAG mem optimization every lfrag is tied
	 * with two host packets and need to send two txcomplete messages.
	 * When 2 or more  complete items need to be queued up, free space
	 * after wrap around also need to be considered
	 */
	if (cpool->pend_item_cnt + num_items  > wr_space_wrap_arnd) {
		/* We cannot process/queue this fetched one now */
		return FALSE;
	}

	/* With AMSDUFRAG if there is a wraparound of tx complete ring
	 * to send two txcompletes, it means it will go up in two different
	 * DMA transactions. So while checking for descriptors, account for
	 * two separate DMA transactions to send txcompletes.
	 * Check for D2H descriptors
	 */
	if ((PCIEDEV_GET_AVAIL_DESC(pciedev, DTOH, TXDESC) < (MIN_TXDESC_AVAIL + 2)) ||
		(PCIEDEV_GET_AVAIL_DESC(pciedev, DTOH, RXDESC) < (MIN_RXDESC_AVAIL + 3))) {
		/* not enough descriptors to queue txstatus */
		return FALSE;
	}

	return TRUE;
}

/**
 * Called when a 'dongle->host' message is drained from a firmware internal queue, with the purpose
 * of sending it to the host. This function schedules a transfer.
 */
static bool
pciedev_check_process_d2h_message(struct dngl_bus *pciedev, uint32 txdesc, uint32 rxdesc,
	void *p, d2h_msg_handler *msg_handler)
{
	uint32  txdesc_needed;
	uint32  rxdesc_needed;
	cmn_msg_hdr_t *msg;

#ifdef BCM_DHDHDR
	if (BCMDHDHDR_ENAB() && PKTISTXFRAG(pciedev->osh, p)) {
		/* When BCM_DHDHDR is defined for txfrag packet we save the txstatus 4B in last 4B
		 * of pkttag we don't leverage the PKTDATA to carry the cmn_msg_hdr_t because
		 * we may not have PKTDATA.
		 * for example the second msdu which has free the D3 buffer.
		 * Here if the p is txfrag means it wants to do txmetadata/txstatus
		 */
		/* Check if there are resources to queue txstatus */
		if (!pciedev_resource_avail_for_txmetadata(pciedev, PKTFRAGTOTNUM(pciedev->osh, p)))
			return FALSE;

		txdesc_needed = MIN_TXDESC_AVAIL + 2;
		rxdesc_needed = MIN_RXDESC_AVAIL + 3;
		*msg_handler = pciedev_process_d2h_txmetadata;

		if (txdesc < txdesc_needed || rxdesc < rxdesc_needed)
			return FALSE;

		return TRUE;
	}
#endif /* BCM_DHDHDR */

	msg = (cmn_msg_hdr_t *)PKTDATA(pciedev->osh, p);

	/* check if dma resources are available to send payload */
	/* check if dma resources are available to dma msgbuf */
	/* check if local buffer slot is available */

	switch (msg->msg_type) {
		case MSG_TYPE_WL_EVENT: /* notify host of dongle event */
			txdesc_needed = 2 + 2;
			rxdesc_needed = 2 + 3;
			if (!LCL_BUFPOOL_AVAILABLE(pciedev->dtoh_ctrlcpl)) {
				return FALSE;
			}
			/*
			 * XXX olympic wants us to make sure we don't drop events
			 * because we don't have host buffers
			 * may be we should have some escape logic to come out of it,
			 * if we have too many events pending and host not posting buffers
			 */
			if (HOST_DMA_BUF_POOL_EMPTY(pciedev->event_pool)) {
				PCI_TRACE(("not sending it because no host buffer avail\n"));
				pciedev->event_delivery_pend = TRUE;
				return FALSE;
			}
			pciedev->event_delivery_pend = FALSE;

			*msg_handler = pciedev_process_d2h_wlevent;
			break;

		case MSG_TYPE_TXMETADATA_PYLD: /* notify host of transmit status */
			/* Check if there are resources to queue txstatus */
			if (!pciedev_resource_avail_for_txmetadata(pciedev, TXMETA_ENTRIES_DFLT))
				return FALSE;

			txdesc_needed = MIN_TXDESC_AVAIL + 2;
			rxdesc_needed = MIN_RXDESC_AVAIL + 3;
			*msg_handler = pciedev_process_d2h_txmetadata;
			break;

		case MSG_TYPE_RX_PYLD:
			{
#if defined(PCIE_DMA_INDEX) && defined(SBTOPCIE_INDICES)
			uint32 wr_space;
			/* Check if there is space in D2H Rx Complete ring */
			/* continuous write space available */
			wr_space = CHECK_WRITE_SPACE(DNGL_RING_RPTR(pciedev->dtoh_rxcpl),
				WRT_PEND(pciedev->dtoh_rxcpl),
				RING_MAX_ITEM(pciedev->dtoh_rxcpl));

			if ((wr_space == 0) &&
				(pciedev->d2h_readindx_dmablock->host_dmabuf_inited)) {
				/* if no space sync up read pointers and check once more */
				pciedev_sync_d2h_read_ptrs(pciedev, pciedev->dtoh_rxcpl);
			}
#endif /* PCIE_DMA_INDEX */

			if (CHECK_NOWRITE_SPACE(DNGL_RING_RPTR(pciedev->dtoh_rxcpl),
				WRT_PEND(pciedev->dtoh_rxcpl),
				RING_MAX_ITEM(pciedev->dtoh_rxcpl))) {
				return FALSE;
			}

			/* Check if local buf pool is available */
			if (!CIR_BUFPOOL_AVAILABLE(pciedev->dtoh_rxcpl->cbuf_pool)) {
				return FALSE;
			}

			txdesc_needed = MIN_TXDESC_AVAIL + 2;
			rxdesc_needed = MIN_RXDESC_AVAIL + 3;
			*msg_handler = pciedev_process_d2h_rxpyld;
			break;
			}
#ifdef PCIE_DMAXFER_LOOPBACK
		case MSG_TYPE_LPBK_DMAXFER_PYLD:
			txdesc_needed = MIN_TXDESC_AVAIL;
			rxdesc_needed = MIN_RXDESC_AVAIL;
			*msg_handler = pciedev_process_d2h_dmaxfer_pyld;
			break;
#endif // endif
		default:
			PCI_ERROR(("unknown message on D2H Rxq %d\n", msg->msg_type));
			PKTFREE(pciedev->osh, p, TRUE);
			*msg_handler = NULL;
			return TRUE;
	}
	if (txdesc < txdesc_needed || rxdesc < rxdesc_needed)
		return FALSE;
	return TRUE;
} /* pciedev_check_process_d2h_message */

/**
 * Called when data received on the wireless medium has completed DMA into host memory, so host
 * needs to be notified of that event by sending it an 'rx complete' message.
 */
static int
pciedev_process_d2h_rxpyld(struct dngl_bus *pciedev, void *p)
{
	uint8 ifidx = 0;
	uint16 pktlen_new;
	uint16 pktlen;
	uint32 bufid = 0;
	uint16 dataoffset = 0, haddr_meta_len = 0;
	dma64addr_t haddr = {0, 0};
	dma64addr_t haddr_meta = {0, 0};
	uint16 len = 0;
	uint16 rxpkt_meta_data_len = 0;
	uint16 rxcpl_id;
	rxcpl_info_t *p_rxcpl_info = NULL;
	bool	queue_rxcpl;
	bool	dot11 = TRUE;

#ifdef TEST_DROP_PCIEDEV_RX_FRAMES
	pciedev_test_drop_rxframe++;
	if (!PKTNEEDRXCPL(pciedev->osh, p)) {
		pciedev_test_drop_norxcpl++;
		if (pciedev_test_drop_norxcpl == pciedev_test_drop_norxcpl_max) {
			pciedev_test_drop_norxcpl = 0;
			pciedev_test_dropped_norxcpls++;
			PKTFREE(pciedev->osh, p, TRUE);
			return 0;
		}
	} else if (pciedev_test_drop_rxframe > pciedev_test_drop_rxframe_max) {
		pciedev_test_drop_rxframe = 0;
		pciedev_test_dropped_rxframes++;
		PKTFREE(pciedev->osh, p, TRUE);
		return 0;
	}
#endif /* TEST_DROP_PCIEDEV_RX_FRAMES */

	ifidx = PKTIFINDEX(pciedev->osh, p);

	/* remove cmn_msg_hdr_t added from proto_push */
	PKTPULL(pciedev->osh, p, sizeof(cmn_msg_hdr_t));

	/* pull the metadata from the packet */
	if (PKTDATAOFFSET(p)) {
		rxpkt_meta_data_len = PKTDATAOFFSET(p) * 4;
		PKTPULL(pciedev->osh, p, rxpkt_meta_data_len);
	}

	/* need to look for metadata if present */
	/* if metadata then setup the right length where the info needs to be sent to */
	if (!HDR_CONV()||!(PKTISRXFRAG(pciedev->osh, p))) {
	pktlen = PKTLEN(pciedev->osh, p);
	if (PKTISRXFRAG(pciedev->osh, p)) {
		pktlen += PKTFRAGUSEDLEN(pciedev->osh, p);
		haddr = pciedev_get_haddr_from_lfrag(pciedev, p, &bufid, &haddr_meta,
			&haddr_meta_len, &dataoffset);
	} else {
		/* host addr not valid : return from local array */
		/* Account for the data offset coming form pcie dma here */
		dataoffset = pciedev->d2h_dma_scratchbuf_len;
		pciedev_get_host_addr(pciedev, &bufid, &len, &haddr, &haddr_meta,
			&haddr_meta_len);
		/* check this out */
		rxcpl_id = PKTRXCPLID(pciedev->osh, p);
		if ((haddr.loaddr != NULL) && (rxcpl_id == 0)) {
			p_rxcpl_info = bcm_alloc_rxcplinfo();
			if (p_rxcpl_info == NULL) {
				PCI_ERROR(("HOST RX BUF: RXCPL ID not free\n"));
				/* try to send an error to host */
				PKTFREE(pciedev->osh, p, TRUE);
				ASSERT(p_rxcpl_info);
				return 0;
			}
			PKTSETRXCPLID(pciedev->osh, p, p_rxcpl_info->rxcpl_id.idx);
		}
	}
	if (haddr.loaddr == NULL) {
		PCI_ERROR(("HOST RX BUF: ret buf not available \n"));
		/* try to send an error to host */
		PKTFREE(pciedev->osh, p, TRUE);
		return 0;
	}
	rxcpl_id = PKTRXCPLID(pciedev->osh, p);
	PKTRESETRXCPLID(pciedev->osh, p);
	p_rxcpl_info = bcm_id2rxcplinfo(rxcpl_id);
	if (p_rxcpl_info == NULL) {
		PCI_ERROR(("rxcpl_id is %d, and rxcpl_info is NULL, lb is %x\n",
			rxcpl_id, (uint32)p));
		ASSERT(p_rxcpl_info);
		PKTFREE(pciedev->osh, p, TRUE);
		return 0;
	}
	queue_rxcpl = PKTNEEDRXCPL(pciedev->osh, p);

	PCI_TRACE(("PKTDATAOFFSET is %d, metadata_len is %d, pktlen %d, pktlen_new %d, ifdx %d\n",
		PKTDATAOFFSET(p), haddr_meta_len, pktlen,  PKTLEN(pciedev->osh, p), ifidx));

	if ((pciedev->force_no_rx_metadata) || ((rxpkt_meta_data_len >= PCIE_MEM2MEM_DMA_MIN_LEN) &&
		(rxpkt_meta_data_len > haddr_meta_len)))
	{
		PCI_TRACE(("don't have enough space at the host buffer\n"));
		haddr_meta_len = 0;
	}
	else {
		/* Push rx meta data onto the packet */
		PKTPUSH(pciedev->osh, p, rxpkt_meta_data_len);
		haddr_meta_len = rxpkt_meta_data_len;
	}
	pktlen_new = PKTLEN(pciedev->osh, p);

	/* pktlen could change due to pad bytes in return_haddr_pool */
	PCI_TRACE(("PKTDATAOFFSET is %d, metadata_len is %d, pktlen %d, pktlen_new %d, ifdx %d\n",
		PKTDATAOFFSET(p), haddr_meta_len, pktlen,  PKTLEN(pciedev->osh, p), ifidx));

	if (!(pciedev_tx_pyld(pciedev, p, (ret_buf_t *)&haddr,
		(pktlen_new + pciedev->d2h_dma_scratchbuf_len),
		(ret_buf_t *)&haddr_meta, haddr_meta_len, MSG_TYPE_RX_PYLD)))
	{
		PKTFREE(pciedev->osh, p, TRUE);
		PCI_ERROR(("pciedev_process_d2h_rxpyld: BAD ERROR: shouldn't happen, "
			"pciedev_tx_pyld shouldn't fail\n"));
		ASSERT(0);
	}
	dot11 = (PKT80211(p)) ? 1 : 0;
#if defined(WL_MONITOR) && !defined(WL_MONITOR_DISABLED)
	if (PKTMON(p)) {
		pciedev->pkt_noise = ((int8*)PKTTAG(p))[MON_PKTTAG_NOISE_IDX];
		pciedev->pkt_rssi = ((int8*)PKTTAG(p))[MON_PKTTAG_RSSI_IDX];
	}
#endif /* WL_MONITOR && !WL_MONITOR_DISABLED */
	}
	else { /* HDR_CONV() */
		/* remove HW rxstats of 4bytes and padding of 2 bytes */
		dataoffset = pciedev->d11rxoffset + 2;
		pktlen = PKTFRAGUSEDLEN(pciedev->osh, p);
		bufid = PKTFRAGPKTID(pciedev->osh, p);

		rxcpl_id = PKTRXCPLID(pciedev->osh, p);
		PKTRESETRXCPLID(pciedev->osh, p);
		p_rxcpl_info = bcm_id2rxcplinfo(rxcpl_id);

		if (p_rxcpl_info == NULL) {
			PCI_ERROR(("rxcpl_id is %x, and rxcpl_info is NULL, lb is %x\n",
				rxcpl_id, *(int*) p));
			ASSERT(p_rxcpl_info);
			PKTFREE(pciedev->osh, p, TRUE);
			return 0;
		}

		queue_rxcpl = PKTNEEDRXCPL(pciedev->osh, p);
		dot11 = (PKT80211(p)) ? 1 : 0;
		/* HDRCONV path we are done ! So, free up the packet */
		PKTRESETRXFRAG(pciedev->osh, p);
		PKTFREE(pciedev->osh, p, FALSE);
		/* crash any invalid refs after FREE */
		p = NULL;
	}

	BCM_RXCPL_CLR_IN_TRANSIT(p_rxcpl_info);
	BCM_RXCPL_SET_VALID_INFO(p_rxcpl_info);

	p_rxcpl_info->host_pktref = bufid;
	p_rxcpl_info->rxcpl_len.metadata_len_w = haddr_meta_len >> 2;
	p_rxcpl_info->rxcpl_len.dataoffset = dataoffset;
	p_rxcpl_info->rxcpl_len.datalen =  pktlen;
	p_rxcpl_info->ifidx = ifidx;
	p_rxcpl_info->rxcpl_id.dot11 = dot11 ? 1 : 0;

	PCI_TRACE(("bufid: 0x%04x, metadata_len_w %d(%d), datalen %d, offset %d\n",
		p_rxcpl_info->host_pktref, p_rxcpl_info->rxcpl_len.metadata_len_w, haddr_meta_len,
		p_rxcpl_info->rxcpl_len.datalen, p_rxcpl_info->rxcpl_len.dataoffset));

	/* Transfer RX complete message with orig len */
	/* Host shouldnt see the pad bytes. so data offset should cover pad too */
	if (queue_rxcpl || BCM_RXCPL_FRST_IN_FLUSH(p_rxcpl_info)) {
		pciedev_queue_rxcomplete_local(pciedev, p_rxcpl_info, pciedev->dtoh_rxcpl,
			BCM_RXCPL_FRST_IN_FLUSH(p_rxcpl_info));
	}

	return 0;
} /* pciedev_process_d2h_rxpyld */

/**
 * When transmit packets have been processed by firmware (eg because they have been transmitted
 * to a wireless remote party, or because they have been suppressed by a channel switch), the host
 * may have to be notified, and suppressed packets may have to be refetched from the host for
 * retransmission.
 *
 *     p : buffer containing txstatus of *one* packet
 */
static int
pciedev_process_d2h_txmetadata(struct dngl_bus *pciedev, void *p)
{
	ret_buf_t haddr;
	uint16 metadata_len = 0;
	uint16 txstatus = 0;
	uint8 ifindx;
	uint32 pktid;
	uint16 ringid;
	uint8 metadatabuf_len = 0;
	bool txfrag = FALSE;
	int ret_val;
#if defined(BCM_DHDHDR) && defined(AMSDU_FRAG_OPT)
	bool dual_frag = FALSE;
	uint32 mfrag_pktid;
#endif /* BCM_DHDHDR && AMSDU_FRAG_OPT */

	haddr.low_addr = 0;
	haddr.high_addr = 0;

	/* assert that the packet has metadata on it */
	/* assert that the packet has big enough metadata len to carry it */
	ASSERT(PKTISTXFRAG(pciedev->osh, p));

#ifdef BCM_DHDHDR
	/* When BCM_DHDHDR is defined for txfrag packet we save the txstatus 2B in latest 2B of
	 * pkttag we don't leverage the PKTDATA to carry the txstatus because we may not have
	 * PKTDATA. For example the second msdu which has free the D3 buffer.
	 * Here use the saved txstatus 2B value in latest 2B of pkttag.
	 */
	if (BCMDHDHDR_ENAB()) {
		/* tx status is passed in unused msg hdr request id, pick it up */

		txstatus = PKTFRAGTXSTATUS(pciedev->osh, p);

#ifdef AMSDU_FRAG_OPT
		if (AMSDUFRAG_ENAB() && PKTISMULTIFRAG(pciedev->osh, p)) {
			dual_frag = TRUE;
			mfrag_pktid = PKTMFRAGPKTID(pciedev->osh, p, LB_FRAG2);
		}
#endif /* AMSDU_FRAG_OPT */
	} else
#endif /* BCM_DHDHDR */
	{
		/* XXX
		 * metadatabuf_len is the additional information that is added to the packet (piggy
		 * backed) on the rx path in order to deliver some information to the host like for
		 * example olympic needs rate at which every individual packet was sent - this
		 * additional data is then piggy backed to the data which adds up as the metabuflen.
		 */
		metadatabuf_len = PKTFRAGMETADATALEN(pciedev->osh, p);

		txstatus = ((cmn_msg_hdr_t *)PKTDATA(pciedev->osh, p))->request_id;

		PKTPULL(pciedev->osh, p, sizeof(cmn_msg_hdr_t)); /* skip common message header */

		/*
		 * When a packet is suppressed then the packet length itself is modified to 1byte or
		 * 4byte size which is nothing but the size of the txstatus attached to the packet
		 */
		metadata_len = PKTLEN(pciedev->osh, p);

		haddr.low_addr = 0;
		haddr.high_addr = 0;

		if (metadatabuf_len) {
			/* Access addr only if length is valid */
			haddr.low_addr = PKTFRAGMETADATA_LO(pciedev->osh, p);
			haddr.high_addr = PKTFRAGMETADATA_HI(pciedev->osh, p);
		}
	}

	PKTRESETHASMETADATA(pciedev->osh, (struct lbuf *)p);
	ifindx = PKTIFINDEX(pciedev->osh, p);
	pktid = PKTFRAGPKTID(pciedev->osh, p);
	ringid = PKTFRAGFLOWRINGID(pciedev->osh, p);

	PCI_TRACE(("haddr is 0x%04x, metadatabuf_len %d, metadata_len %d\n",
		haddr.low_addr, metadatabuf_len, metadata_len));

	if ((haddr.low_addr == 0) || (metadata_len  == 0) || (metadatabuf_len < metadata_len)) {
		PCI_TRACE(("metadata not solvagable address 0x%04x, "
			"meta_data_len %d, meta_data_buf_len %d\n",
			haddr.low_addr, metadata_len, metadatabuf_len));
		/* no need for the callback now */
		PKTFREE(pciedev->osh, p, FALSE);
		goto queue_txstatus;
	}

	/* Copy the right set of metadata for now */
	/* kludge to avoid the dma of txfrag.. */
	/* need something like detach the host buffer from this */
	if (PKTISTXFRAG(pciedev->osh, p)) {
		PKTRESETTXFRAG(pciedev->osh, p);
		txfrag = TRUE;
	}
	PCI_TRACE(("metadata len %d first word %d\n", metadata_len,
		*(uint32 *)((uint8 *)PKTDATA(pciedev->osh, p) + 4)));
	ret_val = pciedev_tx_pyld(pciedev, p, NULL, 0, &haddr, D2H_MSGLEN(pciedev, metadata_len),
		MSG_TYPE_TXMETADATA_PYLD);
	if (txfrag)
		PKTSETTXFRAG(pciedev->osh, p);
	if (!ret_val) {
		PKTFREE(pciedev->osh, p, FALSE);
		PCI_ERROR(("pciedev_process_d2h_txmetadata: BAD ERROR: shouldn't happen, "
			"pciedev_tx_pyld shouldn't fail\n"));
		ASSERT(0);
	}

queue_txstatus:
	/* Transfer RX complete message with orig len */
	/* Host shouldn't see the pad bytes. so data offset should cover pad too */
	PCI_TRACE(("generating the txstatus for pktid 0x%04x, ringid %d, ifindx %d\n",
		pktid, ringid, ifindx));
	pciedev_queue_txstatus(pciedev, pktid, ifindx, ringid, txstatus,
		metadata_len, pciedev->dtoh_txcpl);
#ifdef AMSDU_FRAG_OPT
	if (dual_frag == TRUE) {
		pciedev_queue_txstatus(pciedev, mfrag_pktid, ifindx, ringid, txstatus,
			metadata_len, pciedev->dtoh_txcpl);
	}
#endif // endif
	return 0;
} /* pciedev_process_d2h_txmetadata */

static uint16
pciedev_htoddma_deque(struct dngl_bus *pciedev, msgbuf_ring_t **msgbuf, uint8 *msgtype)
{
	uint16 rd_idx = pciedev->htod_dma_rd_idx;
	pciedev->htod_dma_rd_idx = (rd_idx + 1) % MAX_DMA_QUEUE_LEN_H2D;
	*msgbuf = pciedev->htod_dma_q[rd_idx].msgbuf;
	*msgtype = pciedev->htod_dma_q[rd_idx].msg_type;

	return (pciedev->htod_dma_q[rd_idx].len);
}

/**
 * Called when host needs to be notified of received payload available in host memory. Because of
 * AMPDU reordering, the sequence in which rxcpl messages are send to the host is important.
 */
void
pciedev_queue_rxcomplete_local(struct dngl_bus *pciedev, rxcpl_info_t *p_rxcpl_info,
	msgbuf_ring_t *ring, bool check_flush)
{
	uint32 count = 0;
	uint16 next_idx = 0;

	if (p_rxcpl_info == NULL) {
		PCI_ERROR(("Rxcomplete queue with bogus rxcplID\n"));
		ASSERT(0);
	}

	while (p_rxcpl_info != NULL) {
		if (BCM_RXCPL_IN_TRANSIT(p_rxcpl_info)) {
			BCM_RXCPL_SET_FRST_IN_FLUSH(p_rxcpl_info);
			if (pciedev->rxcpl_list_t)
				pciedev->rxcpl_list_t->rxcpl_id.next_idx = 0;
			p_rxcpl_info = NULL;
			continue;
		}
		next_idx = p_rxcpl_info->rxcpl_id.next_idx;
		if (BCM_RXCPL_VALID_INFO(p_rxcpl_info)) {
			if (pciedev->rxcpl_list_h == NULL) {
				pciedev->rxcpl_list_h = p_rxcpl_info;
				pciedev->rxcpl_list_t = p_rxcpl_info;
			} else {
				pciedev->rxcpl_list_t->rxcpl_id.next_idx =
					p_rxcpl_info->rxcpl_id.idx;
				pciedev->rxcpl_list_t = p_rxcpl_info;
			}
			count++;
		} else {
			/* Should we free the rxcpl if not valid */
			bcm_free_rxcplinfo(p_rxcpl_info);
		}
		p_rxcpl_info = bcm_id2rxcplinfo(next_idx);
	}

	pciedev->rxcpl_pend_cnt += count;

	/* need to check if this need to be queued or not */
	pciedev_queue_rxcomplete_msgring(pciedev, ring);
} /* pciedev_queue_rxcomplete_local */

static void
pciedev_queue_rxcomplete_msgring(struct dngl_bus *pciedev, msgbuf_ring_t *ring)
{
	cir_buf_pool_t *cpool = ring->cbuf_pool;
	host_rxbuf_cmpl_t *rxcmplt_h;
	uint32 avail_ring_entry;
	rxcpl_info_t *rxcpl_info;
	uint32 avail_host_ring_entry =
		CHECK_WRITE_SPACE(DNGL_RING_RPTR(ring), WRT_PEND(ring), RING_MAX_ITEM(ring));

	if (pciedev->rxcompletion_pend || !avail_host_ring_entry)
		return;

	while (pciedev->rxcpl_list_h != NULL) {
		rxcmplt_h = (host_rxbuf_cmpl_t *)pciedev_get_cirbuf_pool(ring, 1);
		if (rxcmplt_h == NULL)
			return;
		cpool->pend_item_cnt++;
		ASSERT(cpool->pend_item_cnt <= cpool->depth);

		rxcpl_info = pciedev->rxcpl_list_h;
		if (rxcpl_info == pciedev->rxcpl_list_t)
			pciedev->rxcpl_list_h = pciedev->rxcpl_list_t = NULL;
		else
			pciedev->rxcpl_list_h = bcm_id2rxcplinfo(rxcpl_info->rxcpl_id.next_idx);

		pciedev->rxcpl_pend_cnt--;

		/* Do we really need to do this */
		rxcmplt_h->cmn_hdr.msg_type = MSG_TYPE_RX_CMPLT;
		rxcmplt_h->rx_status_0 = 0;
		rxcmplt_h->rx_status_1 = 0;

		/* Fill up the completion header */
		rxcmplt_h->compl_hdr.status = 0;
		rxcmplt_h->compl_hdr.flow_ring_id = 0;

		/* For now we are setting only 802.3 or 802.11 in the flags */
		if (!rxcpl_info->rxcpl_id.dot11) {
			rxcmplt_h->flags = htol16(BCMPCIE_PKT_FLAGS_FRAME_802_3);
		} else {
			rxcmplt_h->flags = htol16(BCMPCIE_PKT_FLAGS_FRAME_802_11);
#if defined(WL_MONITOR) && !defined(WL_MONITOR_DISABLED)
			if (pciedev->monitor_mode) {
				int8 *pval = (int8*)&rxcmplt_h->rx_status_0;
				/* rx_status_0 field in rxcmplt_h is unused,
				 *  Using it to send noise and rssi to host
				 */
				pval[0] = pciedev->pkt_noise;
				pval[1] = pciedev->pkt_rssi;
				pval[2] = pciedev->d11rxoffset;
			}
#endif /* WL_MONITOR && !WL_MONITOR_DISABLED */
		}

		/* fill the useful part now */
		rxcmplt_h->cmn_hdr.flags = (rxcmplt_h->cmn_hdr.flags & (~MSGBUF_RING_INIT_PHASE)) |
			(ring->current_phase & MSGBUF_RING_INIT_PHASE);
		rxcmplt_h->cmn_hdr.if_id = rxcpl_info->ifidx;
		rxcmplt_h->cmn_hdr.request_id = htol32(rxcpl_info->host_pktref);
		rxcmplt_h->data_offset = htol16(rxcpl_info->rxcpl_len.dataoffset);
		rxcmplt_h->data_len = htol16(rxcpl_info->rxcpl_len.datalen);
		rxcmplt_h->metadata_len = htol16((rxcpl_info->rxcpl_len.metadata_len_w << 2));

		bcm_free_rxcplinfo(rxcpl_info);
		ASSERT(cpool->pend_item_cnt <= avail_host_ring_entry);
		avail_ring_entry = MIN(cpool->depth - cpool->r_pend, avail_host_ring_entry);
		/* do not queue more than MAXRXCMPLT tunable */
		avail_ring_entry = MIN(avail_ring_entry, pciedev->tunables[MAXRXCMPLT]);
#ifdef PCIEDEV_HOST_PKTID_AUDIT_ENABLED
	pciedev_host_pktid_audit(pciedev, rxcmplt_h->cmn_hdr.request_id, FALSE);
#endif /* PCIEDEV_HOST_PKTID_AUDIT_ENABLED */

#if defined(PCIE_M2M_D2H_SYNC)
		PCIE_M2M_D2H_SYNC_MARKER_INSERT(rxcmplt_h, RING_LEN_ITEMS(ring),
				pciedev->rxbuf_cmpl_epoch);
#endif /* PCIE_M2M_D2H_SYNC */
		if (cpool->pend_item_cnt >= avail_ring_entry) {
			ASSERT(cpool->pend_item_cnt <= avail_ring_entry);
			pciedev_xmit_rxcomplete(pciedev, ring);
			/* pciedev_xmit_rxcomplte updates wr_pending, so update host ring entry */
			avail_host_ring_entry = CHECK_WRITE_SPACE(DNGL_RING_RPTR(ring),
					WRT_PEND(ring), RING_MAX_ITEM(ring));
			if (pciedev->rxcompletion_pend || !avail_host_ring_entry)
				break;
		}
	} /* while loop */
} /* pciedev_queue_rxcomplete_msgring */

/** D2H transfers for message packets */
uint8
pciedev_xmit_msgbuf_packet(struct dngl_bus *pciedev, void *p, uint16  msglen, msgbuf_ring_t *msgbuf)
{
	ret_buf_t ret_buf = {0};
	cmn_msg_hdr_t *msg;
	uint8 current_phase = msgbuf->current_phase;
#if defined(PCIE_DMA_INDEX) && defined(SBTOPCIE_INDICES)
	/* Update host read pointer before trying to post */
	if (pciedev->d2h_readindx_dmablock->host_dmabuf_inited) {
		pciedev_sync_d2h_read_ptrs(pciedev, msgbuf);
	}
#endif // endif

	/* get ring space from d2h ring */
	ret_buf.low_addr = (uint32)pciedev_get_ring_space(pciedev, msgbuf, msglen);

	if (ret_buf.low_addr == NULL) {

		PCI_ERROR(("msgbuf name: DtoH%c pciedev_xmit_msgbuf_packet:"
			"DTOH ring not available \n", msgbuf->name[3]));
		return FALSE;
	}
	ret_buf.high_addr = (uint32)HIGH_ADDR_32(msgbuf->ringmem->base_addr);

	/* Inject the phase bit into flags in cmn_msg_hdr_t
	 * queued MSG_TYPE_RX_CMPLT and MSG_TYPE_TX_STATUS
	 * have the phase bit set already
	 */
	msg = (cmn_msg_hdr_t *)p;
	msg->flags = (msg->flags & (~MSGBUF_RING_INIT_PHASE)) |
		(current_phase & MSGBUF_RING_INIT_PHASE);

#if defined(PCIE_M2M_D2H_SYNC)
	if (msg->msg_type == MSG_TYPE_TX_STATUS)
		PCIE_M2M_D2H_SYNC_MARKER_REPLACE((host_txbuf_cmpl_t *)p,
			RING_LEN_ITEMS(msgbuf));
	else if (msg->msg_type == MSG_TYPE_RX_CMPLT)
		PCIE_M2M_D2H_SYNC_MARKER_REPLACE((host_rxbuf_cmpl_t *)p,
			RING_LEN_ITEMS(msgbuf));
	else
		PCIE_M2M_D2H_SYNC_MARKER_INSERT((ctrl_compl_msg_t *)p,
			RING_LEN_ITEMS(msgbuf), pciedev->ctrl_compl_epoch);
#endif /* PCIE_M2M_D2H_SYNC */

	pciedev_tx_msgbuf(pciedev, (void *) p, &ret_buf, msglen, msgbuf);

	/*
	 * Update IPC Data
	 * queued rx_cmplt and tx_status are already counted
	 * through pciedev.c
	 */
	if ((msg->msg_type != MSG_TYPE_TX_STATUS) && (msg->msg_type != MSG_TYPE_RX_CMPLT))
		pciedev->metrics.num_completions++;

	/* Change the phase bit here... */
	if (msgbuf->wr_pending == 0) {
		msgbuf->current_phase = (~current_phase) & MSGBUF_RING_INIT_PHASE;
		PCI_TRACE(("msgbuf name:  DtoH flipping the phase from 0x%02x to 0x%02x\n",
			current_phase, msgbuf->current_phase));
	}

	return TRUE;
} /* pciedev_xmit_msgbuf_packet */

int
pciedev_create_d2h_messages_tx(struct dngl_bus *pciedev, void *p)
{
#ifdef PCIE_PWRMGMT_CHECK
	if ((pciedev->in_d3_suspend) && (pciedev->no_device_inited_d3_exit)) {
		PKTFREE(pciedev->osh, p, TRUE);
		return 0;
	}
#endif /* PCIE_PWRMGMT_CHECK */

	return (pciedev_create_d2h_messages(pciedev, p, pciedev->dtoh_rxcpl));
}

/**
 * Prepare the Messages for D2H message transfers
 * Add header info according to message type and trigger actual dma
 */
int
pciedev_create_d2h_messages(struct dngl_bus *bus, void *p, msgbuf_ring_t *msgbuf)
{
	int ret = TRUE;
	uint16  msglen;
	uint8 msgtype;
	void * pkt;
	struct dngl_bus *pciedev = (struct dngl_bus *)bus;
	cmn_msg_hdr_t * msg;

#ifdef PCIE_PWRMGMT_CHECK
	if (pciedev->in_d3_suspend) {
		/* Send Host_Wake Signal */
		pciedev_host_wake_gpio_enable(pciedev, TRUE);
	}
#endif /* PCIE_PWRMGMT_CHECK */

	msg = (cmn_msg_hdr_t *)PKTDATA(pciedev->osh, p);
	msgtype = msg->msg_type;

	switch (msgtype) {
		case MSG_TYPE_LOOPBACK:
			PCI_TRACE(("MSG_TYPE_LOOPBACK: \n"));
			msglen = msgbuf->ringmem->len_items;
			pkt = MALLOC(pciedev->osh, msglen);
			if (pkt == NULL) {
				PCI_ERROR(("Could not allocate memory, malloc failed\n"));
				PCIEDEV_MALLOC_ERR_INCR(pciedev);
				return FALSE;
			}
			bcopy(PKTDATA(pciedev->osh, p), pkt, msglen);
			PKTFREE(pciedev->osh, p, TRUE);
			if (!pciedev_xmit_msgbuf_packet(pciedev, pkt, msglen,
				pciedev->dtoh_ctrlcpl)) {
				MFREE(pciedev->osh, pkt, msglen);
				return FALSE;
			}
			return TRUE;
			break;
		case MSG_TYPE_WL_EVENT:
			PCI_TRACE(("MSG_TYPE_WL_EVENT: \n"));
#ifdef UART_TRANSPORT
			/* check for MSGTRACE event and send that up to host over UART */
			if (pciedev->uarttrans_enab) {
				bcm_event_t *evtmsg = (bcm_event_t *) &msg[1];
				uint32 event_type = ntoh32_ua(&evtmsg->event.event_type);
				if (event_type < WLC_E_LAST &&
				    isset(pciedev->uart_event_inds_mask, event_type)) {
					h5_send_msgbuf((uchar *)msg, PKTLEN(pciedev->osh, p),
					               msgtype, pciedev->event_seqnum);
					++pciedev->event_seqnum;
					if (event_type == WLC_E_TRACE) {
						msgtrace_hdr_t *trace_msg;
						trace_msg = (msgtrace_hdr_t*)&evtmsg[1];
#ifdef LOGTRACE
						if (trace_msg->trace_type == MSGTRACE_HDR_TYPE_LOG)
							logtrace_sent();
#endif // endif
#ifdef MSGTRACE
						if (trace_msg->trace_type == MSGTRACE_HDR_TYPE_MSG)
							msgtrace_sent();
#endif // endif
					}
					PKTFREE(pciedev->osh, p, TRUE);
					return TRUE;
				}
			}
#endif /* UART_TRANSPORT */
			pciedev_queue_d2h_req(pciedev, p);
			if (pciedev->ioctl_pend != TRUE)
				pciedev_queue_d2h_req_send(pciedev);
			break;
		case MSG_TYPE_RX_PYLD:
			PCI_TRACE(("MSG_TYPE_RX_PYLD\n"));
			if (pciedev->in_d3_suspend &&
				++pciedev->in_d3_pktcount > PCIE_IN_D3_SUSP_PKTMAX) {
				PCI_TRACE(("In D3 Suspend. Dropping packet..\n"));
				PKTFREE(pciedev->osh, p, TRUE);
				return FALSE;
			}

			/* Chained rx packets should not come to this layer */
			if (PKTNEXT(pciedev->osh, p) != NULL) {
				pciedev_chained_rxpkt(pciedev, p);
				return FALSE;
			}

			pciedev_queue_d2h_req(pciedev, p);
			break;
		default:
			PCI_TRACE(("pciedev_create_d2h_messages:"
				"Unknown msgtype %d \n", msgtype));
			break;
	}
	return ret;
} /* pciedev_create_d2h_messages */

static void
pciedev_chained_rxpkt(struct dngl_bus *pciedev, void* p)
{

	PCI_TRACE(("%s: Drop chained Pkt! PktId:0x%d, NextPkt:%p\n",
		__FUNCTION__, PKTID(p), PKTNEXT(pciedev->osh, p)));

	pciedev->dropped_chained_rxpkts++;
	PKTFREE(pciedev->osh, p, TRUE);

}

/**
 * Return host address & bufid stored in the rx frag
 * Returned address is used by PCIe dma to transfer pending portions of payload + .3 hdr
 * returned address should account for local pktlength + pad + dma offset
 * Returned dataoffset specifies the start-addr of the payload in the host buffer
 */
static dma64addr_t
pciedev_get_haddr_from_lfrag(struct dngl_bus *pciedev, void* p, uint32* bufid,
	dma64addr_t *metaaddr, uint16 *metalen, uint16 *dataoffset)
{
	dma64addr_t haddr;
	uint32 addr_offset;
	uint32 pktlen = PKTLEN(pciedev->osh, p);

	*metalen = PKTFRAGMETADATALEN(pciedev->osh, p);
	/* Access addr only if length is valid */
	if (*metalen) {
		metaaddr->hiaddr = PKTFRAGMETADATA_HI(pciedev->osh, p);
		metaaddr->loaddr = PKTFRAGMETADATA_LO(pciedev->osh, p);
	}

	/* Packet structure in host
	 * Unused area
	 * dma_rx_offset(pciedev->d2h_dma_rxoffset)
	 * alignment for 4 bytes(pad)
	 * pending pkt from TCM + .3 hdr(pktlen)
	 * pkt dmaed by d11 dma(start addr at addr_offset)
	 * -----------------
	*/
	if (PKTFRAGUSEDLEN(pciedev->osh, p)) {
		/* Some part of host buffer already contains partial payload. So, we */
		/* need to stitch the packet up from the start of the existing payload. */

		/* We had reserved PKTRXFRAGSZ to be stitched back by mem2mem dma */
		/* But after hdr conversion, if length is less than that */
		/* account for unused area in data offset */
		/* Retrieve host address stored in rx frag */
		addr_offset = PKTFRAGDATA_LO(pciedev->osh, p, 1);
		if (pciedev->copycount) {
			/* Copy count bytes come from dongle */
			addr_offset = addr_offset + pciedev->copycount;
			*dataoffset = pciedev->copycount - pktlen;
		} else if (pciedev->tcmsegsz) {
			*dataoffset = pciedev->tcmsegsz - pktlen;
		} else {
			PCI_ERROR(("Both segsize & copycount not programmed \n"));
		}
		haddr.hiaddr = PKTFRAGDATA_HI(pciedev->osh, p, 1);
		/* account for data offset coming from pcie dma here */
		haddr.loaddr = (addr_offset - pktlen - pciedev->d2h_dma_scratchbuf_len);
	} else {
		/* No part of host buffer has been used yet. */
		/* account for data offset coming from pcie dma here */
		*dataoffset = pciedev->d2h_dma_scratchbuf_len;
		haddr.hiaddr = PKTFRAGDATA_HI(pciedev->osh, p, 1);
		haddr.loaddr = PKTFRAGDATA_LO(pciedev->osh, p, 1) - pciedev->tcmsegsz;
	}

#ifdef PCIE_PHANTOM_DEV
	/* Destination address has to be 4 byte aligned for phantom dev dmas */
	uint8 pad = 0;
	pad = ((uint32)haddr.loaddr & 3);

	if (pad) {
		pad = 4 - pad;
		/* Adjust host addrress */
		haddr.loaddr = haddr.loaddr - pad;
		PKTPUSH(pciedev->osh, p, pad);
	}
#endif // endif

	/* Buffer id */
	*bufid = PKTFRAGPKTID(pciedev->osh, p);

	/* host address for this frag was used by d11 dma */
	/* reset host addr avail flag */
	PKTRESETRXFRAG(pciedev->osh, p);

	/* Return dest addr for PCIe dma */
	return haddr;
} /* pciedev_get_haddr_from_lfrag */

void
pciedev_xmit_txstatus(struct dngl_bus *pciedev, msgbuf_ring_t *ring)
{
	cir_buf_pool_t *cpool = ring->cbuf_pool;
	void *p = NULL;

	if (cpool->pend_item_cnt == 0)
		return;

	if (pciedev->ioctl_pend == TRUE) {
		pciedev->txcompletion_pend = TRUE;
		return;
	}

	ASSERT(cpool->pend_item_cnt <=
		CHECK_WRITE_SPACE(DNGL_RING_RPTR(ring), WRT_PEND(ring), RING_MAX_ITEM(ring)));

	if ((PCIEDEV_GET_AVAIL_DESC(pciedev, DTOH, RXDESC) < MIN_RXDESC_AVAIL) ||
		(PCIEDEV_GET_AVAIL_DESC(pciedev, DTOH, TXDESC) < MIN_TXDESC_AVAIL)) {
		pciedev->txcompletion_pend = TRUE;
		PCI_TRACE(("tx cplt message failed :len %d\n",
			(RING_LEN_ITEMS(ring) * cpool->pend_item_cnt)));
		return;
	}

	p = cpool->buf + (cpool->r_pend * RING_LEN_ITEMS(ring));

#if defined(PCIE_DMA_INDEX) && defined(SBTOPCIE_INDICES)
	/* To optimize the sbtopcie access, flow ring read pointer
	 * updates are batched together and sent as lazy updates.
	 * A good place to do that would be along with txstatus xmit.
	 */
	if (ring->dma_d2h_indices_supported &&
		(pciedev->h2d_readindx_dmablock->host_dmabuf_inited)) {

		/* Update flow ring read pointers before sending txstatus */
		if (pciedev->txcpl_last_queued_ring)
			pciedev_sync_h2d_read_ptrs(pciedev,
				pciedev->txcpl_last_queued_ring);

		/* reset the last queue info */
		pciedev->txcpl_last_queued_ring = NULL;

	}
#endif /* PCIE_DMA_INDEX && SBTOPCIE_INDICES */
	if (!pciedev_xmit_msgbuf_packet(pciedev, p,
			(RING_LEN_ITEMS(ring) * cpool->pend_item_cnt), ring)) {
		pciedev->txcompletion_pend = TRUE;
		PCI_ERROR(("tx cplt message failed : not handled now , len %d\n",
			(RING_LEN_ITEMS(ring) * cpool->pend_item_cnt)));
		ASSERT(0);
		return;
	}

	/* Update IPC Stats */
	pciedev->metrics.num_txstatus_drbl++;
	pciedev->metrics.num_txstatus += cpool->pend_item_cnt;
	pciedev->metrics.num_completions += cpool->pend_item_cnt;

	cpool->r_pend = NEXTNTXP(cpool->r_pend, cpool->pend_item_cnt, cpool->depth);
	cpool->pend_item_cnt = 0;
	pciedev->txcompletion_pend = FALSE;
}

/**
 * Called back by the 'lbuf' subsystem for every pkt free
 * for tx frag, send out tx status.
 * for rx frags with host addr valid, reclaim host addresses
 */
bool
pciedev_lbuf_callback(void *arg, void* p)
{
	struct dngl_bus *pciedev = (struct dngl_bus *) arg;
	uint32 status = 0;
	uint16 seq = 0;
#ifdef PCIEDEV_HOST_PKTID_AUDIT_ENABLED
	uint32 pktid;
#endif // endif
	uint32 metadata_len = 0;

	if (PKTISTXFRAG(pciedev->osh, p)) {
		cmn_msg_hdr_t *cmn_msg;

		if (!PKTHASMETADATA(pciedev->osh, (struct lbuf *)p))
			return FALSE;

		/* Check if PKT has been TXstatus processed */
		if (PKTISTXSPROCESSED(pciedev->osh, p)) {
			/* Reset TXstatus processed state */
			PKTRESETTXSPROCESSED(pciedev->osh, p);

#ifdef BCM_DHDHDR
			if (BCMDHDHDR_ENAB()) {
				status = PKTFRAGTXSTATUS(pciedev->osh, p);
				seq = PKTWLFCSEQ(pciedev->osh, p);

				if (seq && ((status == WLFC_CTL_PKTFLAG_D11SUPPRESS) ||
					(status == WLFC_CTL_PKTFLAG_WLSUPPRESS))) {
					/*
					 * The WLC layer assigns d11 sequence numbers to packets. If
					 * a packet was suppressed, it might or might not have been
					 * assigned a valid sequence number. Therefore, need to
					 * check if seq number is valid from FW by checking the
					 * FROMFW flag before reusing it. Note that for PCIe FD, the
					 * usage of the 'FROMDRV' flag does not correspond with its
					 * name, since the driver was not compiled with
					 * PROP_TXSTATUS support.
					 * Symptom was sniff capture showed pkts with seq number
					 * of zero being re-used leading to OOO.
					 */
					if (WL_SEQ_GET_FROMFW(seq)) {
						WL_SEQ_SET_FROMDRV(seq, 1);
						WL_SEQ_SET_FROMFW(seq, 0);
					} else {
						/* Explicitly clear seq no to indicate 'no reuse' */
						seq = 0;
					}
				}
			} else
#endif /* BCM_DHDHDR */
			{
				metadata_len = PKTLEN(pciedev->osh, p);
				if (metadata_len == TXSTATUS_LEN) {
					status = *((uint8*)PKTDATA(pciedev->osh, p) +
						BCMPCIE_D2H_METADATA_HDRLEN);
				} else if (metadata_len >= (BCMPCIE_D2H_METADATA_HDRLEN +
					TLV_HDR_LEN + WLFC_CTL_VALUE_LEN_TXSTATUS)) {
					/* Copy txstatus which is the first 4 bytes after
					 * metdata header
					 */
					memcpy(&status, (char *)PKTDATA(pciedev->osh, p) +
						BCMPCIE_D2H_METADATA_HDRLEN + TLV_HDR_LEN,
						WLFC_CTL_VALUE_LEN_TXSTATUS);
					status = WL_TXSTATUS_GET_FLAGS(status);

					if ((status == WLFC_CTL_PKTFLAG_D11SUPPRESS) ||
						(status == WLFC_CTL_PKTFLAG_WLSUPPRESS)) {
						uint16 *buf = (uint16 *)((char *)PKTDATA(
							pciedev->osh, p) +
							BCMPCIE_D2H_METADATA_HDRLEN +
							TLV_HDR_LEN + WLFC_CTL_VALUE_LEN_TXSTATUS);
						memcpy((uint *)&seq, buf, WLFC_CTL_VALUE_LEN_SEQ);
						/*
						 * The WLC layer assigns d11 sequence numbers to
						 * packets. If a packet was suppressed, it might or
						 * might not have been assigned a valid sequence
						 * number. Therefore, need to check if seq number
						 * is valid from FW by checking the FROMFW flag
						 * before reusing it. Note that for PCIe FD, the
						 * usage of the 'FROMDRV' flag does not correspond
						 * with its name, since the driver was not compiled
						 * with PROP_TXSTATUS support.
						 * Symptom was sniff capture showed pkts with seq
						 * number of zero being re-used leading to OOO.
						 */
						if (WL_SEQ_GET_FROMFW(seq)) {
							WL_SEQ_SET_FROMDRV(seq, 1);
							WL_SEQ_SET_FROMFW(seq, 0);
						} else {
							/* Explicitly clear seq no to indicate
							 * 'no reuse'
							 */
							seq = 0;
						}
					}
				}
			}
		}
#ifdef BCM_DHDHDR
		else if (BCMDHDHDR_ENAB()) {
			PKTFRAGSETTXSTATUS(pciedev->osh, p, 0);
		}
#endif /* BCM_DHDHDR */

#ifdef AMSDU_FRAG_OPT
		if (AMSDUFRAG_ENAB() && PKTISMULTIFRAG(pciedev->osh, p)) {
			uint16 flowid, rindex1, rindex2;

			/* Initialize */
			flowid = PKTFRAGFLOWRINGID(pciedev->osh, p);

			rindex1 = PKTFRAGRINGINDEX(pciedev->osh, p);
			rindex2 = PKTMFRAGFETCHIDX(pciedev->osh, p, LB_FRAG2);
			/* TX status process for first frag */
			pciedev_update_txstatus(pciedev, status, rindex1,
				flowid, seq);

			/* Ignore return values of first call */
			/* TX status process for 2nd frag */
			if (pciedev_update_txstatus(pciedev, status, rindex2,
				flowid, seq)) {
#ifdef PCIEDEV_HOST_PKTID_AUDIT_ENABLED
				{
					uint16 pktid1, pktid2;
					/* Initialize pktids */
					pktid1 = PKTFRAGPKTID(pciedev->osh, p);
					pktid2 = PKTMFRAGPKTID(pciedev->osh, p, LB_FRAG2);
					/* audit the IDs */
					pciedev_host_pktid_audit(pciedev, pktid1, FALSE);
					pciedev_host_pktid_audit(pciedev, pktid2, FALSE);
				}
#endif /* PCIEDEV_HOST_PKTID_AUDIT_ENABLED */
				/* Reset the Metadata flag to avoid recursion of PKTFREE */
				PKTRESETHASMETADATA(pciedev->osh, (struct lbuf *) p);
				PKTFREE(pciedev->osh, p, FALSE);
				return TRUE;

			}
		} else
#endif /* AMSDU_FRAG_OPT */
		{
			if (pciedev_update_txstatus(pciedev, status,
				PKTFRAGRINGINDEX(pciedev->osh, p),
				PKTFRAGFLOWRINGID(pciedev->osh, p), seq)) {
#ifdef PCIEDEV_HOST_PKTID_AUDIT_ENABLED
				pciedev_host_pktid_audit(pciedev, pktid, FALSE);
#endif /* PCIEDEV_HOST_PKTID_AUDIT_ENABLED */
				/* Reset the Metadata flag to avoid recursion of PKTFREE */
				PKTRESETHASMETADATA(pciedev->osh, (struct lbuf *) p);
				PKTFREE(pciedev->osh, p, FALSE);
				return TRUE;
			}

		}

		if (!BCMDHDHDR_ENAB()) {
			/* check for tx frag */
			if (PKTHEADROOM(pciedev->osh, p) < 8) {
				PCI_ERROR(("PKTHEADROOM is less than needed 8, %d\n",
					PKTHEADROOM(pciedev->osh, p)));
				ASSERT(0);
			}
			PKTPUSH(pciedev->osh, p, sizeof(cmn_msg_hdr_t));
			cmn_msg = (cmn_msg_hdr_t *)PKTDATA(pciedev->osh, p);

			cmn_msg->msg_type = MSG_TYPE_TXMETADATA_PYLD;
			/* Passing tx status on request_id as it is unused internally */
			cmn_msg->request_id = status;
		}

		/* Why do we need this check */
		/* if (pktid != DMA_XFER_PKTID) */
		/* if the txstatus is pending for a packet should we reduce the count here */
		if (pciedev->d3_wait_for_txstatus) {
			pciedev->d3_wait_for_txstatus--;
			if (!pciedev->d3_wait_for_txstatus && pciedev->d3_ack_pending)
				pciedev_D3_ack(pciedev);
		}
		pciedev_queue_d2h_req(pciedev, p);

		if (dll_empty(&pciedev->active_prioring_list) &&
			(pciedev->dtoh_txcpl->buf_pool->pend_item_cnt)) {
			pciedev_xmit_txstatus(pciedev, pciedev->dtoh_txcpl);
		}
		return TRUE;
	} else if (!PKTISTXFRAG(pciedev->osh, p)) {
		uint16 rxcpl_id;
		rxcpl_info_t *p_rxcpl_info;

		rxcpl_id = PKTRXCPLID(pciedev->osh, p);
		if (rxcpl_id == 0)
			return FALSE;

		/* that means pkt did not go through pciedev rx path */
		/* see if this is carrying a chain of rxcplids  */
		p_rxcpl_info = bcm_id2rxcplinfo(rxcpl_id);

		if (!BCM_RXCPL_IN_TRANSIT(p_rxcpl_info))
			return FALSE;

		PKTRESETRXCPLID(pciedev->osh, p);
		BCM_RXCPL_CLR_IN_TRANSIT(p_rxcpl_info);
		BCM_RXCPL_CLR_VALID_INFO(p_rxcpl_info);
		/* call the queue logic, let it handle the dropping of rxcpl info */
		if (PKTNEEDRXCPL(pciedev->osh, p) || BCM_RXCPL_FRST_IN_FLUSH(p_rxcpl_info)) {
			pciedev_queue_rxcomplete_local(pciedev, p_rxcpl_info, pciedev->dtoh_rxcpl,
				BCM_RXCPL_FRST_IN_FLUSH(p_rxcpl_info));
		}
	}

	return FALSE;
} /* pciedev_lbuf_callback */

static void
pciedev_queue_txstatus(struct dngl_bus *pciedev, uint32 bufid, uint8 ifindx, uint16 ringid,
	uint16 txstatus, uint16 metadata_len, msgbuf_ring_t *ring)
{
	cir_buf_pool_t *cpool = ring->cbuf_pool;
	host_txbuf_cmpl_t *txcmplt_h;
	uint32 avail_ring_entry, avail_host_ring_entry;

	if (bufid == 0) {
#ifdef PCIEDEV_HOST_PKTID_AUDIT_ENABLED
		pciedev_host_pktid_audit(pciedev, bufid, FALSE);
#endif /* PCIEDEV_HOST_PKTID_AUDIT_ENABLED */
		PCI_ERROR(("pktid is NULL\n"));
		ASSERT(0);
		return;
	}

	ASSERT(cpool->pend_item_cnt <= cpool->depth);
	avail_host_ring_entry =
		CHECK_WRITE_SPACE(DNGL_RING_RPTR(ring), WRT_PEND(ring), RING_MAX_ITEM(ring));
	if (avail_host_ring_entry == 0) {
		PCI_ERROR(("No host ring entry available for txstatus\n"));
		ASSERT(0);
		return;
	}

	txcmplt_h = (host_txbuf_cmpl_t *)pciedev_get_cirbuf_pool(ring, 1);
	if (txcmplt_h == NULL) {
		PCI_ERROR(("No circular local buffer available for txstatus\n"));
		ASSERT(0);
		return;
	}
	cpool->pend_item_cnt++;

#if defined(PCIE_DMA_INDEX) && defined(SBTOPCIE_INDICES)
	/* If multiple flow rings gets queued up, sync up read pointers
	 * on every flow ring change.
	 * Maintain a head of line ring info;
	 * if ring changes sync up read pointer for the saved HOL ring
	 */
	{
		/* Get flow ring info from ringid */
		msgbuf_ring_t *flow_ring = &pciedev->flow_ring_msgbuf[ringid -
			BCMPCIE_H2D_MSGRING_TXFLOW_IDX_START];

		ASSERT(flow_ring);

		if (flow_ring != pciedev->txcpl_last_queued_ring) {
			/* Change of Flow rings. flush read pointer for prev ring */
			if (pciedev->txcpl_last_queued_ring) {
				pciedev_sync_h2d_read_ptrs(pciedev,
					pciedev->txcpl_last_queued_ring);
			}

			/* Update last queued ring
			 * Read index for this flow ring would be synced
			 * along with txstatus xmit.
			 */
			pciedev->txcpl_last_queued_ring = flow_ring;
		}
	}
#endif /* PCIE_DMA_INDEX && SBTOPCIE_INDICES */
	txcmplt_h->cmn_hdr.msg_type = MSG_TYPE_TX_STATUS;
	txcmplt_h->cmn_hdr.if_id = ifindx;
	txcmplt_h->cmn_hdr.flags = (txcmplt_h->cmn_hdr.flags & (~MSGBUF_RING_INIT_PHASE)) |
		(ring->current_phase & MSGBUF_RING_INIT_PHASE);
	txcmplt_h->compl_hdr.status = 0;
	txcmplt_h->compl_hdr.flow_ring_id = ringid;

	/* useful status */
	txcmplt_h->cmn_hdr.request_id = htol32(bufid);
	txcmplt_h->metadata_len = htol16(metadata_len);
	txcmplt_h->tx_status = htol16(txstatus);

	ASSERT(cpool->pend_item_cnt <= avail_host_ring_entry);

#ifdef PCIEDEV_HOST_PKTID_AUDIT_ENABLED
	pciedev_host_pktid_audit(pciedev, txcmplt_h->cmn_hdr.request_id, FALSE);
#endif /* PCIEDEV_HOST_PKTID_AUDIT_ENABLED */

#if defined(PCIE_M2M_D2H_SYNC)
	PCIE_M2M_D2H_SYNC_MARKER_INSERT(txcmplt_h, RING_LEN_ITEMS(ring),
		pciedev->txbuf_cmpl_epoch);
#endif /* PCIE_M2M_D2H_SYNC */

	avail_ring_entry = MIN(cpool->depth - cpool->r_pend, avail_host_ring_entry);

	if ((cpool->pend_item_cnt >= avail_ring_entry) ||
		(cpool->pend_item_cnt && dll_empty(&pciedev->active_prioring_list))) {
		pciedev_xmit_txstatus(pciedev, ring);

		/* Tx status xmit at wrap over can not fail
		 * make sure pend count ==0
		 */
		ASSERT(cpool->pend_item_cnt == 0);
	}
} /* pciedev_queue_txstatus */

static void
pciedev_xmit_rxcomplete(struct dngl_bus *pciedev, msgbuf_ring_t *ring)
{
	cir_buf_pool_t *cpool = ring->cbuf_pool;
	void *p = NULL;

	if (cpool->pend_item_cnt == 0) {
		return;
	}

	if (pciedev->ioctl_pend == TRUE) {
		pciedev->rxcompletion_pend = TRUE;
		return;
	}

	if ((PCIEDEV_GET_AVAIL_DESC(pciedev, DTOH, RXDESC) < MIN_RXDESC_AVAIL) ||
		(PCIEDEV_GET_AVAIL_DESC(pciedev, DTOH, TXDESC) < MIN_TXDESC_AVAIL)) {
		pciedev->rxcompletion_pend = TRUE;
		PCI_ERROR(("rx cplt message failed : size %d \n",
			(RING_LEN_ITEMS(ring) * cpool->pend_item_cnt)));
		return;
	}

	p = cpool->buf + (cpool->r_pend * RING_LEN_ITEMS(ring));
	if (!pciedev_xmit_msgbuf_packet(pciedev, p,
			(RING_LEN_ITEMS(ring) * cpool->pend_item_cnt), ring)) {
		pciedev->rxcompletion_pend = TRUE;
		PCI_ERROR(("rx cplt message failed : not handled now, size %d \n",
			(RING_LEN_ITEMS(ring) * cpool->pend_item_cnt)));
		ASSERT(0);
		return;
	}

	/* Update IPC Stats */
	pciedev->metrics.num_rxcmplt_drbl++;
	pciedev->metrics.num_rxcmplt += cpool->pend_item_cnt;
	pciedev->metrics.num_completions += cpool->pend_item_cnt;

	cpool->r_pend = NEXTNTXP(cpool->r_pend, cpool->pend_item_cnt, cpool->depth);
	cpool->pend_item_cnt = 0;
	pciedev->rxcompletion_pend = FALSE;
}

void
pciedev_queue_d2h_req_send(struct dngl_bus *pciedev)
{
	uint32 txdesc, rxdesc;
	void *p;

	if (pciedev->in_d3_suspend) {
		/* Do not service the queue if the device is in D3 suspend */
		PCI_TRACE(("pciedev_queue_d2h_req_send:In D3 Suspend\n"));
		return;
	}

	if ((pciedev->ioctl_pend == TRUE) && (pciedev->ioctl_ack_pend != TRUE)) {
		/* ioctl is pending waiting for resources to be avilable */
		/* so check it the IOCTL could be processed now */
		/* all the needed from the IOCTL request is already in dongle memory */
		pciedev_process_ioctl_pend(pciedev);
		if (pciedev->ioctl_pend) {
			PCI_ERROR(("not enough resources to process ioctl request\n"));
			return;
		}
	}

	/* If local pciedev_ring_cmplt_queue_t is NOT empty, process it */
	if (pciedev_ctrl_resp_q_avail(pciedev) < PCIEDEV_CNTRL_CMPLT_Q_SIZE) {
	        pciedev_process_ctrl_cmplt(pciedev);
	}

	if (pciedev->rxcompletion_pend == TRUE) {
		/* tried to send rxcompletion but looks like failed with no resources */
		/* so try it now */
		pciedev_xmit_rxcomplete(pciedev, pciedev->dtoh_rxcpl);
	}
	if (pciedev->txcompletion_pend == TRUE) {
		/* tried to send rxcompletion but looks like failed with no resources */
		/* so try it now */
		pciedev_xmit_txstatus(pciedev, pciedev->dtoh_txcpl);
	}

#ifdef PCIE_DMA_INDEX
#ifndef SBTOPCIE_INDICES
	if ((pciedev->dma_d2h_indices_pending == 1) &&
		pktq_empty(&pciedev->d2h_req_q)) {
		/* DMAing indices is pending waiting for resources to be available */
		/* so try it now */
		pciedev_dma_set_indices(pciedev, NULL);

	}
#endif /* !SBTOPCIE_INDICES */
#endif /* PCIE_DMA_INDEX */

	/* check if there are any pending rxcompletes waiting for space on local buf */
	pciedev_queue_rxcomplete_msgring(pciedev, pciedev->dtoh_rxcpl);

	/* Check if any pending fetch process that were queued up and need to be processed */
	if  (pciedev->fetch_req_pend_list.head)
		pciedev_process_pending_fetches(pciedev);

	if (pktq_empty(&pciedev->d2h_req_q))
		return;

	txdesc = PCIEDEV_GET_AVAIL_DESC(pciedev, DTOH, TXDESC);
	rxdesc = PCIEDEV_GET_AVAIL_DESC(pciedev, DTOH, RXDESC);

	while ((txdesc > MIN_TXDESC_AVAIL) && (rxdesc > MIN_RXDESC_AVAIL)) {
		d2h_msg_handler msg_handler;
		p = pktq_pdeq(&pciedev->d2h_req_q, 0);
		if (p == NULL)
			break;
		if (!pciedev_check_process_d2h_message(pciedev, txdesc, rxdesc, p, &msg_handler)) {
			PCI_TRACE(("not enough resources to send event to Host\n"));
			pktq_penq_head(&pciedev->d2h_req_q, 0, p);
			break;
		}
		if (msg_handler) {
			(msg_handler)(pciedev, p);
		}
		/* Make this part of the ring data structure */
		if (pciedev->rxcompletion_pend | pciedev->ioctl_pend | pciedev->txcompletion_pend) {
			PCI_TRACE(("pends are Rx: %d, Tx: %d, IOCTL: %d\n",
				pciedev->rxcompletion_pend, pciedev->txcompletion_pend,
				pciedev->ioctl_pend));
			break;
		}

		txdesc = PCIEDEV_GET_AVAIL_DESC(pciedev, DTOH, TXDESC);
		rxdesc = PCIEDEV_GET_AVAIL_DESC(pciedev, DTOH, RXDESC);
	}

	/* Send rx completes for the chain of pkts */
	if (pciedev->dtoh_rxcpl->cbuf_pool->pend_item_cnt < PCIEDEV_MIN_RXCPLGLOM_COUNT) {
		pciedev_xmit_rxcomplete(pciedev, pciedev->dtoh_rxcpl);
	} else {
		if (!pciedev->glom_timer_active) {
			dngl_add_timer(pciedev->glom_timer, MSGBUF_TIMER_DELAY, FALSE);
			pciedev->glom_timer_active = TRUE;
		}
	}
} /* pciedev_queue_d2h_req_send */

void
pciedev_queue_d2h_req(struct dngl_bus *pciedev, void *p)
{
	if (pktq_full(&pciedev->d2h_req_q)) {
		PCI_ERROR(("pciedev_queue_d2h_req full\n"));

		/* Reset the Metadata flag to avoid recursion of PKTFREE */
		PKTRESETHASMETADATA(pciedev->osh, (struct lbuf *) p);
		/* Callbacks should not be invoked again */
		PKTFREE(pciedev->osh, p, FALSE);
		return;
	}
	pktq_penq(&pciedev->d2h_req_q, 0, p);
}

/**
 * Starts pumping a message to the host. Is called when eg host needs to be notified of payload
 * that was received by the dongle. Calls both dma_rxfast() and dma_txfast() because of M2M DMA.
 */
int
pciedev_tx_pyld(struct dngl_bus *pciedev, void *p, ret_buf_t *data_buf, uint16 data_len,
	ret_buf_t *meta_data_buf, uint16 meta_data_len, uint8 msgtype)
{
	dma64addr_t addr = {0, 0};
	uint16 msglen;

	/* check if both meta_data_len and data_len are zeros */

	/* somehow some of the corerevs can't deal with DMA less than 8 bytes */
	if ((meta_data_len > 0) && (meta_data_len < PCIE_MEM2MEM_DMA_MIN_LEN)) {
		meta_data_len = PCIE_MEM2MEM_DMA_MIN_LEN;
		ASSERT(0);
	}
	if ((data_len > 0) && (data_len < PCIE_MEM2MEM_DMA_MIN_LEN)) {
		data_len = PCIE_MEM2MEM_DMA_MIN_LEN;
	}

	msglen = data_len + meta_data_len;
	if (msglen < PCIE_MEM2MEM_DMA_MIN_LEN) {
		PCI_ERROR(("msglen is %d, data_len %d, meta_data_len %d\n",
			msglen, data_len, meta_data_len));
		ASSERT(0);
	}

	pciedev_dma_tx_account(pciedev, msglen, msgtype, 0, NULL); /* updates bookkeeping */

#ifdef PCIE_PHANTOM_DEV
	/* For phantom devices PCIE_ADDR_OFFSET is still required for sdio dma */
	/* for pcie dma devices, outbound dma is hardwired */
	/* to take in only pcie addr as dest space. so no need to give extra offset */
	if (pciedev->phtm) {
		PHYSADDR64HISET(addr, data_buf->high_addr);
		PHYSADDR64LOSET(addr, data_buf->low_addr);
		pcie_phtm_tx(pciedev->phtm, p, addr, msglen, l_msgtype);
	}
#else
	/* Program RX descriptor for rxoffset */
	/* offset is handled through dataoffset portion of rx completion  for rx payloads */
	/* None of the other messages have this dataoffset field */
	if ((pciedev->d2h_dma_scratchbuf_len) && (msgtype != MSG_TYPE_RX_PYLD)) {
		if (dma_rxfast(pciedev->d2h_di, pciedev->d2h_dma_scratchbuf, 8)) {
			TRAP_DUE_DMA_RESOURCES(("pciedev_tx_pyld: dma_rxfast failed "
			"for rxoffset, descs_avail %d\n",
				PCIEDEV_GET_AVAIL_DESC(pciedev, DTOH, RXDESC)));
		}
	}
	/* check if there is a need to transfer metadata */
	if (meta_data_len) {
		meta_data_buf->high_addr = meta_data_buf->high_addr & ~PCIE_ADDR_OFFSET;
		PHYSADDR64HISET(addr, meta_data_buf->high_addr);
		PHYSADDR64LOSET(addr, meta_data_buf->low_addr);
		if (dma_rxfast(pciedev->d2h_di, addr, (uint32)meta_data_len)) {
			TRAP_DUE_DMA_RESOURCES(("pciedev_tx_pyld : dma_rxfast"
				"failed for meta data, descs avail %d\n",
				PCIEDEV_GET_AVAIL_DESC(pciedev, DTOH, RXDESC)));
		}
	}
	/* check if there is a need to transfer real data */
	if (data_len) {
		data_buf->high_addr = data_buf->high_addr & ~PCIE_ADDR_OFFSET;
		PHYSADDR64HISET(addr, data_buf->high_addr);
		PHYSADDR64LOSET(addr, data_buf->low_addr);
		if (dma_rxfast(pciedev->d2h_di, addr, (uint32)data_len)) {
			TRAP_DUE_DMA_RESOURCES(("pciedev_tx_pyld : dma_rxfast failed for data, "
				"descs avail %d\n",
				PCIEDEV_GET_AVAIL_DESC(pciedev, DTOH, RXDESC)));
		}
	}
	/* how do we make sure pktlen of p is greater than metadata len */

	/* should we check the pkttotlen instead */
	if ((PKTLEN(pciedev->osh, p) < msglen) && (msglen == PCIE_MEM2MEM_DMA_MIN_LEN)) {
		PCI_INFORM(("ALIGN: dmalen(%d) < minimum(%d), tailroom %d, msgtype %d\n",
			PKTLEN(pciedev->osh, p), msglen,  PKTTAILROOM(pciedev->osh, p), msgtype));
		/* to cover one of the HW wars we need packet len to be a min of 8 */
		if (PKTTAILROOM(pciedev->osh, p) < msglen - pkttotlen(pciedev->osh, p)) {
			PCI_ERROR(("BAD CASE: dmalen(%d) < minimum(%d), tailroom %d, msgtype %d\n",
				PKTLEN(pciedev->osh, p), msglen,
				PKTTAILROOM(pciedev->osh, p), msgtype));
		}
		PKTSETLEN(pciedev->osh, p, msglen);
	}

	if (dma_txfast(pciedev->d2h_di, p, TRUE)) {
		PCI_ERROR(("pciedev_tx_pyld : dma_txfast failed\n"));
		ASSERT(0);
	}

#endif /* PCIE_PHANTOM_DEV */
	return TRUE;
} /* pciedev_tx_pyld */

/** Handle D2H dma transfers for messages and payload. Program RX and TX descriptors */
static int
pciedev_tx_msgbuf(struct dngl_bus *pciedev, void *p, ret_buf_t *ret_buf,
	uint16 msglen, msgbuf_ring_t *msgbuf)
{
	cmn_msg_hdr_t *cmn_hdr = (cmn_msg_hdr_t *) p;
	dma64addr_t addr = {0, 0};
	uint32 phase_offset = 0;
	bool commit = TRUE;
	uint8 flags = 0;
	bool evnt_ctrl = 0;

	/* We could just check the msgbuf null or not */
	if (msgbuf->phase_supported) {
		phase_offset = sizeof(cmn_msg_hdr_t);
		flags |= PCIEDEV_INTERNAL_SENT_D2H_PHASE;
	}

	pciedev_dma_tx_account(pciedev, msglen, cmn_hdr->msg_type, flags, msgbuf);
	ASSERT(msglen >= (phase_offset + PCIE_MEM2MEM_DMA_MIN_LEN));

#ifndef PCIE_PHANTOM_DEV
	/* For phantom devices PCIE_ADDR_OFFSET is still required for sdio dma */
	/* for pcie dma devices, outbound dma is hardwired */
	/* to take in only pcie addr as dest space. so no need to give extra offset */
#endif // endif

#ifdef PCIE_PHANTOM_DEV
	PHYSADDR64HISET(addr, ret_buf->high_addr);
	PHYSADDR64LOSET(addr, ret_buf->low_addr + phase_offset);
	if (pciedev->phtm)
		pcie_phtm_tx(pciedev->phtm, p, addr, msglen, cmn_hdr->msgtype);
#else
	ret_buf->high_addr = ret_buf->high_addr & ~PCIE_ADDR_OFFSET;
	/* Program RX descriptor */
	if (pciedev->d2h_dma_scratchbuf_len) {
		if (dma_rxfast(pciedev->d2h_di, pciedev->d2h_dma_scratchbuf, 8)) {
			TRAP_DUE_DMA_RESOURCES(("msgbuf name: pciedev_tx_msgbuf: DtoH%c"
				"dma_rxfast failed for rxoffset, descs_avail %d\n",
				msgbuf->name[3], PCIEDEV_GET_AVAIL_DESC(pciedev, DTOH, RXDESC)));
		}
	}
	PHYSADDR64HISET(addr, ret_buf->high_addr);
	PHYSADDR64LOSET(addr, ret_buf->low_addr + phase_offset);
	/* rx buffer for data excluding the phase bits */
	if (dma_rxfast(pciedev->d2h_di, addr, (uint32)(msglen - phase_offset))) {
		TRAP_DUE_DMA_RESOURCES(("msgbuf name :DtoH  pciedev_tx_msgbuf :DtoH%c"
			"dma_rxfast failed for data, descs avail %d\n",
			msgbuf->name[3], PCIEDEV_GET_AVAIL_DESC(pciedev, DTOH, RXDESC)));
	}
	if (phase_offset) {
		/* rx buffer for phase offset */
		PHYSADDR64LOSET(addr, ret_buf->low_addr);
		if (dma_rxfast(pciedev->d2h_di, addr, phase_offset)) {
			TRAP_DUE_DMA_RESOURCES(("msgbuf name : DtoH%c  pciedev_tx_msgbuf :"
				"dma_rxfast failed for data, descs avail %d\n",
				msgbuf->name[3], PCIEDEV_GET_AVAIL_DESC(pciedev, DTOH, RXDESC)));
		}
	}

	if (phase_offset) {
		/* tx for data excluding the phase offset */
		PHYSADDR64HISET(addr, (uint32) 0);
		PHYSADDR64LOSET(addr, (uint32)p + phase_offset);
		if (dma_msgbuf_txfast(pciedev->d2h_di, addr, FALSE, msglen - phase_offset,
			TRUE, FALSE))
		{
			evnt_ctrl = PCIE_ERROR1;
			goto fail;
		}
		/* tx for phase offset */
		PHYSADDR64LOSET(addr, (uint32)p);
		if (dma_msgbuf_txfast(pciedev->d2h_di,
			addr, commit, phase_offset, FALSE, commit)) {
			evnt_ctrl = PCIE_ERROR2;
			goto fail;
		}
	} else {
		PHYSADDR64HISET(addr, (uint32) 0);
		PHYSADDR64LOSET(addr, (uint32)p);
		if (dma_msgbuf_txfast(pciedev->d2h_di, addr, commit, msglen, TRUE, commit))
		{
			evnt_ctrl = PCIE_ERROR3;
			goto fail;
		}
	}
#endif /* PCIE_PHANTOM_DEV */

fail:
	if (evnt_ctrl) {
		TRAP_DUE_DMA_RESOURCES(("pciedev_tx_msgbuf : dma fill failed %d\n", evnt_ctrl));
	}

	return TRUE;
} /* pciedev_tx_msgbuf */

void
pciedev_dma_tx_account(struct dngl_bus *pciedev, uint16 msglen, uint8 msgtype,
	uint8 flags, msgbuf_ring_t *msgbuf)
{
	uint16 wr_idx = pciedev->dtoh_dma_wr_idx;
	dma_queue_t *dmaq = &(pciedev->dtoh_dma_q[wr_idx]);

	/* Validate the index, what if we are overwriting an existing entry */
	dmaq->len       = msglen;
	dmaq->flags       = flags;
	dmaq->msg_type  = msgtype;
	dmaq->msgbuf  = msgbuf;

	pciedev->dtoh_dma_wr_idx = (wr_idx+1) % MAX_DMA_QUEUE_LEN_D2H;

	ASSERT(pciedev->dtoh_dma_wr_idx != pciedev->dtoh_dma_rd_idx);
}
#ifdef HOST_HDR_FETCH
/* Update flags for last entry in dma queue */
void
pciedev_dma_tx_account_update_flags(struct dngl_bus *pciedev, uint8 flags)
{
	/* Retrive previous entry */
	/* MOD op will work only for power of 2 MAX_DMA_QUEUE_LEN_D2H */
	uint16 wr_idx = (pciedev->dtoh_dma_wr_idx) ?
		(pciedev->dtoh_dma_wr_idx -1) : (MAX_DMA_QUEUE_LEN_D2H - 1);
	dma_queue_t *dmaq = &(pciedev->dtoh_dma_q[wr_idx]);
	dmaq->flags |= flags;
}
#endif // endif

/**
 * Called after a h2d DMA interrupt, when a new so-called 'internal' (=firmware generated) message
 * is available in device memory.
 */
static uint16
pciedev_handle_h2d_pyld(struct dngl_bus *pciedev, uint8 msgtype, void *p,
	uint16 pktlen, msgbuf_ring_t *msgbuf)
{
	switch (msgtype) {
		case MSG_TYPE_HOST_FETCH:
			pciedev_process_tx_payload(pciedev);
			break;

#ifdef PCIE_DMAXFER_LOOPBACK
		case MSG_TYPE_LPBK_DMAXFER_PYLD:
			break;
#endif /* PCIE_DMAXFER_LOOPBACK */

		default:
			PCI_ERROR(("Unknown internal a message type 0x%02x\n", msgtype));
			break;
	}
	return 0;
}

/**
 * Called when one or more new 'receive submit buffer' message from the host (in argument 'p') have
 * been received in device memory.
 *     p:      points at first message
 *     pktlen: length of the packet that was received (containing messages)
 */
uint16
pciedev_handle_h2d_msg_rxsubmit(struct dngl_bus *pciedev, void *p,
	uint16 pktlen, msgbuf_ring_t *msgbuf)
{
	uint8 msglen = RING_LEN_ITEMS(pciedev->htod_rx);

	ASSERT((pktlen % msglen) == 0);

	/* Increment IPC Data for rxsubmit */
	pciedev->metrics.num_submissions += pktlen / msglen;
	PCI_INFORM(("h2d_drbl:%d, new rxsubmit:%d, tot_num_submissions:%d\n",
		pciedev->metrics.num_h2d_doorbell, pktlen/msglen,
		pciedev->metrics.num_submissions));

	pciedev_add_to_inuselist(pciedev, p, pktlen/msglen); /* queues messages for processing */

	/* Invoke dma rxfill every time you get host rx buffers */
	if (BCMSPLITRX_ENAB())
		pktpool_invoke_dmarxfill(pciedev->pktpool_rxlfrag);

	return 0;
}

/**
 * Called when a new so-called 'internal' 'host fetch' message is available in device memory,
 * signalling that new data to transmit to the wireless medium is now available in dongle memory.
 * Parameter 'p' points at the related 'txbuf post' message.
 */
uint16
pciedev_handle_h2d_msg_txsubmit(struct dngl_bus *pciedev, void *p,
	uint16 pktlen, msgbuf_ring_t *msgbuf, uint16 f_idx, uint32 flags)
{
	uint8 msglen = RING_LEN_ITEMS(msgbuf);
	host_txbuf_post_t *txdesc;

	ASSERT((pktlen % msglen) == 0);

	BCM_REFERENCE(flags);

	/* Increment IPC Data for txsubmit */
	pciedev->metrics.num_submissions += pktlen / msglen;
	PCI_INFORM(("h2d_drbl:%d, new txsubmit:%d, tot_num_submissions:%d\n",
		pciedev->metrics.num_h2d_doorbell, pktlen/msglen,
		pciedev->metrics.num_submissions));

	while (pktlen) {
		txdesc = (host_txbuf_post_t *)p;

		ASSERT(txdesc->cmn_hdr.msg_type == MSG_TYPE_TX_POST);

		if (msgbuf->status & FLOW_RING_FLUSH_PENDING) {
			/* Check if there are resources to queue txstatus */
			if (!pciedev_resource_avail_for_txmetadata(pciedev, TXMETA_ENTRIES_DFLT)) {
				/* We cannot process/queue this fetched one now */
				return pktlen;
			} else {
				msgbuf->status |= FLOW_RING_NOFLUSH_TXUPDATE;
				msgbuf->flow_info.pktinflight++;
				pciedev->pend_user_tx_pkts++;

				if (!pciedev_update_txstatus(pciedev, BCMPCIE_PKT_FLUSH, f_idx,
					msgbuf->ringid, 0)) {
#ifdef PCIEDEV_HOST_PKTID_AUDIT_ENABLED
					pciedev_host_pktid_audit(pciedev,
						ltoh32(txdesc->cmn_hdr.request_id), TRUE);
#endif /* PCIEDEV_HOST_PKTID_AUDIT_ENABLED */
					pciedev_queue_txstatus(pciedev,
						ltoh32(txdesc->cmn_hdr.request_id),
						txdesc->cmn_hdr.if_id, msgbuf->ringid,
						BCMPCIE_PKT_FLUSH, 0, pciedev->dtoh_txcpl);
				}
				msgbuf->status &= ~FLOW_RING_NOFLUSH_TXUPDATE;
			}
		} else if (!(msgbuf->status & FLOW_RING_SUP_PENDING)) {
			pciedev_process_tx_post(pciedev, p, msglen, msgbuf->ringid, f_idx);
		} else {
			/* If we are already in suppress, no need to push these packets,
			 * adjust rd pending and move on
			 */
			clrbit(msgbuf->inprocess, MODULO_RING_IDX(f_idx, msgbuf));

			pciedev_adjust_flow_fetch_ptr(msgbuf, f_idx);
		}
		f_idx = (f_idx + 1) %  RING_MAX_ITEM(msgbuf);
		pktlen = pktlen - msglen;
		p = p + msglen;
	}

#ifdef PKTC_TX_DONGLE
	/* We can send the chained packets here */
	if (pciedev->pkthead) {
		/* Forward the tx packets to the wireless subsystem */
		dngl_sendup(pciedev->dngl, pciedev->pkthead, pciedev->pkt_chain_cnt);
		pciedev->pkt_chain_cnt = 0;
		pciedev->pkthead = pciedev->pkttail = NULL;
	}
#endif /* PKTC_TX_DONGLE */
	return pktlen;
} /* pciedev_handle_h2d_msg_txsubmit */

/**
 * Called when new transmit payload data is available in dongle memory, accompanied by a message
 * (in parameter 'p') that originated from firmware. This means that payload data can be forwarded
 * towards the wireless subsystem.
 *
 * 1. Decode message in 'p'
 * 2. Get an lfrag from the pool, this lfrag will be fed to the wireless subsystem
 * 3. Update lfrag details
 * 4. Forward lfrag to wl layer
 */
static void
pciedev_process_tx_post(struct dngl_bus *pciedev, void* p, uint16 msglen,
	uint16 ringid, uint16 fetch_idx)
{
	host_txbuf_post_t * txdesc;
	uint32 pktid;
	uint8 priority;
	uint8 exempt;
	/* Find a way to point to TXOFF in pciedev */
	/* Need 202 bytes of headroom for TXOFF, 22 bytes for amsdu path */
	uint16 headroom = 224;	/* TXOFF + amsdu headroom */
	void *lfrag;
	const uint8 hdrlen = ETHER_HDR_LEN;
	msgbuf_ring_t *flow_ring = &pciedev->flow_ring_msgbuf[ringid -
		BCMPCIE_H2D_MSGRING_TXFLOW_IDX_START];
#ifdef PKTC_TX_DONGLE
	struct ether_header *eh;
	bool break_chain = FALSE;
#endif // endif

	txdesc = (host_txbuf_post_t *)p;
#ifdef PKTC_TX_DONGLE
	eh = (struct ether_header *)txdesc->txhdr;
	if (ntoh16(eh->ether_type) == ETHER_TYPE_802_1X)
		break_chain = TRUE;
#endif // endif
	pktid = ltoh32(txdesc->cmn_hdr.request_id);
	priority = (txdesc->flags &
		BCMPCIE_PKT_FLAGS_PRIO_MASK) >> BCMPCIE_PKT_FLAGS_PRIO_SHIFT;
	exempt = (txdesc->flags >> BCMPCIE_PKT_FLAGS_FRAME_EXEMPT_SHIFT) &
		BCMPCIE_PKT_FLAGS_FRAME_EXEMPT_MASK;

	/* nsegs supported is 1....err */

	/* Allocate a lbuf_frag  with hdrlen + headroom */
#ifndef BCMFRAGPOOL
	lfrag = PKTGETLF(pciedev->osh, headroom + hdrlen, TRUE, lbuf_frag);
	if (lfrag == NULL) {
		/* No free packets in heap. Just drop this packet. */
		PCI_TRACE(("pciedev_process_tx_post: %d: No free packets"
		"in the pool. Dropping packets here\n",
			__LINE__));
		pciedev->dropped_txpkts ++;
		return;
	}
#else
#ifdef BCM_DHDHDR
	if (BCMDHDHDR_ENAB())
		lfrag = pktpool_lfrag_get(pciedev->pktpool_lfrag, D3_LFRAG_BUF_POOL);
	else
#endif /* BCM_DHDHDR */
		lfrag = pktpool_lfrag_get(pciedev->pktpool_lfrag, NULL);
	if (lfrag == NULL) {
		/* No free packets in the pool. Just drop this packet. */
		PCI_ERROR(("pciedev_process_tx_post:%d: No free packets in the pool. "
			"Dropping packets here\n", __LINE__));
		flow_ring->flow_info.pktinflight++;
		pciedev->pend_user_tx_pkts++;
		pciedev_update_txstatus(pciedev, WLFC_CTL_PKTFLAG_WLSUPPRESS,
			fetch_idx, ringid, 0);
		pciedev->dropped_txpkts ++;
		return;
	}
#ifdef BCM_DHDHDR
	if (BCMDHDHDR_ENAB())
		headroom = PKTLEN(pciedev->osh, lfrag) - ETHER_HDR_LEN;
#endif /* BCM_DHDHDR */
#endif /* BCMFRAGPOOL */

#ifdef PCIEDEV_HOST_PKTID_AUDIT_ENABLED
	pciedev_host_pktid_audit(pciedev, pktid, TRUE);
#endif /* PCIEDEV_HOST_PKTID_AUDIT_ENABLED */

	PKTPULL(pciedev->osh, lfrag, headroom);	/* Push headroom */

	/* Copy ether header to lfrag */
	ehcopy32(txdesc->txhdr, PKTDATA(pciedev->osh, lfrag));

	/* Set frag params */
	PKTTAG_SET_VALUE(lfrag, exempt << 16); /* WLF_EXEMPT_MASK is shifted 16 bits */
	PKTSETLEN(pciedev->osh, lfrag, hdrlen);	/* Set Len */
	PKTSETFRAGPKTID(pciedev->osh, lfrag, pktid);
	PKTSETFRAGTOTNUM(pciedev->osh, lfrag, 1);
	PKTSETPRIO(lfrag, priority);
	PKTSETIFINDEX(pciedev->osh, lfrag, txdesc->cmn_hdr.if_id);
	PKTSETFRAGDATA_HI(pciedev->osh, lfrag, 1,
		(ltoh32(txdesc->data_buf_addr.high_addr) | 0x80000000));
	PKTSETFRAGDATA_LO(pciedev->osh, lfrag, 1,
		(ltoh32(txdesc->data_buf_addr.low_addr)));
	PKTSETFRAGLEN(pciedev->osh, lfrag, 1, ltoh16(txdesc->data_len));

	/* Set tot len and tot fragment count */
	PKTSETFRAGTOTLEN(pciedev->osh, lfrag, ltoh16(txdesc->data_len));

	PKTSETFRAGFLOWRINGID(pciedev->osh, lfrag, ringid);
	flow_ring->flow_info.pktinflight++;
	PKTFRAGSETRINGINDEX(pciedev->osh, lfrag, fetch_idx);
	pciedev->pend_user_tx_pkts++;

	/* Should be based on a config option to use it as meta data or regular frag */
	/* don't need to set the 63rd bit, as this is handled by the mem2mem DMA */
	if (txdesc->metadata_buf_len) {
		PKTSETFRAGMETADATA_HI(pciedev->osh, lfrag,
			(ltoh32(txdesc->metadata_buf_addr.high_addr)));
		PKTSETFRAGMETADATA_LO(pciedev->osh, lfrag,
			(ltoh32(txdesc->metadata_buf_addr.low_addr)));
		PKTSETFRAGMETADATALEN(pciedev->osh, lfrag,
			ltoh16(txdesc->metadata_buf_len));
	} else {
#ifdef TEST_USE_DMASCRATCH_AS_METADATA_HDR
		PKTSETFRAGMETADATA_HI(pciedev->osh, lfrag, pciedev->d2h_dma_scratchbuf.hiaddr);
		PKTSETFRAGMETADATA_LO(pciedev->osh, lfrag, pciedev->d2h_dma_scratchbuf.loaddr);
		PKTSETFRAGMETADATALEN(pciedev->osh, lfrag, pciedev->d2h_dma_scratchbuf_len);
#endif // endif
	}
	/* Metadata is handled, doesn't matter if the host has given buffer or not */
	PKTSETHASMETADATA(pciedev->osh, (struct lbuf *)lfrag);

	PCI_TRACE(("frag tot len %d, totlen %d, lfrag is %x\n",
		PKTFRAGTOTLEN(pciedev->osh, lfrag),
		pkttotlen(pciedev->osh, lfrag), (uint32)lfrag));

	if (pciedev->lpback_test_mode) {
		pciedev->lpback_test_drops++;
		PKTFREE(pciedev->osh, lfrag, TRUE);
		return;
	}
	if ((flow_ring->flow_info.flags & FLOW_RING_FLAG_PKT_REQ) ||
		(isset(flow_ring->reuse_sup_seq, MODULO_RING_IDX(fetch_idx, flow_ring))))
		pciedev_push_pkttag_tlv_info(pciedev, lfrag, flow_ring, fetch_idx);

#ifdef PKTC_TX_DONGLE
	/* Chain this packet to the existing chain */
	if (break_chain) {
		PKTSETCLINK(lfrag, NULL);
		/* Forward the tx packets to the wireless subsystem */
		dngl_sendup(pciedev->dngl, lfrag, 1);
	} else {
		if (pciedev->pkttail) {
			PKTSETCLINK(pciedev->pkttail, lfrag);
			pciedev->pkttail = lfrag;
		} else {
			pciedev->pkthead = pciedev->pkttail = lfrag;
		}
		PKTSETCLINK(pciedev->pkttail, NULL);
		pciedev->pkt_chain_cnt++;
	}
#else
	/* No packet chaining. Send it out immediately */
	PKTSETCLINK(lfrag, NULL);
	dngl_sendup(pciedev->dngl, lfrag); /* forward the tx packets to the wireless subsystem */
#endif /* PKTC_TX_DONGLE */
} /* pciedev_process_tx_post */

/**
 * After the host notified the device that new message(s) or non inline ioctl requests are available
 * in host memory, the device needs to pull these message(s) into local memory. This function kicks
 * off the DMA transfer for that.
 */
static void pciedev_h2dmsgbuf_dma(struct dngl_bus *pciedev, dma64addr_t src,
	uint16 src_len, uint8 *dst, uint8 *dummy_rxoff, msgbuf_ring_t *msgbuf, uint8 msgtype)
{
	uint16 wr_idx;
#ifdef PCIE_PHANTOM_DEV
	if (pciedev->phtm)
		pcie_phtm_msgbuf_dma(pciedev->phtm, dst, src, src_len);
#else
	uint16 dma_len, rx_len;
	bool evnt_ctrl = 0;
	dma64addr_t addr = {0, 0};

	/* CRWLPCIEGEN2-97 */
	if (src_len  <  PCIE_MEM2MEM_DMA_MIN_LEN)
		dma_len = PCIE_MEM2MEM_DMA_MIN_LEN;
	else
		dma_len = src_len;

	rx_len = dma_len;
#ifdef PCIE_PWRMGMT_CHECK
	if (pciedev->in_d3_suspend) {
		PCI_TRACE(("pciedev_h2dmsgbuf_dma:  IN D3 Suspend!\n"));
		ASSERT(0);

		return;
	}
#endif /* PCIE_PWRMGMT_CHECK */

	if (!pciedev->force_ht_on) {
		PCI_TRACE(("%s: requesting force HT for this core\n", __FUNCTION__));
		pciedev->force_ht_on = TRUE;
		pciedev_manage_h2d_dma_clocks(pciedev);
	}

	PHYSADDR64LOSET(addr, (uint32)dst);
	if (pciedev->h2d_dma_rxoffset) {
		/* RX descriptor for rx offset */
		PHYSADDR64LOSET(addr, (uint32)dummy_rxoff);
		if (dma_rxfast(pciedev->h2d_di, addr, 8)) {
			evnt_ctrl = PCIE_ERROR1;
		}

		/* actual rx decriptor */
		PHYSADDR64LOSET(addr, (uint32)dst);
		if (dma_rxfast(pciedev->h2d_di, addr, rx_len))
			evnt_ctrl = PCIE_ERROR2;
	} else {
		if (dma_rxfast(pciedev->h2d_di, addr, rx_len))
			evnt_ctrl = PCIE_ERROR3;
	}

	if (evnt_ctrl)
		PCI_ERROR(("pciedev_h2dmsgbuf_dma : dma_rxfast failed %d\n", evnt_ctrl));

	/* Src_len can't really be less than 8...should we check */
	/* TX descriptor */
	if (dma_msgbuf_txfast(pciedev->h2d_di, src, TRUE, dma_len, TRUE, TRUE))
		PCI_ERROR(("pciedev_h2dmsgbuf_dma : dma fill failed  \n"));
#endif /* PCIE_PHANTOM_DEV */

	/* Queue up pktlength */
	/* since we are using dummy rxoffset region, length is not obtained from rxoffset */
	wr_idx = pciedev->htod_dma_wr_idx;
	pciedev->htod_dma_wr_idx = (wr_idx + 1) % MAX_DMA_QUEUE_LEN_H2D;
	pciedev->htod_dma_q[wr_idx].len = src_len;
	pciedev->htod_dma_q[wr_idx].msg_type = msgtype;
	pciedev->htod_dma_q[wr_idx].msgbuf = msgbuf;

	ASSERT(pciedev->htod_dma_wr_idx != pciedev->htod_dma_rd_idx);
} /* pciedev_h2dmsgbuf_dma */

/**
 * Processing for d2h dma completion
 * Free up the packet if its payload or update circular buf pointers
 */
uint8
pciedev_handle_d2h_dmacomplete(struct dngl_bus *pciedev, void *cur_buf)
{
	uint8 ignore_cnt = 0;
	dma_queue_t *item;

	void *buf = cur_buf;

#ifdef HOST_HDR_FETCH
	/* Handle MAC DMA submissions if host header fetch enabled */
	if (pciedev_mac_dma_submit(pciedev, &buf, &item) != BCME_OK) {
		return 0;
	}
#else
	item = &pciedev->dtoh_dma_q[pciedev->dtoh_dma_rd_idx];
	pciedev->dtoh_dma_rd_idx = (pciedev->dtoh_dma_rd_idx+1) % MAX_DMA_QUEUE_LEN_D2H;

#endif /* HOST_HDR_FETCH */

#if defined(MSGTRACE) || defined(LOGTRACE)
	if (MESSAGE_PAYLOAD(item->msg_type)) {

		if (item->msg_type == MSG_TYPE_EVENT_PYLD) {
			bcm_event_t *bcm_hdr;

			bcm_hdr = (bcm_event_t*)PKTDATA(pciedev->osh, buf);
			if (bcm_hdr->event.event_type == hton32(WLC_E_TRACE)) {
					msgtrace_hdr_t *trace_msg;

					trace_msg = (msgtrace_hdr_t*)&bcm_hdr[1];
#ifdef LOGTRACE
					if (trace_msg->trace_type == MSGTRACE_HDR_TYPE_LOG) {
						logtrace_sent();
					}
#endif // endif
#ifdef MSGTRACE
					if (trace_msg->trace_type == MSGTRACE_HDR_TYPE_MSG) {
							msgtrace_sent();
					}
#endif // endif
			}
		}
	}

#endif /* defined(MSGTRACE) || defined(LOGTRACE) */
	if (MESSAGE_PAYLOAD(item->msg_type)) {
		/* Payload */
#ifdef PCIE_DMAXFER_LOOPBACK
		if (item->msg_type == MSG_TYPE_LPBK_DMAXFER_PYLD) {
			pciedev_process_do_dest_lpbk_dmaxfer_done(pciedev, buf);
		} else
#endif /* PCIE_DMAXFER_LOOPBACK */
		/* ifdef PCIE_DMA_INDEX */
		/* If write index of completions are updated generate doorbell */
		if (item->msg_type == MSG_TYPE_INDX_UPDATE) {
			ignore_cnt++;
#if defined(PCIE_D2H_DOORBELL_RINGER)
			if (item->msgbuf) {
				uint32 index;
				d2h_doorbell_ringer_t *ringer;
				index = item->msgbuf->ringid - BCMPCIE_H2D_COMMON_MSGRINGS;
				ringer = &pciedev->d2h_doorbell_ringer[index];
				ringer->db_fn(pciedev, ringer->db_info.value,
					ringer->db_info.haddr.low);
			}
			else
			/* could be multiple msgbufs ... wake DHD and let DHD figure */
#endif /* PCIE_D2H_DOORBELL_RINGER */
			{
				pciedev_generate_host_db_intr(pciedev,
				    PCIE_D2H_DB0_VAL, PCIE_DB_DEV2HOST_0);
			}
		} else
		/* endif of PCIE_DMA_INDEX */
		if (item->msg_type != MSG_TYPE_IOCT_PYLD) {
			/* IOCTL Payload free is handled with the IOCTL completion message */
			PKTFREE(pciedev->osh, buf, FALSE);
		}
	} else {
		/* free local message space */
		pciedev_ring_update_writeptr(item->msgbuf, item->len);
		if (item->msg_type == MSG_TYPE_TX_STATUS ||
				item->msg_type == MSG_TYPE_RX_CMPLT) {
			pciedev_free_cirbuf_pool(item->msgbuf, NULL, item->len);
			if (item->flags & PCIEDEV_INTERNAL_SENT_D2H_PHASE) {
				ignore_cnt++;
			}
		} else {
			if (item->flags & PCIEDEV_INTERNAL_SENT_D2H_PHASE) {
				ignore_cnt++;
				pciedev_free_lclbuf_pool(item->msgbuf, buf - sizeof(cmn_msg_hdr_t));
			} else {
				pciedev_free_lclbuf_pool(item->msgbuf, buf);
			}
			if (item->msg_type == MSG_TYPE_IOCTL_CMPLT) {
				pciedev_process_ioctl_done(pciedev);
			}
		}
	}
	return ignore_cnt;
} /* pciedev_handle_d2h_dmacomplete */

#ifndef PCIE_PHANTOM_DEV

bool
pciedev_handle_d2h_dma(struct dngl_bus *pciedev)
{
	void * prev;
	uint8 ignore_txd;

	while ((prev = dma_getnexttxp(pciedev->d2h_di, HNDDMA_RANGE_TRANSMITTED))) {
#if defined(PCIE_M2M_HOST_READ_BARRIER)
		pciedev_d2h_read_barrier(pciedev);
#endif /* PCIE_M2M_HOST_READ_BARRIER */
		ignore_txd = pciedev_handle_d2h_dmacomplete(pciedev, prev);
		if (ignore_txd > 0) {
			/* to support phase bit for msgbufs same buffer was split up into 2 */
			while (ignore_txd--) {
				prev = dma_getnexttxp(pciedev->d2h_di, HNDDMA_RANGE_TRANSMITTED);
			}
		}
	}

	/* free up the rx descriptors */
	dma_clearrxp(pciedev->d2h_di);

	pciedev_queue_d2h_req_send(pciedev);
	return FALSE;
}

/**
 * The PCIe core receives an h2d DMA interrupt when a message has been copied from host memory into
 * device memory. The firmware now has to decode the host message and take action on it.
 * Has to decode message or forward tx payload towards antenna
 * 1. get pkt address from dma module
 * 2. Decode message
 * 3. forward tx payload towards antenna
 */
bool
pciedev_handle_h2d_dma(struct dngl_bus *pciedev)
{
	void *txpkt, *rxpkt;
	msgbuf_ring_t *msgbuf;
	uint16 dmalen;
	uint8 msgtype;

	PCI_TRACE(("handle the h2d interrupt\n"));

	/* Release DMA tx descriptors so they can be reused */
	while ((txpkt = dma_getnexttxp(pciedev->h2d_di, HNDDMA_RANGE_TRANSMITTED)));
	while (1) {
		/* Rx offset Pkt */
		rxpkt = dma_getnextrxp(pciedev->h2d_di, FALSE);
		if (rxpkt == NULL)
			break;

		if (rxpkt == pciedev->dummy_rxoff)
			continue;

		/* Consume 1 message from the queue and retrieve queued up pktlength */
		dmalen = pciedev_htoddma_deque(pciedev, &msgbuf, &msgtype);

		/* internal payload frames */
		if (msgtype  & MSG_TYPE_INTERNAL_USE_START) {
			pciedev_handle_h2d_pyld(pciedev, msgtype, rxpkt, dmalen, msgbuf);
			continue;
		} else {
			ASSERT(msgbuf);
			/*
			 * Invokes message handler specific for the ring used. e.g.
			 * pciedev_handle_h2d_msg_ctrlsubmit(), or pciedev_handle_h2d_msg_rxsubmit()
			 */
			(msgbuf->process_rtn)(pciedev, rxpkt, dmalen, msgbuf);

			/* Update read pointer */
			pciedev_ring_update_readptr(pciedev, msgbuf, dmalen);

			/* free up local message space if inuse pool is not used */
			if (!INUSEPOOL(msgbuf)) {
				pciedev_free_lclbuf_pool(msgbuf, rxpkt);
			}
			pciedev->msg_pending--;
		}
	}

	/* see if there is a pending event packet waiting for host buffer */
	pciedev_queue_d2h_req_send(pciedev);

	/* Call the generic HNDRTE layer callback signalling that bus layer
	 * DMA descrs were freed / something was dequed from the bus layer DMA queue
	 */
	hnd_dmadesc_avail_cb();

	/* There might be new messages in the circular buffer. Schedule DMA for those too */
	pciedev_msgbuf_intr_process(pciedev);

	if (pciedev->htod_dma_wr_idx == pciedev->htod_dma_rd_idx) {
		if (pciedev->force_ht_on) {
			PCI_TRACE(("%s: no more H2D DMA, so no more force HT\n", __FUNCTION__));
			pciedev->force_ht_on = FALSE;
			pciedev_manage_h2d_dma_clocks(pciedev);
		}
	}

	return FALSE;
} /* pciedev_handle_h2d_dma */
#endif /* PCIE_PHANTOM_DEV */

/** Transmit / PropTx related */
void pciedev_upd_flr_port_handle(struct dngl_bus * pciedev, uint8 handle, bool open)
{
	msgbuf_ring_t	*flow_ring;
	struct dll * cur, * prio;

	/* loop through nodes, weighted ordered queue implementation */
	prio = dll_head_p(&pciedev->active_prioring_list);
	while (!dll_end(prio, &pciedev->active_prioring_list)) {
		prioring_pool_t *prioring = (prioring_pool_t *)prio;
		cur = dll_head_p(&prioring->active_flowring_list);
		while (!dll_end(cur, &prioring->active_flowring_list)) {
			flow_ring = ((flowring_pool_t *)cur)->ring;
			if (flow_ring->handle != handle) {
				cur = dll_next_p(cur);
				continue;
			}
			if (!open)
				flow_ring->status |= FLOW_RING_PORT_CLOSED;
			else {
				flow_ring->status &= ~FLOW_RING_PORT_CLOSED;
				/*
				 * When the port opens, we need to trigger
				 * fetch once again.
				 */
				pciedev_schedule_flow_ring_read_buffer(pciedev);
			}
			flow_ring->flow_info.flags &= ~FLOW_RING_FLAG_LAST_TIM;

			/* next node */
			cur = dll_next_p(cur);
		}

		/* get next priority ring node */
		prio = dll_next_p(prio);
	}
}

/** Transmit / PropTx related */
void pciedev_upd_flr_if_state(struct dngl_bus * pciedev, uint8 ifindex, bool open)
{
	msgbuf_ring_t	*flow_ring;
	struct dll * cur, * prio;

	/* loop through nodes */
	prio = dll_head_p(&pciedev->active_prioring_list);
	while (!dll_end(prio, &pciedev->active_prioring_list)) {
		prioring_pool_t *prioring = (prioring_pool_t *)prio;
		cur = dll_head_p(&prioring->active_flowring_list);
		while (!dll_end(cur, &prioring->active_flowring_list)) {
			flow_ring = ((flowring_pool_t *)cur)->ring;

			if (flow_ring->flow_info.ifindex != ifindex) {
				cur = dll_next_p(cur);
				continue;
			}
			if (!open)
				flow_ring->status |= FLOW_RING_PORT_CLOSED;
			else {
				flow_ring->status &= ~FLOW_RING_PORT_CLOSED;
				/*
				 * When the port opens, we need to trigger
				 * fetch once again.
				 */
				pciedev_schedule_flow_ring_read_buffer(pciedev);
			}
			/* next node */
			cur = dll_next_p(cur);
		}

		/* get next priority ring node */
		prio = dll_next_p(prio);
	}
}

/** Transmit / PropTx related */
void pciedev_upd_flr_tid_state(struct dngl_bus * pciedev, uint8 tid, bool open)
{
	msgbuf_ring_t	*flow_ring;
	struct dll * cur, * prio;

	/* loop through nodes */
	prio = dll_head_p(&pciedev->active_prioring_list);
	while (!dll_end(prio, &pciedev->active_prioring_list)) {
		prioring_pool_t *prioring = (prioring_pool_t *)prio;
		cur = dll_head_p(&prioring->active_flowring_list);
		while (!dll_end(cur, &prioring->active_flowring_list)) {
			flow_ring = ((flowring_pool_t *)cur)->ring;

			if (flow_ring->flow_info.tid_ac != tid) {
				cur = dll_next_p(cur);
				continue;
			}
			if (!open)
				flow_ring->status |= FLOW_RING_PORT_CLOSED;
			else {
				flow_ring->status &= ~FLOW_RING_PORT_CLOSED;
				/*
				 * When the port opens, we need to trigger
				 * fetch once again.
				 */
				pciedev_schedule_flow_ring_read_buffer(pciedev);
			}
			/* next node */
			cur = dll_next_p(cur);
		}

		/* get next priority ring node */
		prio = dll_next_p(prio);
	}
}

/** Transmit / PropTx related */
void pciedev_upd_flr_hanlde_map(struct dngl_bus * pciedev, uint8 handle, uint8 ifindex,
	bool add, uint8 *addr)
{
	msgbuf_ring_t	*flow_ring;
	struct dll * cur, * prio;

	/* loop through nodes */
	prio = dll_head_p(&pciedev->active_prioring_list);
	while (!dll_end(prio, &pciedev->active_prioring_list)) {
		prioring_pool_t *prioring = (prioring_pool_t *)prio;
		cur = dll_head_p(&prioring->active_flowring_list);
		while (!dll_end(cur, &prioring->active_flowring_list)) {
			flow_ring = ((flowring_pool_t *)cur)->ring;

			if ((flow_ring->flow_info.ifindex != ifindex) ||
				(memcmp(flow_ring->flow_info.da, addr, ETHER_ADDR_LEN))) {
				cur = dll_next_p(cur);
				continue;
			}

			if (add)
				flow_ring->handle = handle;
			else
				flow_ring->handle = 0xff;

			/* Next node */
			cur = dll_next_p(cur);
		}

		/* get next priority ring node */
		prio = dll_next_p(prio);
	}
}

/** Related to power save mode for softAP in PropTx, processing WLFC_CTL_TYPE_MAC_REQUEST_PACKET */
void pciedev_process_reqst_packet(struct dngl_bus * pciedev,
	uint8 handle, uint8 ac_bitmap, uint8 count)
{
	uint32 w_ptr;
	int i;
	uint16 prev_maxpktcnt;
	msgbuf_ring_t	*flow_ring;
	msgbuf_ring_t	*fptr[PCIE_MAX_TID_COUNT];
	struct dll * cur, * prio;

	if (pciedev->in_d3_suspend || !count) {
		return;
	}
	bzero(fptr, sizeof(fptr));

	/* loop through active queues */
	prio = dll_head_p(&pciedev->active_prioring_list);
	while (!dll_end(prio, &pciedev->active_prioring_list)) {
		prioring_pool_t *prioring = (prioring_pool_t *)prio;
		cur = dll_head_p(&prioring->active_flowring_list);
		while (!dll_end(cur, &prioring->active_flowring_list)) {
			uint8 tid_ac;
			flow_ring = ((flowring_pool_t *)cur)->ring;
			tid_ac = flow_ring->flow_info.tid_ac;
			if (flow_ring->handle != handle) {
				cur = dll_next_p(cur);
				continue;
			}
			if (pciedev->schedule_prio == PCIEDEV_SCHEDULER_TID_PRIO)
				tid_ac = PCIEDEV_TID2AC(flow_ring->flow_info.tid_ac);

			if (ac_bitmap & (0x1 << tid_ac))
				fptr[flow_ring->flow_info.tid_ac] = flow_ring;

			cur = dll_next_p(cur);
		}

		/* get next priority ring node */
		prio = dll_next_p(prio);
	}

	i = PCIE_MAX_TID_COUNT - 1;
	while (i >= 0) {
		if (fptr[i] == NULL) {
			i--;
			continue;
		}
		w_ptr = DNGL_RING_WPTR(fptr[i]);

		/* Check write pointer of the flow ring and mark it for pending pkt pull */
		if (!READ_AVAIL_SPACE(w_ptr, fptr[i]->rd_pending, RING_MAX_ITEM(fptr[i]))) {
			fptr[i]->status &= ~FLOW_RING_PKT_PENDING;
			if (fptr[i]->flow_info.flags & FLOW_RING_FLAG_LAST_TIM) {
				/* informs WL subsystem about the TIM_RESET operation */
				uint8 tid_ac = fptr[i]->flow_info.tid_ac;
				if (pciedev->schedule_prio == PCIEDEV_SCHEDULER_TID_PRIO)
					tid_ac = PCIEDEV_TID2AC(fptr[i]->flow_info.tid_ac);
				fptr[i]->flow_info.flags &= ~FLOW_RING_FLAG_LAST_TIM;
				dngl_flowring_update(pciedev->dngl, fptr[i]->flow_info.ifindex,
					(uint8) fptr[i]->handle, FLOW_RING_TIM_RESET,
					(uint8 *)&fptr[i]->flow_info.sa,
					(uint8 *)&fptr[i]->flow_info.da, tid_ac);
			}
			i--;
			continue;
		}
		if (count > 0) {
			prev_maxpktcnt = fptr[i]->flow_info.maxpktcnt;
			fptr[i]->flow_info.maxpktcnt = count;
			fptr[i]->flow_info.reqpktcnt += count;
			fptr[i]->flow_info.flags |= FLOW_RING_FLAG_PKT_REQ;
			count -= pciedev_read_flow_ring_host_buffer(pciedev, fptr[i],
					pciedev->fetch_pending);
			fptr[i]->flow_info.maxpktcnt = prev_maxpktcnt;
			i--;
		} else
			break;
	}
} /* pciedev_process_reqst_packet */

/**
 * Related to transmit / power save mode for softAP. Called upon receiving a 'tx post' message from
 * the host. Handles packet fetch request from wl apps (AP power save) for PM 1 (pspoll) case.
 * Flow control makes use of TLVs to communicate with the host, the LTV is placed in front of a
 * host packet.
 *
 * Adds WLFC_PKTFLAG_PKT_REQUESTED flag / d11 seq no. in front of wl header in TLV format.
 */
static void pciedev_push_pkttag_tlv_info(struct dngl_bus *pciedev, void* p,
       msgbuf_ring_t *flow_ring, uint16 index)
{
	uint8 *buf;
	uint32 pkt_flags = 0;
	uint8 flags = 0;
	uint8 len = ROUNDUP((TLV_HDR_LEN + WLFC_CTL_VALUE_LEN_PKTTAG + WLFC_CTL_VALUE_LEN_SEQ),
		sizeof(uint32));
	uint16 seq = 0;
	uint16 bit_idx = MODULO_RING_IDX(index, flow_ring);
#ifdef BCM_DHDHDR
	if (BCMDHDHDR_ENAB()) {
		ASSERT(len <= sizeof(struct lbuf_finfo));
		buf = PKTFRAGFCTLV(pciedev->osh, p);
	} else
#endif // endif
	{
		if (PKTHEADROOM(pciedev->osh, p) < len) {
			PCI_ERROR(("pciedev_push_pkttag_tlv_info: No room for pkttag TLV\n"));
			return;
		}

		PKTPUSH(pciedev->osh, p, len);
		buf = PKTDATA(pciedev->osh, p);
	}

	PKTSETDATAOFFSET(p, len >> 2);
	buf[TLV_TAG_OFF] = WLFC_CTL_TYPE_PKTTAG;
	buf[TLV_LEN_OFF] = WLFC_CTL_VALUE_LEN_PKTTAG + WLFC_CTL_VALUE_LEN_SEQ;

	/* for packets sent as a result of Ps-poll from peer STA in PS */
	if (flow_ring->flow_info.flags & FLOW_RING_FLAG_PKT_REQ) {
		flags |= WLFC_PKTFLAG_PKT_REQUESTED;
		WL_TXSTATUS_SET_FLAGS(pkt_flags, flags);
		memcpy(&buf[TLV_HDR_LEN], &pkt_flags, sizeof(uint32));
		flow_ring->flow_info.reqpktcnt--;
		if (!flow_ring->flow_info.reqpktcnt)
			flow_ring->flow_info.flags &= ~FLOW_RING_FLAG_PKT_REQ;
	}

	/* for reusing d11 seq number for suppressed packets */
	if (isset(flow_ring->reuse_sup_seq, bit_idx)) {
		seq = flow_ring->reuse_seq_list[bit_idx];
		PCI_TRACE(("PCIE Reused Seq %d %d %d\n", index,
			flow_ring->reuse_seq_list[bit_idx],
			WL_SEQ_GET_NUM(flow_ring->reuse_seq_list[bit_idx])));
	}
	memcpy(&buf[TLV_HDR_LEN + WLFC_CTL_VALUE_LEN_PKTTAG],
		&seq, sizeof(uint16));
}

/** Called on wl dpc. Send out chained pkt completions */
void
pciedev_flush_chained_pkts(struct dngl_bus *pciedev)
{
	/* dequeue and send out rx payloads and rx completes */
	if (pciedev->ioctl_pend != TRUE) {
		pciedev_queue_d2h_req_send(pciedev);
	}

	/* Send out tx completes */
	if (pciedev->dtoh_txcpl->cbuf_pool->pend_item_cnt) {
		PCI_TRACE(("TX: pend count = %d\n", pciedev->dtoh_txcpl->cbuf_pool->pend_item_cnt));
		pciedev_xmit_txstatus(pciedev, pciedev->dtoh_txcpl);
	}
}

/** Timer callback function */
void
pciedev_flush_glommed_rxstatus(dngl_timer_t *t)
{
	struct dngl_bus *pciedev = (struct dngl_bus *) hnd_timer_get_ctx(t);

	if (pciedev->dtoh_rxcpl->cbuf_pool->pend_item_cnt) {
		PCI_TRACE(("%s: RX: pend count = %d\n", __FUNCTION__,
			pciedev->dtoh_rxcpl->cbuf_pool->pend_item_cnt));
		pciedev_xmit_rxcomplete(pciedev, pciedev->dtoh_rxcpl);
	}
	pciedev->glom_timer_active = FALSE;
}

#ifdef HOST_HDR_FETCH
/**
 * Post the packet to TX FIFO
 * 1. if different messagey type, return
 * 2. queue/ifidx differ, commit all packets
 * 3. submit all packets available in the interrupt completion
 */
static int
pciedev_mac_dma_submit(struct dngl_bus *pciedev, void **buf, dma_queue_t** item)
{
	uint queue;
	int ifidx;
	bool commit;
	bool first_entry;
	uint8 flags;

	/* Initialize */
	first_entry = TRUE;
	commit = TRUE;
	queue = QUEUE_INVALID;
	ifidx = IFIDX_INVALID;
	flags = 0;

	do {
		/* Dequeue from d2h dma completion queue */
		*item = &pciedev->dtoh_dma_q[pciedev->dtoh_dma_rd_idx];
		pciedev->dtoh_dma_rd_idx = (pciedev->dtoh_dma_rd_idx+1) % MAX_DMA_QUEUE_LEN_D2H;

		if ((*item)->msg_type == MSG_TYPE_INVALID) {
			continue;
		}
		/* Check for TX header message */
		if ((*item)->msg_type != MSG_TYPE_TXD_PUSH) {
			return BCME_OK;
		}

		/* Retrieve pkt and queue info
		 *
		 * Item entries are overloaded with queue and pkt buffer pointer
		 * item->len is redefined ad queue
		 * item->msgbuf is redefined as pkt
		 */
		queue = (*item)->len;
		*buf = (void*)((*item)->msgbuf);
		flags = (*item)->flags;
		ifidx = PKTIFINDEX(pciedev->osh, *buf);

		commit = (flags & PCIEDEV_MPDU_COMMIT) ? TRUE : FALSE;

		/* Post the packet to TX FIFO */
		dngl_mac_dma_submit(pciedev->dngl, ifidx, *buf, queue, commit, first_entry);

		first_entry = FALSE;
	} while ((*buf = dma_getnexttxp(pciedev->d2h_di, HNDDMA_RANGE_TRANSMITTED)) != NULL);

	return BCME_ERROR;
}

/* Exported function to report pciedev d2h descriptor availability */
static bool
pciedev_d2h_desc_avail(struct dngl_bus *pciedev)
{
	uint32 txdesc, rxdesc;

	txdesc = PCIEDEV_GET_AVAIL_DESC(pciedev, DTOH, TXDESC);
	rxdesc = PCIEDEV_GET_AVAIL_DESC(pciedev, DTOH, RXDESC);

	/* Need 1 RX ,TX descriptor to post tx header to host */
	return ((txdesc >= 1) && (rxdesc >= 1));
}
/* Push TX header section to host scratch area */
int
__pciedev_txhdr_push(struct dngl_bus *pciedev, void *p, uint queue, bool commit)
{
	uint16 pktlen;
	dma64addr_t addr;
	uint8 flags;

	/* Check for enough descriptors */
	if (!pciedev_d2h_desc_avail(pciedev)) {
		/* pcie D2H descriptors not available now */
		return BCME_NOMEM;
	}

	/* Initialize */

	/* With CTDMA enabled  only last packet in the chain should be committed
	 * commit here should be always FALSE
	 * But for CTDMA disabled case each individual packet should be committed
	 * commit here should be always TRUE
	 */
	flags = (commit) ? PCIEDEV_MPDU_COMMIT : 0;
	pktlen = PKTLEN(pciedev->osh, p);

	pktlen = MIN(BCM_DHDHDR_SIZE, pktlen);

	/* Address extension */
	lbuf_dhdhdr_memory_extension(p, &addr);

	/*
	 * Store pkt and queue info
	 * len redefined as queue
	 * msgbuf redefined as pkt
	 */
	pciedev_dma_tx_account(pciedev, (uint16)queue, MSG_TYPE_TXD_PUSH, flags, p);

	/* RX descriptor => HOST scratch buffer */
	if (dma_rxfast(pciedev->d2h_di, addr, pktlen)) {
		PCI_ERROR(("__pciedev_txhdr_push: RX posting failed \n"));
		ASSERT(0);
	}

	/* TX Descriptor => PKTDATA */
	PHYSADDR64HISET(addr, (uint32) 0);
	PHYSADDR64LOSET(addr, (uint32)PKTDATA(pciedev->osh, p));

	if (dma_msgbuf_txfast(pciedev->d2h_di, addr, TRUE, pktlen, TRUE, TRUE))
	{
		PCI_ERROR(("__pciedev_txhdr_push: TX posting failed \n"));
		ASSERT(0);
	}

	return BCME_OK;
}
/**
 * Pull out packets to given fifo
 * Enque it back to pkt_list
 * Reset message type for those entries to MSG_TYPE_INVALID
 * MSG_TYPE_INVALID will be skipped during DMA done ISR
 */
uint16
__pciedev_reclaim_txpkts(struct dngl_bus *pciedev, struct spktq *pkt_list, uint16 fifo, bool free)
{
	uint16 idx;
	uint16 active, pkt_cnt;
	dma_queue_t *dmaq;

	if (!free)
		ASSERT(pkt_list);

	/* Initialize */
	idx = 0;
	pkt_cnt = 0;
	active = NTXPACTIVE(pciedev->dtoh_dma_rd_idx,
		pciedev->dtoh_dma_wr_idx, MAX_DMA_QUEUE_LEN_D2H);

	idx = pciedev->dtoh_dma_rd_idx;
	while (active) {
		dmaq = &(pciedev->dtoh_dma_q[idx]);

		/* Pull out packets matching given fifo */
		if ((dmaq->msg_type == MSG_TYPE_TXD_PUSH) && (dmaq->len == fifo)) {
			if (free) {
				/* Free the packets here itself */
				/* reset HDR IN HOST Flag */
				PKTRESETHDRINHOST(pciedev->osh, dmaq->msgbuf);
				PKTFREE(pciedev->osh, dmaq->msgbuf, TRUE);
			} else {
				/* Enque the packet */
				pktenq(pkt_list, dmaq->msgbuf);
			}
			pkt_cnt++;
			/* Reset the msgtype so as to skip this while DMA done */
			dmaq->msg_type = MSG_TYPE_INVALID;
		}
		active--;
		idx = (idx + 1) % MAX_DMA_QUEUE_LEN_D2H;
	};

	if (pkt_cnt) {
		PCI_TRACE(("Reclaim from pciedev Fifo %d : free cnt %d\n", fifo, pkt_cnt));
	}
	return pkt_cnt;
}

void
__pciedev_map_txpkts(struct dngl_bus *pciedev, map_pkts_cb_fn cb, void *ctx)
{
	uint16 idx;
	uint16 active;
	dma_queue_t *dmaq;

	/* Initialize */
	idx = 0;
	active = NTXPACTIVE(pciedev->dtoh_dma_rd_idx,
		pciedev->dtoh_dma_wr_idx, MAX_DMA_QUEUE_LEN_D2H);

	idx = pciedev->dtoh_dma_rd_idx;
	while (active) {
		dmaq = &(pciedev->dtoh_dma_q[idx]);

		/* Call map packets cb */
		if ((dmaq->msg_type == MSG_TYPE_TXD_PUSH) && (dmaq->msg_type != MSG_TYPE_INVALID)) {
			(void)cb(ctx, dmaq->msgbuf);
		}
		active--;
		idx = (idx + 1) % MAX_DMA_QUEUE_LEN_D2H;
	};
}
#endif /* HOST_HDR_FETCH */

#if defined(PCIE_DMA_INDEX) && defined(SBTOPCIE_INDICES)
/*
 * sb2pcie access to host memmory using sbtopcie0 address translation
 * Used for temporary accees only; Put back base reg values after access
 *
 * Input : <4 byte Low address> < 2/4 byte pointer to data>
 */
void
pciedev_sbtopcie_access(struct dngl_bus * pciedev, uint32 low_addr,
	pcie_rw_index_t *data, bool read)
{
	uint32 host_mem;
	uint32 base_bkp;
	uint32 base_addr;

	ASSERT(pciedev->regs);
	BCM_REFERENCE(base_addr);

	/* Use sbtopcie translation 0 */
	/* Remap the address as per transalation 0 */
	SBTOPCIE_ADDR_REMAP(pciedev, low_addr, 0, host_mem);

	/* Take a backup of base address */
	base_bkp = R_REG(pciedev->osh, &pciedev->regs->sbtopcie0);

	SBTOPCIE_BASE_CONFIG(pciedev, low_addr, sizeof(uint32),
		0, base_addr, FALSE);

	/* Access host memory via sbtopcie translation 0 */
	if (read)
		*data = *((pcie_rw_index_t *)host_mem);
	else
		*((pcie_rw_index_t *)host_mem) = *data;

	/* Insert an instruction barrier to prevent base reg from over-writing */
	INSTR_BARRIER();

	/* restore back old base address */
	W_REG(pciedev->osh, &pciedev->regs->sbtopcie0, base_bkp);

}
/* Sync up read pointers of h2d rings[common rings + flow rings */
void
pciedev_sync_h2d_read_ptrs(struct dngl_bus * pciedev, msgbuf_ring_t* ring)
{
	const uint32 rw_index_sz = PCIE_RW_INDEX_SZ;
	uint32 host_addr;
	pcie_rw_index_t host_data;

	ASSERT(ring);

	/* Host address to access */
	host_addr = PHYSADDR64LO(pciedev->h2d_readindx_dmablock->host_dmabuf);
	host_addr = (uint32)BCMPCIE_RW_INDEX_ADDR(host_addr, rw_index_sz, ring->ringid);

	/* Data to be written */
	host_data = DNGL_RING_RPTR(ring);

	/* Write to host Mem */
	pciedev_sbtopcie_access(pciedev, host_addr, &host_data, FALSE);
}
/* Sync up D2h write pointers between host and dongle [completion rings] */
static void
pciedev_sync_d2h_write_ptrs(struct dngl_bus * pciedev, msgbuf_ring_t* ring)
{
	uint32 index;
	const uint32 rw_index_sz = PCIE_RW_INDEX_SZ;
	uint32 host_addr;
	pcie_rw_index_t host_data;

	ASSERT(ring);

	index = BCMPCIE_D2H_RING_IDX(ring->ringid);

	/* Host address to access */
	host_addr = PHYSADDR64LO(pciedev->d2h_writeindx_dmablock->host_dmabuf);
	host_addr = (uint32)BCMPCIE_RW_INDEX_ADDR(host_addr, rw_index_sz, index);

	/* Data to be written */
	host_data = DNGL_RING_WPTR(ring);

	/* Write to host mem */
	pciedev_sbtopcie_access(pciedev, host_addr, &host_data, FALSE);
}
/* Sync up read pointers for d2h completion rings from host */
void
pciedev_sync_d2h_read_ptrs(struct dngl_bus * pciedev, msgbuf_ring_t* ring)
{
	uint32 index;
	const uint32 rw_index_sz = PCIE_RW_INDEX_SZ;
	uint32 host_addr;
	pcie_rw_index_t host_data;

	ASSERT(ring);
	index = BCMPCIE_D2H_RING_IDX(ring->ringid);

	/* Host address to access */
	host_addr = PHYSADDR64LO(pciedev->d2h_readindx_dmablock->host_dmabuf);
	host_addr = (uint32)BCMPCIE_RW_INDEX_ADDR(host_addr, rw_index_sz, index);

	/* Read from host */
	pciedev_sbtopcie_access(pciedev, host_addr, &host_data, TRUE);
	DNGL_RING_RPTR(ring) = host_data;
}
#endif /* SBTOPCIE_INDICES && DMA_INDICES */

#ifdef FLOW_RING_LAZY_FETCH
void
pciedev_flow_watchdog_timerfn(dngl_timer_t *t)
{
	struct dngl_bus *pciedev = (struct dngl_bus *) hnd_timer_get_ctx(t);
	msgbuf_ring_t	*flow_ring_node;
	struct dll * cur, * prio;
	uint8 active_flow_ring_cnt = 0;

	if (pciedev->lazy_fetch_enabled == FALSE)
		return;

	/* loop through active queues */
	prio = dll_head_p(&pciedev->active_prioring_list);
	while (!dll_end(prio, &pciedev->active_prioring_list)) {
		prioring_pool_t *prioring = (prioring_pool_t *)prio;
		cur = dll_head_p(&prioring->active_flowring_list);
		while (!dll_end(cur, &prioring->active_flowring_list)) {
			flow_ring_node = ((flowring_pool_t *)cur)->ring;

			if ((flow_ring_node->status & FLOW_RING_PORT_CLOSED) ||
				(flow_ring_node->status & FLOW_RING_DELETE_RESP_PENDING) ||
				(flow_ring_node->status & FLOW_RING_SUP_PENDING)) {
				goto nextnode;
			}
			/* Only look for active flow_ring which sent packet recently */
			if ((flow_ring_node->status & FLOW_RING_ACTIVE) &&
				(DELTA(hnd_time(), flow_ring_node->lazy_fetch_last_active_time) <
				LAZY_FETCH_RING_ACTIVE_CHECK_TIME))
				active_flow_ring_cnt++;

		nextnode:
			cur = dll_next_p(cur);
		}
		/* get next priority ring node */
		prio = dll_next_p(prio);
	}
	if (pciedev->active_flow_ring_cnt != active_flow_ring_cnt) {
		pciedev->active_flow_ring_cnt = active_flow_ring_cnt;
		PCI_TRACE(("active flow ring number update: %d \n",
				pciedev->active_flow_ring_cnt));
		/* update lazy fetch max wait time based on different ZONE */
		if (active_flow_ring_cnt <= pciedev->lazy_fetch_thresh)
			pciedev->lazy_fetch_max_wait_time = 0;
		else if (active_flow_ring_cnt <= LAZY_FETCH_NUM_ACTIVE_RING_ZONE1)
			pciedev->lazy_fetch_max_wait_time = LAZY_FETCH_MAX_WAIT_TIME_ZONE1;
		else if (active_flow_ring_cnt <= LAZY_FETCH_NUM_ACTIVE_RING_ZONE2)
			pciedev->lazy_fetch_max_wait_time = LAZY_FETCH_MAX_WAIT_TIME_ZONE2;
		else
			pciedev->lazy_fetch_max_wait_time = LAZY_FETCH_MAX_WAIT_TIME_ZONE3;
	}

	if ((pciedev->lazy_fetch_max_wait_time_override) &&
		(pciedev->lazy_fetch_max_wait_time != pciedev->lazy_fetch_max_wait_time_override))
		pciedev->lazy_fetch_max_wait_time = pciedev->lazy_fetch_max_wait_time_override;
}
#endif /* FLOW_RING_LAZY_FETCH */
